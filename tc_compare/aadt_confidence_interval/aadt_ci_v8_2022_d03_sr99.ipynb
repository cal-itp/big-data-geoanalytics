{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826e200c-086a-4cd6-962b-0a3d41ee4e11",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AADT Confidence Interval, SR-99 District 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f1a47-13bc-413e-bf96-bc0f40580be1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helpful Links\n",
    "\n",
    "---\n",
    "\n",
    "### FHWA Links\n",
    "* Guidelines for Obtaining AADT Estimates from Non-Traditional Sources:\n",
    "    * https://www.fhwa.dot.gov/policyinformation/travel_monitoring/pubs/aadtnt/Guidelines_for_AADT_Estimates_Final.pdf\n",
    "\n",
    "---\n",
    "  \n",
    "### AADT Analysis Locations\n",
    "* 10 locations were used in the analysis\n",
    "* Locations were determined based on the location on installed & recording Traffic Operations cameras\n",
    "    * for additional information contact Zhenyu Zhu with Traffic Operations\n",
    "\n",
    "### Traffic Census Data\n",
    "* https://dot.ca.gov/programs/traffic-operations/census/traffic-volumes\n",
    "* Back AADT, Peak Month, and Peak Hour usually represents traffic South or West of the count location.  \n",
    "* Ahead AADT, Peak Month, and Peak Hour usually represents traffic North or East of the count location. Listing of routes with their designated  \n",
    "\n",
    "* Because the Back & Ahead counts are included at each location in the Traffic Census Data, (e.g., \"IRWINDALE, ARROW HIGHWAY\") only one [OBJECTID*] per location was pulled; for this analysis the North Bound Nodes were used for the analysis. \n",
    "    * for more information see the diagram: https://traffic.onramp.dot.ca.gov/downloads/traffic/files/performance/census/Back_and_Ahead_Leg_Traffic_Count_Diagram.pdf\n",
    "\n",
    "### StreetLight Analysis Data\n",
    "* Analysis Type == Network Performance\n",
    "* Segment Metrics\n",
    "* 2022 was used to match currently available Traffic Census Data (as of 8/27/2025)\n",
    "* pulled a variety of Day Types, but plan to just look at \"\"\"All Day Types\"\"\"\n",
    "* pulled a variety of Day Parts, but plan to just look at \"\"\"All Day Parts\"\"\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b47dc-41a0-41ed-9d63-8fe73c9c0549",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Identify the corridor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa6bce3a-11a1-4f2a-a7d9-eb7904946817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pull in the coordinates from the utils docs\n",
    "#from osow_frp_o_d_utils_v3 import origin_intersections, destination_intersections\n",
    "import shs_ct_tc_locations_utils as tc_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b6f8329-3a2d-49ec-ba88-cc69b26f6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available corridors\n",
    "    # \"interstate_605_d7_tc_aadt_locations\"\n",
    "    # \"sr_99_d3_tc_aadt_locations\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f50459-4690-4594-891e-0d18de73dfb3",
   "metadata": {},
   "source": [
    "### Update the corridor name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362e5fc0-0edf-460d-bc30-82e2647c7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the corridor to be analyzed\n",
    "#CORRIDOR_VAR_NAME = \"interstate_605_d7_tc_aadt_locations\"\n",
    "CORRIDOR_VAR_NAME = \"sr_99_d3_tc_aadt_locations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6c6c3b7-87f4-4c6d-b468-50ae993dd498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resolve the object from the module by name\n",
    "try:\n",
    "    aadt_locations = getattr(tc_locs, CORRIDOR_VAR_NAME)\n",
    "except AttributeError:\n",
    "    raise KeyError(\n",
    "        f\"'{CORRIDOR_VAR_NAME}' not found in shs_ct_tc_locations_utils. \"\n",
    "        \"Double-check the variable name.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e192bde-dff4-46cf-b2cd-da8347440ec5",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "171fa2b0-9b02-4947-a4d1-dd18ef86de42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import t as student_t  # if SciPy is not available, use a small lookup table\n",
    "import csv\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628fa29-7fdc-46d2-bc5a-84c271f150d9",
   "metadata": {},
   "source": [
    "## Step 0, Pull in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c01657-5051-44da-b79f-594359c06427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# 0) Pull in the Data\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# This function will pull in the data and clean the column headers in a way that will make them easier to work with\n",
    "def getdata_and_cleanheaders(path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Clean column headers: remove spaces, convert to lowercase, and strip trailing asterisks\n",
    "    cleaned_columns = []\n",
    "    for column in df.columns:\n",
    "        cleaned_column = column.replace(\" \", \"\").lower().rstrip(\"*\")\n",
    "        cleaned_columns.append(cleaned_column)\n",
    "\n",
    "    df.columns = cleaned_columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0a612-d979-43d3-87f3-e8d69df16d42",
   "metadata": {},
   "source": [
    "### import option 0.1: Identify the Google Cloud Storage path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce4826a9-4c1c-46e4-95c5-20c820351b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the GCS path to the data\n",
    "gcs_path = \"gs://calitp-analytics-data/data-analyses/big_data/compare_traffic_counts/0_2022/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "686ea444-0d68-415c-beda-7ecdfe887062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pull in the data & create dataframes\n",
    "df_tc = getdata_and_cleanheaders(f\"{gcs_path}caltrans_traffic_census_2022.csv\")  # Traffic Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f8efd6d-9ca8-4919-9dba-e8a7c4c3ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available datasets\n",
    "    # streetlight_605_d7_all_vehicles_np_2022.csv\n",
    "    # streetlight_99_d3_all_vehicles_2022_np.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a890154-e2c9-435a-b136-48d932f3359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the StreetLight Analysis to be used in the AADT comparison\n",
    "# df_stl = getdata_and_cleanheaders(f\"{gcs_path}streetlight_605_d7_all_vehicles_np_2022.csv\")  # StreetLight\n",
    "df_stl = getdata_and_cleanheaders(f\"{gcs_path}streetlight_99_d3_all_vehicles_2022_np.csv\")  # StreetLight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f23b9-689a-4f72-a6a3-f3301ced1d71",
   "metadata": {},
   "source": [
    "### import option 0.2: Identify the local data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eec02ebb-e675-4ac5-947d-7103ae894902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Base data folder: aadt_confidence_interval/aadt_data/2022\n",
    "# LOCAL_DATA_DIR = Path.cwd() / \"aadt_data\" / \"2022\"\n",
    "# if not LOCAL_DATA_DIR.exists():\n",
    "#     raise FileNotFoundError(f\"Data folder not found: {LOCAL_DATA_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ef46a3b-f788-4083-8bec-336216431520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Traffic Census (traditional) — local CSV\n",
    "# df_tc = getdata_and_cleanheaders(LOCAL_DATA_DIR / \"caltrans_traffic_census_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dffda5d2-5eea-4f9a-b369-038899234656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available datasets\n",
    "    # streetlight_605_d7_all_vehicles_np_2022.csv\n",
    "    # streetlight_99_d3_all_vehicles_2022_np.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb87b2f6-ee56-4734-98f7-e7766c6abc31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Non-Traditional (StreetLight) — local location\n",
    "# df_stl = getdata_and_cleanheaders(LOCAL_DATA_DIR / \"streetlight_605_d7_all_vehicles_np_2022.csv\")\n",
    "# df_stl = getdata_and_cleanheaders(LOCAL_DATA_DIR / \"streetlight_99_d3_all_vehicles_2022_np.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac917f3-c048-43cf-9c13-18907e39df11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Export to a CSV for viewing/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b73b9a53-0066-4643-b8b7-ec90ceec2f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export traditional dataset (Traffic Census\n",
    "df_tc.to_csv(\"df_tc.csv\", index=False)\n",
    "\n",
    "# export non-traditional dataset (StreetLight)\n",
    "df_stl.to_csv(\"df_stl.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95377851-70c6-4d89-ac93-c81ba703221f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7480907-d484-4e8f-8e53-eeb4517487ce",
   "metadata": {},
   "source": [
    "## Step 1, Build a per-location summary of Traffic Census locations\n",
    "\n",
    "* **Goal:** For each location, pick the relevant **Traffic Census node** (objectid's), compute a representative **traditional AADT**, and emit a tidy per-location record.\n",
    "* **OBJECTID selection policy**: Prefer **ODD** objectid's, if non available, use **all** available.\n",
    "* **Outputs (one row per location):**\n",
    "    * location, daytype, objectids, n_objectids, n_found_in_tc, missing_objectids, traditional_ahead_mean, traditional_behind_mean, traditional_aadt\n",
    "* **Why this matters:** We now have a clean, comparable **traditional AADT** baseline per location to contrast with the non-traditional estimates in later steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "316ff939-7384-4f6d-86bc-109a2a6a54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1) Build a per-location summary of Traffic Census locations\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "def traditional_aadt_by_location(aadt_locations, df_tc, as_df=True, use_parity=False):\n",
    "    \"\"\"\n",
    "    Build a per-location summary of *traditional* (Traffic Census) AADT.\n",
    "\n",
    "    Policy:\n",
    "      • If multiple objectids exist for a location, prefer those whose numeric value is ODD.\n",
    "        If no odd ids exist, fall back to all ids found.\n",
    "      • Default behavior (use_parity=False): for each kept objectid, compute (ahead_aadt + back_aadt)/2,\n",
    "        then average across kept objectids.\n",
    "        \n",
    "    Output columns:\n",
    "      location, daytype, objectids, n_objectids, n_found_in_tc, missing_objectids,\n",
    "      traditional_ahead_mean, traditional_behind_mean, traditional_aadt\n",
    "    \"\"\"\n",
    "    # Requires: import pandas as pd; import numpy as np\n",
    "\n",
    "    def _ensure_list(x):\n",
    "        if x is None: return []\n",
    "        if isinstance(x, (list, tuple, set)): return list(x)\n",
    "        return [x]\n",
    "\n",
    "    def _gather_objectids(node_dict):\n",
    "        ids = []\n",
    "        if not isinstance(node_dict, dict): return ids\n",
    "        if \"objectid\"  in node_dict: ids.extend(_ensure_list(node_dict[\"objectid\"]))\n",
    "        if \"objectids\" in node_dict: ids.extend(_ensure_list(node_dict[\"objectids\"]))\n",
    "        return [str(i).strip() for i in ids if i is not None and str(i).strip() != \"\"]\n",
    "\n",
    "    def _dedup(seq):\n",
    "        seen=set(); out=[]\n",
    "        for x in seq:\n",
    "            if x not in seen:\n",
    "                out.append(x); seen.add(x)\n",
    "        return out\n",
    "\n",
    "    def _keep_odd_objectids(ids):\n",
    "        odds = [i for i in ids if i.isdigit() and (int(i) % 2 == 1)]\n",
    "        return odds if odds else ids\n",
    "\n",
    "    def _normalize_one_location(name, loc, include_oid_in_name=True):\n",
    "        nodes = (loc.get(\"nodes\") if isinstance(loc, dict) else None) or {}\n",
    "        all_ids=[]\n",
    "        for _, node in nodes.items():\n",
    "            all_ids.extend(_gather_objectids(node))\n",
    "        if not all_ids and isinstance(loc, dict) and \"objectid\" in loc:\n",
    "            all_ids = [str(loc[\"objectid\"])]\n",
    "\n",
    "        all_ids = _dedup([i for i in all_ids if i])\n",
    "        kept_ids = _keep_odd_objectids(all_ids)\n",
    "\n",
    "        name_out = name\n",
    "        if include_oid_in_name and kept_ids:\n",
    "            name_out = f\"{name} [{','.join(kept_ids)}]\"\n",
    "\n",
    "        return {\n",
    "            \"name\": name_out,\n",
    "            \"daytype\": (loc.get(\"daytype\") if isinstance(loc, dict) else None) or \"0: All Days (M-Su)\",\n",
    "            \"objectids\": kept_ids,\n",
    "        }\n",
    "\n",
    "    def _normalize_input(aadt_locs):\n",
    "        if isinstance(aadt_locs, pd.DataFrame) and {\"name\",\"daytype\",\"objectids\"}.issubset(aadt_locs.columns):\n",
    "            recs = aadt_locs.to_dict(orient=\"records\")\n",
    "            for r in recs:\n",
    "                r[\"objectids\"] = _keep_odd_objectids(_ensure_list(r.get(\"objectids\")))\n",
    "            return recs\n",
    "        if isinstance(aadt_locs, list) and aadt_locs and isinstance(aadt_locs[0], dict) and \\\n",
    "           {\"name\",\"daytype\",\"objectids\"}.issubset(aadt_locs[0].keys()):\n",
    "            recs = []\n",
    "            for r in aadt_locs:\n",
    "                r = dict(r)\n",
    "                r[\"objectids\"] = _keep_odd_objectids(_ensure_list(r.get(\"objectids\")))\n",
    "                recs.append(r)\n",
    "            return recs\n",
    "\n",
    "        recs = []\n",
    "        if isinstance(aadt_locs, dict):\n",
    "            for nm, loc in aadt_locs.items():\n",
    "                recs.append(_normalize_one_location(nm, loc))\n",
    "            return recs\n",
    "\n",
    "        if isinstance(aadt_locs, list):\n",
    "            for item in aadt_locs:\n",
    "                if not isinstance(item, dict):\n",
    "                    continue\n",
    "                if \"nodes\" in item:\n",
    "                    nm = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "                    recs.append(_normalize_one_location(nm, item))\n",
    "                elif \"objectid\" in item:\n",
    "                    oid = str(item.get(\"objectid\")).strip()\n",
    "                    nm  = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "                    kept = _keep_odd_objectids([oid])\n",
    "                    recs.append({\n",
    "                        \"name\": f\"{nm} [{','.join(kept)}]\" if kept else nm,\n",
    "                        \"daytype\": item.get(\"daytype\", \"0: All Days (M-Su)\"),\n",
    "                        \"objectids\": kept,\n",
    "                    })\n",
    "                else:\n",
    "                    for nm, loc in item.items():\n",
    "                        recs.append(_normalize_one_location(nm, loc))\n",
    "        return recs\n",
    "\n",
    "    def _traditional_aadt_for_ids(df_tc_in, obj_ids):\n",
    "        \"\"\"\n",
    "        Default (use_parity=False): per-oid average of (ahead_aadt, back_aadt), then mean across oids.\n",
    "        If use_parity=True: even->back_aadt, odd->ahead_aadt.\n",
    "        \"\"\"\n",
    "        obj_ids = [str(x).strip() for x in (obj_ids or []) if str(x).strip()]\n",
    "        if not obj_ids:\n",
    "            return np.nan, np.nan, np.nan, 0\n",
    "\n",
    "        sub = df_tc_in[df_tc_in[\"objectid\"].astype(str).str.strip().isin(obj_ids)].copy()\n",
    "        if sub.empty:\n",
    "            return np.nan, np.nan, np.nan, 0\n",
    "\n",
    "        if use_parity:\n",
    "            vals = []\n",
    "            for oid in obj_ids:\n",
    "                row = sub[sub[\"objectid\"].astype(str).str.strip() == oid]\n",
    "                if row.empty:\n",
    "                    continue\n",
    "                v = row.iloc[0][\"back_aadt\"] if (oid.isdigit() and int(oid) % 2 == 0) else row.iloc[0][\"ahead_aadt\"]\n",
    "                vals.append(pd.to_numeric(v, errors=\"coerce\"))\n",
    "            vals = pd.Series(vals, dtype=\"float64\").dropna()\n",
    "            if vals.empty: return np.nan, np.nan, np.nan, 0\n",
    "            overall = float(vals.mean())\n",
    "            return overall, np.nan, np.nan, int(vals.shape[0])\n",
    "\n",
    "        # --- average ahead/back per objectid, then average across objectids ---\n",
    "        sub[\"ahead_aadt\"] = pd.to_numeric(sub.get(\"ahead_aadt\"), errors=\"coerce\")\n",
    "        sub[\"back_aadt\"]  = pd.to_numeric(sub.get(\"back_aadt\"),  errors=\"coerce\")\n",
    "\n",
    "        # per-oid average: mean of available sides (ignore NaN)\n",
    "        per_oid_avg = sub[[\"ahead_aadt\",\"back_aadt\"]].mean(axis=1, skipna=True)\n",
    "        per_oid_avg = per_oid_avg.dropna()\n",
    "\n",
    "        if per_oid_avg.empty:\n",
    "            return np.nan, np.nan, np.nan, 0\n",
    "\n",
    "        overall = float(per_oid_avg.mean())\n",
    "\n",
    "        # side means (for reporting only)\n",
    "        ahead_vals = sub[\"ahead_aadt\"].dropna()\n",
    "        back_vals  = sub[\"back_aadt\"].dropna()\n",
    "        mean_ahead = float(ahead_vals.mean()) if not ahead_vals.empty else np.nan\n",
    "        mean_back  = float(back_vals.mean())  if not back_vals.empty  else np.nan\n",
    "        count_used = int(per_oid_avg.shape[0])\n",
    "\n",
    "        return overall, mean_ahead, mean_back, count_used\n",
    "\n",
    "    # ---- main ----\n",
    "    norm = _normalize_input(aadt_locations)\n",
    "    tc_ids_all = set(df_tc[\"objectid\"].astype(str).str.strip().unique())\n",
    "\n",
    "    rows = []\n",
    "    for loc in norm:\n",
    "        obj_ids = [str(x).strip() for x in (loc.get(\"objectids\") or []) if str(x).strip()]\n",
    "        overall, mean_ahead, mean_back, n_found = _traditional_aadt_for_ids(df_tc, obj_ids)\n",
    "        missing = [x for x in obj_ids if x not in tc_ids_all]\n",
    "\n",
    "        rows.append({\n",
    "            \"location\": loc.get(\"name\"),\n",
    "            \"daytype\":  loc.get(\"daytype\"),\n",
    "            \"objectids\": \"|\".join(obj_ids),\n",
    "            \"n_objectids\": len(obj_ids),\n",
    "            \"n_found_in_tc\": int(n_found),\n",
    "            \"missing_objectids\": \"|\".join(missing) if missing else \"\",\n",
    "            \"traditional_ahead_mean\": mean_ahead,\n",
    "            \"traditional_behind_mean\": mean_back,\n",
    "            \"traditional_aadt\": overall,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows) if as_df else rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db115308-4010-4c5a-a51c-49db69dfaa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run step 1 - traditional aadt counts\n",
    "trad_df = traditional_aadt_by_location(aadt_locations, df_tc, as_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69ea7086-b4d3-47eb-bf6e-bae9c86c506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Step 1 as a CSV to take a look\n",
    "trad_df.to_csv(\"step_1_traditional_aadt_by_location_99_d3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30ac04-cfd4-45bc-a85f-16437dbafdd2",
   "metadata": {},
   "source": [
    "## Step 2, Non-Traditional AADT Summarizer\n",
    "\n",
    "* **Goal:** For each locaiton, use its **StreetLight segment lists** ahead_zones/behind_zones (and also keep the \"all\" lists before de-dup for optional counts\n",
    "* **How it works:**\n",
    "    1. **Normalize locations** to get clean, de-duplicated ahead_zones/behind_zones (and also keep the \"all\" lists before de-dup for optional counts). \n",
    "    2. **Filter StreetLight rows** by daytype, daypart, and optional modeoftravel\n",
    "    3. **Collapse StreetLight by segment:** groupby(zonename) to get a per-segment mean volume and a row count backing that mean\n",
    "    4. For each location/side:\n",
    "        * Pick which list to use for aggregation (**unique** segments for **all** before de-dup)\n",
    "        * Pull the per-segment means for segments that are present; record any **missing** segments\n",
    "        * **Aggregate within a side** using *agg* (**sum** by default; or *mean* if chosen).\n",
    "    5. **Combine directions**: average the two sides when both exist otherwise take the side that exists. \n",
    "* **Outputs (per location)**: metadata (filters used), the zone list, **per-side values, overall non-traditional AADT, backing row counts,** and **segment presence/missing** diagnostics. \n",
    "* **Why this matters:** Produces a consistent, auditable **Non-Traditional baseline** to compare with the traditional AADT from Step 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69e68861-59c8-4426-bf8c-16db1ea1ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# 2) Non-Traditional AADT Summarizer\n",
    "# ------------------------------------------------------\n",
    "\n",
    "def non_traditional_aadt_by_location(\n",
    "    aadt_locations,\n",
    "    df_stl,\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=None,\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\",\n",
    "    as_df=True,\n",
    "    agg=\"sum\",                       # per-side aggregation across segments: \"sum\" | \"mean\"\n",
    "    segment_count_mode=\"unique\",     # count unique zonenames per side (\"unique\") or all (\"all\")\n",
    "    include_oid_in_name=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    pseudo-plan:\n",
    "      1) normalize mapping → keep per-direction (northbound/southbound) lists\n",
    "      2) filter df_stl to daytype/daypart(/mode) and precompute per-zonename mean + row counts\n",
    "      3) for each direction:\n",
    "           side_a = agg(segments in ahead), side_b = agg(segments in behind)\n",
    "           dir_total = average(side_a, side_b)  # use mean to avoid doubling a single direction\n",
    "      4) location total = SUM of all dir_total (NB + SB)            <-- key change\n",
    "      5) emit legacy side columns as NaN (optional to backfill later), plus rich debug fields\n",
    "    \"\"\"\n",
    "    import numpy as np, pandas as pd, re, unicodedata\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def _ensure_list(x):\n",
    "        if x is None: return []\n",
    "        if isinstance(x, (list, tuple, set)): return list(x)\n",
    "        return [x]\n",
    "\n",
    "    def _dedup(seq):\n",
    "        seen=set(); out=[]\n",
    "        for x in seq:\n",
    "            if x not in seen:\n",
    "                out.append(x); seen.add(x)\n",
    "        return out\n",
    "\n",
    "    def _clean_z(s):\n",
    "        if s is None: return \"\"\n",
    "        s = str(s)\n",
    "        s = unicodedata.normalize(\"NFKC\", s)\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        return s\n",
    "\n",
    "    def _gather_zones(node_dict):\n",
    "        ahead  = [_clean_z(z) for z in _ensure_list(node_dict.get(\"zonename_ahead\", []))]\n",
    "        behind = [_clean_z(z) for z in _ensure_list(node_dict.get(\"zonename_behind\", []))]\n",
    "        return ahead, behind\n",
    "\n",
    "    def _gather_objectids(node_dict):\n",
    "        ids=[]\n",
    "        if isinstance(node_dict, dict):\n",
    "            if \"objectid\"  in node_dict:  ids += _ensure_list(node_dict[\"objectid\"])\n",
    "            if \"objectids\" in node_dict:  ids += _ensure_list(node_dict[\"objectids\"])\n",
    "        return [str(i).strip() for i in ids if i is not None and str(i).strip()!=\"\"]\n",
    "\n",
    "    def _combine_sides(a, b):\n",
    "        # average the two sides if both exist; else take the one that exists\n",
    "        a = np.nan if a is None else a\n",
    "        b = np.nan if b is None else b\n",
    "        if pd.notna(a) and pd.notna(b): return (float(a)+float(b))/2.0\n",
    "        if pd.notna(a): return float(a)\n",
    "        if pd.notna(b): return float(b)\n",
    "        return np.nan\n",
    "\n",
    "    # ---------- normalize input (preserve directions) ----------\n",
    "    def _normalize(aadt_locs):\n",
    "        recs=[]\n",
    "        # accept: dict; list with one giant dict (your pattern); or list of single-loc dicts\n",
    "        items = [aadt_locs] if isinstance(aadt_locs, dict) else (aadt_locs if isinstance(aadt_locs, list) else [])\n",
    "        for item in items:\n",
    "            if not isinstance(item, dict): continue\n",
    "            if \"nodes\" in item:\n",
    "                # single location object\n",
    "                nm = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "                nodes = item.get(\"nodes\", {}) or {}\n",
    "                oids=[]; dirs=[]\n",
    "                for k,node in nodes.items():\n",
    "                    a,b = _gather_zones(node)\n",
    "                    oids += _gather_objectids(node)\n",
    "                    direction = (_ensure_list(node.get(\"direction\")) or [\"\"])[0] or k\n",
    "                    dirs.append({\"direction\":direction, \"ahead\":a, \"behind\":b})\n",
    "                name_out = f\"{nm} [{','.join(_dedup(oids))}]\" if (include_oid_in_name and oids) else nm\n",
    "                recs.append({\"name\": name_out, \"daytype\": item.get(\"daytype\", daytype_filter), \"dirs\": dirs})\n",
    "            else:\n",
    "                # giant dict keyed by location names\n",
    "                for nm,loc in item.items():\n",
    "                    nodes = loc.get(\"nodes\", {}) or {}\n",
    "                    oids=[]; dirs=[]\n",
    "                    for k,node in nodes.items():\n",
    "                        a,b = _gather_zones(node)\n",
    "                        oids += _gather_objectids(node)\n",
    "                        direction = (_ensure_list(node.get(\"direction\")) or [\"\"])[0] or k\n",
    "                        dirs.append({\"direction\":direction, \"ahead\":a, \"behind\":b})\n",
    "                    name_out = f\"{nm} [{','.join(_dedup(oids))}]\" if (include_oid_in_name and oids) else nm\n",
    "                    recs.append({\"name\": name_out, \"daytype\": loc.get(\"daytype\", daytype_filter), \"dirs\": dirs})\n",
    "        return recs\n",
    "\n",
    "    # ---------- filter STL + precompute per-zonename means ----------\n",
    "    must = {zonename_col, stl_volume_col, \"daytype\", \"daypart\"}\n",
    "    miss = [c for c in must if c not in df_stl.columns]\n",
    "    if miss:\n",
    "        raise KeyError(f\"df_stl missing columns: {miss}\")\n",
    "\n",
    "    filt = (df_stl[\"daytype\"]==daytype_filter) & (df_stl[\"daypart\"]==daypart_filter)\n",
    "    if modeoftravel_filter and (\"modeoftravel\" in df_stl.columns):\n",
    "        filt = filt & (df_stl[\"modeoftravel\"]==modeoftravel_filter)\n",
    "\n",
    "    stl = df_stl.loc[filt, [zonename_col, stl_volume_col]].copy()\n",
    "    stl[zonename_col] = stl[zonename_col].map(_clean_z)\n",
    "    stl[stl_volume_col] = pd.to_numeric(stl[stl_volume_col], errors=\"coerce\")\n",
    "\n",
    "    zone_group = stl.groupby(zonename_col)[stl_volume_col]\n",
    "    zone_mean  = zone_group.mean()   # per-zonename AADT mean\n",
    "    zone_rows  = zone_group.size()   # row counts backing that mean\n",
    "    present    = set(zone_mean.index)\n",
    "\n",
    "    def _side_val(zones, agg_local=\"sum\", dedup_mode=segment_count_mode):\n",
    "        # clean → (dedup if requested) → split present/missing → aggregate\n",
    "        zs = [_clean_z(z) for z in _ensure_list(zones) if _clean_z(z)]\n",
    "        zs = _dedup(zs) if dedup_mode==\"unique\" else zs\n",
    "        if not zs: return (np.nan, 0, [], 0, 0)\n",
    "        got  = [z for z in zs if z in present]\n",
    "        miss = [z for z in zs if z not in present]\n",
    "        vals = zone_mean.reindex(got).dropna()\n",
    "        if not len(vals): return (np.nan, 0, miss, 0, len(zs))\n",
    "        val  = float(vals.sum()) if agg_local==\"sum\" else float(vals.mean())\n",
    "        nrows= int(zone_rows.reindex(got).fillna(0).sum())\n",
    "        return (val, nrows, miss, len(got), len(zs))\n",
    "\n",
    "    # ---------- compute per-location ----------\n",
    "    rows=[]\n",
    "    for rec in _normalize(aadt_locations):\n",
    "        loc_name = rec[\"name\"]\n",
    "        loc_day  = rec.get(\"daytype\", daytype_filter)\n",
    "\n",
    "        # unions for legacy/readability\n",
    "        ahead_union=[]; behind_union=[]\n",
    "        ahead_rows_total=0; behind_rows_total=0\n",
    "        listed_ahead_total=0; listed_behind_total=0\n",
    "        present_ahead_total=0; present_behind_total=0\n",
    "        miss_a_all=[]; miss_b_all=[]\n",
    "\n",
    "        dir_totals=[]; dir_dbg=[]; dir_counts_dbg=[]; dir_missing_dbg=[]\n",
    "\n",
    "        for d in rec.get(\"dirs\", []):\n",
    "            a_val, a_n, a_miss, a_present, a_listed = _side_val(d[\"ahead\"], agg)\n",
    "            b_val, b_n, b_miss, b_present, b_listed = _side_val(d[\"behind\"], agg)\n",
    "\n",
    "            dir_total = _combine_sides(a_val, b_val)  # per-direction total (avg of sides)\n",
    "            dir_totals.append(dir_total)\n",
    "            dir_dbg.append(f\"{d['direction']}={'' if pd.isna(dir_total) else round(dir_total,3)}\")\n",
    "            dir_counts_dbg.append(f\"{d['direction']}(ahead {a_present}/{a_listed}, behind {b_present}/{b_listed})\")\n",
    "            dir_missing_dbg.append(\n",
    "                f\"{d['direction']}_ahead_missing={','.join(_dedup(a_miss))};\"\n",
    "                f\"{d['direction']}_behind_missing={','.join(_dedup(b_miss))}\"\n",
    "            )\n",
    "\n",
    "            ahead_union  += _ensure_list(d[\"ahead\"])\n",
    "            behind_union += _ensure_list(d[\"behind\"])\n",
    "            ahead_rows_total  += a_n\n",
    "            behind_rows_total += b_n\n",
    "            listed_ahead_total  += a_listed\n",
    "            listed_behind_total += b_listed\n",
    "            present_ahead_total += a_present\n",
    "            present_behind_total+= b_present\n",
    "            if a_miss: miss_a_all += a_miss\n",
    "            if b_miss: miss_b_all += b_miss\n",
    "\n",
    "        # KEY: location total is SUM of directional totals (NB + SB)\n",
    "        if any(pd.notna(x) for x in dir_totals):\n",
    "            loc_total = float(pd.Series(dir_totals).dropna().sum())\n",
    "        else:\n",
    "            loc_total = np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"location\": loc_name,\n",
    "            \"daytype_expected\": loc_day,\n",
    "            \"daytype_used\": daytype_filter,\n",
    "            \"daypart_used\": daypart_filter,\n",
    "            \"modeoftravel_used\": modeoftravel_filter or \"\",\n",
    "\n",
    "            # deduped unions across directions\n",
    "            \"ahead_zones\": \"|\".join(_dedup([z for z in ahead_union if z])),\n",
    "            \"behind_zones\": \"|\".join(_dedup([z for z in behind_union if z])),\n",
    "\n",
    "            # leave side means blank (optional to compute later)\n",
    "            \"non_trad_ahead_mean\": np.nan,\n",
    "            \"non_trad_behind_mean\": np.nan,\n",
    "\n",
    "            # TOTAL non-traditional AADT for the location (NB + SB)\n",
    "            \"non_trad_aadt\": loc_total,\n",
    "\n",
    "            # backing row counts (sum across dirs)\n",
    "            \"stl_ahead_rows\": ahead_rows_total,\n",
    "            \"stl_behind_rows\": behind_rows_total,\n",
    "\n",
    "            # segment counts (sum across dirs)\n",
    "            \"listed_ahead_segments\": listed_ahead_total,\n",
    "            \"listed_behind_segments\": listed_behind_total,\n",
    "            \"present_ahead_segments\": present_ahead_total,\n",
    "            \"present_behind_segments\": present_behind_total,\n",
    "\n",
    "            # missing (union across dirs)\n",
    "            \"missing_ahead_zones\": \"|\".join(_dedup(miss_a_all)),\n",
    "            \"missing_behind_zones\": \"|\".join(_dedup(miss_b_all)),\n",
    "\n",
    "            # debug: per-direction totals & coverage\n",
    "            # (name kept for backward-compat even though these are totals)\n",
    "            \"non_trad_dir_means\": \"|\".join(dir_dbg),\n",
    "            \"dir_present_counts\": \"|\".join(dir_counts_dbg),\n",
    "            \"dir_missing_zones\": \"|\".join(dir_missing_dbg),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows) if as_df else rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2bffe53-019a-424d-99bd-6880d47e4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will run the \"non_traditional_aadt_by_location\" function if  you have the raw nested structure:\n",
    "stl_df = non_traditional_aadt_by_location(\n",
    "    aadt_locations,\n",
    "    df_stl,\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=\"All Vehicles - StL All Vehicles Volume\",  # or None\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\",\n",
    "    as_df=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1054eca1-bde9-4b77-881f-3faf0f052fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export step 2 to a CSV\n",
    "stl_df.to_csv(\"step_2_non_traditional_aadt_by_location_99_d3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11dbbc8-84a8-4a7c-bca5-5b89aee3259a",
   "metadata": {},
   "source": [
    "### Step 3, Build the per-location comparison DataFrame\n",
    "* **Joining the base location**, not the raw location\n",
    "\n",
    "-----------------------\n",
    "\n",
    "* Build inputs with traditional_aadt_by_location(...) and non_traditional_aadt_by_location(...).\n",
    "* Normalize names to a **base location** by stripping trailing [...] objectids.\n",
    "* **Merge on** location_base (merge_how=\"inner\" by default; pass \"outer\" to keep one-sided locations).\n",
    "* Unify suffixed columns so downstream aggregation sees a consistent schema.\n",
    "* Collapse duplicates by location_base:\n",
    "    * Traditional metrics: **mean** across duplicate rows.\n",
    "    * StreetLight per-direction metrics: **sum** across duplicate rows.\n",
    "    * Strings (zones, missing): **stable unique union.\n",
    "    * IDs: **extract digits** and **stable-unique join** with \"|\"; recompute n_objectids.\n",
    "* Recompute StreetLight overall: non_trad_aadt = avg(non_trad_ahead_sum, non_trad_behind_sum) (or the lone side if one is missing).\n",
    "* Compute tce_percent = 100 * (non_trad_aadt - traditional_aadt) / traditional_aadt.\n",
    "* Return a tidy, consistent column order, with display location set to the base name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec234022-6cff-4b8f-90c7-ed0f67a0bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# 3) Build the per-location comparison DataFrame\n",
    "# ------------------------------------------------------\n",
    "\n",
    "def build_aadt_comparison_df(\n",
    "    aadt_locations,\n",
    "    df_tc,\n",
    "    df_stl,\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=None,\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\",\n",
    "    merge_how=\"inner\",  # \"outer\" to keep one-sided locations\n",
    "):\n",
    "    \"\"\"\n",
    "    Step 4 (concise, pseudo-code style)\n",
    "\n",
    "    # plan\n",
    "    # 1) build traditional side + non-traditional side\n",
    "    # 2) merge on location_base (strip \"[...]\" OIDs)\n",
    "    # 3) unify column names; aggregate\n",
    "    # 4) prefer Step 3's non_trad_aadt; only backfill from side means if missing\n",
    "    # 5) compute TCE\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # -- helpers --------------------------------------------------------------\n",
    "    def _base_location_local(s: str) -> str:\n",
    "        # drop trailing \"[...]\" OID decoration\n",
    "        if not isinstance(s, str): \n",
    "            return \"\"\n",
    "        return re.sub(r\"\\s*\\[[^\\]]+\\]\\s*$\", \"\", s).strip()\n",
    "\n",
    "    def _pick_col_local(df: pd.DataFrame, base: str):\n",
    "        # pick the first present variant\n",
    "        if base in df.columns:\n",
    "            return base\n",
    "        for suf in (\"_trad\", \"_nt\", \"_x\", \"_y\"):\n",
    "            c = f\"{base}{suf}\"\n",
    "            if c in df.columns:\n",
    "                return c\n",
    "        return None\n",
    "\n",
    "    def _unify_cols(df, bases):\n",
    "        # copy any found variant back into the base name\n",
    "        for base in bases:\n",
    "            c = _pick_col_local(df, base)\n",
    "            if c and c != base:\n",
    "                df[base] = df[c]\n",
    "\n",
    "    def uniq_join(series):\n",
    "        # pipe-join unique tokens across rows\n",
    "        seen=set(); out=[]\n",
    "        for s in series.dropna().astype(str):\n",
    "            for tok in str(s).split(\"|\"):\n",
    "                tok = tok.strip()\n",
    "                if tok and tok not in seen:\n",
    "                    seen.add(tok); out.append(tok)\n",
    "        return \"|\".join(out)\n",
    "\n",
    "    def join_objectids(series):\n",
    "        # pull digit runs, dedupe, pipe-join\n",
    "        toks=[]\n",
    "        for s in series.astype(str).fillna(\"\"):\n",
    "            toks.extend(re.findall(r\"\\d+\", s))\n",
    "        out=[]; seen=set()\n",
    "        for t in toks:\n",
    "            if t not in seen:\n",
    "                seen.add(t); out.append(t)\n",
    "        return \"|\".join(out)\n",
    "\n",
    "    # -- 1) build sides -------------------------------------------------------\n",
    "    trad_df = traditional_aadt_by_location(\n",
    "        aadt_locations=aadt_locations,\n",
    "        df_tc=df_tc,\n",
    "        as_df=True\n",
    "    )\n",
    "    nt_df = non_traditional_aadt_by_location(\n",
    "        aadt_locations=aadt_locations,\n",
    "        df_stl=df_stl,\n",
    "        daytype_filter=daytype_filter,\n",
    "        daypart_filter=daypart_filter,\n",
    "        modeoftravel_filter=modeoftravel_filter,\n",
    "        zonename_col=zonename_col,\n",
    "        stl_volume_col=stl_volume_col,\n",
    "        as_df=True\n",
    "    )\n",
    "\n",
    "    # add base names (strip \"[...]\" to guarantee merges)\n",
    "    trad_df[\"location_base\"] = trad_df[\"location\"].apply(_base_location_local)\n",
    "    nt_df[\"location_base\"]   = nt_df[\"location\"].apply(_base_location_local)\n",
    "\n",
    "    # -- 2) merge -------------------------------------------------------------\n",
    "    merged = pd.merge(\n",
    "        trad_df,\n",
    "        nt_df,\n",
    "        how=merge_how,\n",
    "        on=\"location_base\",\n",
    "        suffixes=(\"_trad\", \"_nt\")\n",
    "    )\n",
    "\n",
    "    # unify expected columns (pull _trad/_nt into base)\n",
    "    _unify_cols(merged, [\n",
    "        # ids/meta\n",
    "        \"objectids\",\"n_objectids\",\"n_found_in_tc\",\n",
    "        \"daytype\",\"daytype_expected\",\"daytype_used\",\"daypart_used\",\"modeoftravel_used\",\n",
    "        # zones & missing\n",
    "        \"ahead_zones\",\"behind_zones\",\"missing_ahead_zones\",\"missing_behind_zones\",\n",
    "        # STL counts\n",
    "        \"stl_ahead_rows\",\"stl_behind_rows\",\n",
    "        \"listed_ahead_segments\",\"listed_behind_segments\",\n",
    "        \"present_ahead_segments\",\"present_behind_segments\",\n",
    "        # metrics\n",
    "        \"traditional_ahead_mean\",\"traditional_behind_mean\",\"traditional_aadt\",\n",
    "        \"non_trad_ahead_mean\",\"non_trad_behind_mean\",\"non_trad_aadt\",  # <-- include total\n",
    "    ])\n",
    "\n",
    "    # display name = base\n",
    "    merged[\"location\"] = merged[\"location_base\"]\n",
    "\n",
    "    # -- 3) collapse dup rows by location_base --------------------------------\n",
    "    agg = {\n",
    "        \"traditional_ahead_mean\": \"mean\",\n",
    "        \"traditional_behind_mean\": \"mean\",\n",
    "        \"traditional_aadt\": \"mean\",\n",
    "        \"non_trad_ahead_mean\": \"mean\",     # was \"sum\" -> turned NaN into 0; fixed\n",
    "        \"non_trad_behind_mean\": \"mean\",\n",
    "        \"non_trad_aadt\": \"mean\",           # prefer Step 3 total; average if multiple rows\n",
    "        \"stl_ahead_rows\": \"sum\",\n",
    "        \"stl_behind_rows\": \"sum\",\n",
    "        \"listed_ahead_segments\": \"sum\",\n",
    "        \"listed_behind_segments\": \"sum\",\n",
    "        \"present_ahead_segments\": \"sum\",\n",
    "        \"present_behind_segments\": \"sum\",\n",
    "        \"ahead_zones\": uniq_join,\n",
    "        \"behind_zones\": uniq_join,\n",
    "        \"missing_ahead_zones\": uniq_join,\n",
    "        \"missing_behind_zones\": uniq_join,\n",
    "        \"objectids\": join_objectids,\n",
    "        \"n_objectids\": \"sum\",              # recomputed below anyway\n",
    "        \"n_found_in_tc\": \"sum\",\n",
    "        \"daytype\": \"first\",\n",
    "        \"daytype_expected\": \"first\",\n",
    "        \"daytype_used\": \"first\",\n",
    "        \"daypart_used\": \"first\",\n",
    "        \"modeoftravel_used\": \"first\",\n",
    "    }\n",
    "    agg = {k:v for k,v in agg.items() if k in merged.columns}\n",
    "\n",
    "    out = (merged\n",
    "           .groupby(\"location_base\", as_index=False)\n",
    "           .agg(agg)\n",
    "           .rename(columns={\"location_base\":\"location\"}))\n",
    "\n",
    "    # harden objectids; recompute counts from the pipe string\n",
    "    if \"objectids\" in out.columns:\n",
    "        out[\"objectids\"] = out[\"objectids\"].astype(str)\n",
    "        out[\"n_objectids\"] = out[\"objectids\"].str.split(r\"\\|\").apply(lambda xs: len([t for t in xs if t and t.strip()]))\n",
    "\n",
    "    # -- 4) prefer Step 3 non_trad_aadt, backfill from sides if missing -------\n",
    "    if {\"non_trad_ahead_mean\",\"non_trad_behind_mean\"}.issubset(out.columns):\n",
    "        if \"non_trad_aadt\" not in out.columns:\n",
    "            out[\"non_trad_aadt\"] = np.nan\n",
    "        out[\"non_trad_aadt\"] = pd.to_numeric(out[\"non_trad_aadt\"], errors=\"coerce\")\n",
    "\n",
    "        a = pd.to_numeric(out[\"non_trad_ahead_mean\"], errors=\"coerce\")\n",
    "        b = pd.to_numeric(out[\"non_trad_behind_mean\"], errors=\"coerce\")\n",
    "        comp = np.where(a.notna() & b.notna(), (a + b) / 2.0,\n",
    "                        np.where(a.notna(), a, b))\n",
    "        # only fill where total is missing\n",
    "        out[\"non_trad_aadt\"] = out[\"non_trad_aadt\"].where(out[\"non_trad_aadt\"].notna(), comp)\n",
    "\n",
    "    # -- 5) TCE ---------------------------------------------------------------\n",
    "    if {\"traditional_aadt\",\"non_trad_aadt\"}.issubset(out.columns):\n",
    "        T = pd.to_numeric(out[\"traditional_aadt\"], errors=\"coerce\")\n",
    "        N = pd.to_numeric(out[\"non_trad_aadt\"], errors=\"coerce\")\n",
    "        out[\"tce_percent\"] = np.where(T.notna() & (T != 0) & N.notna(), 100.0*(N - T)/T, np.nan)\n",
    "\n",
    "    # -- 6) column order ------------------------------------------------------\n",
    "    preferred_cols = [\n",
    "        \"location\",\n",
    "        \"objectids\",\"n_objectids\",\"n_found_in_tc\",\n",
    "        \"ahead_zones\",\"behind_zones\",\n",
    "        \"traditional_ahead_mean\",\"traditional_behind_mean\",\"traditional_aadt\",\n",
    "        \"non_trad_ahead_mean\",\"non_trad_behind_mean\",\"non_trad_aadt\",\n",
    "        \"tce_percent\",\n",
    "        \"daytype\",\"daytype_expected\",\"daytype_used\",\"daypart_used\",\"modeoftravel_used\",\n",
    "        \"stl_ahead_rows\",\"stl_behind_rows\",\n",
    "        \"listed_ahead_segments\",\"listed_behind_segments\",\n",
    "        \"present_ahead_segments\",\"present_behind_segments\",\n",
    "        \"missing_ahead_zones\",\"missing_behind_zones\",\n",
    "    ]\n",
    "    cols = [c for c in preferred_cols if c in out.columns]\n",
    "    return out[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f183dac9-4407-491a-8e88-76339267e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1, Build the combined comparison DataFrame\n",
    "cmp_df = build_aadt_comparison_df(\n",
    "    aadt_locations=aadt_locations,  # your dict/list structure\n",
    "    df_tc=df_tc,                                 # Traffic Census dataframe\n",
    "    df_stl=df_stl,                               # StreetLight dataframe\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=None,                    # e.g., \"0: All Modes\" if you need it\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b5789ef-bea8-4369-9f79-b8ff23aef61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2, (Optional) sort by absolute TCE to see big deltas first\n",
    "cmp_df = cmp_df.sort_values(\"tce_percent\", key=lambda s: s.abs(), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfd90d29-5647-4cf8-ac6a-911e4cea503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3, Export to CSV \n",
    "cmp_df.to_csv(\"step_3_comparison_dataframe_99_d3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0c8ed-8d3b-41d4-a68e-77daf218d4fc",
   "metadata": {},
   "source": [
    "### Step 4 Collapse to one row per location\n",
    "\n",
    "* **Goal**: Produce one row per physical location and compute the CI on TCE in a single pass.\n",
    "\n",
    "-----------------------\n",
    "\n",
    "* **How it collapses**:\n",
    "    * Build a base location key by stripping trailing bracketed IDs\n",
    "    * Traditional AADT metrics\n",
    "    * Non-traditional side fields (non_trad_ahead_mean, non_trad_behind_mean): sum with NaN-safety (uses sum(min_count=1) so all-NaN stays NaN—no accidental zeros).\n",
    "    * non_trad_aadt (the total): prefer the total from Step 4 (already NB+SB). If missing, backfill as the average of the side sums (or the lone side if only one exists).\n",
    "    * Counts (stl_*_rows, listed_*_segments, present_*_segments): sum across duplicates.\n",
    "    * String fields (zones, missing lists): stable unique union (pipe-joined).\n",
    "    * ObjectIDs: extract digits only, stable-dedupe, pipe-join; recompute n_objectids.\n",
    "    \n",
    "* **Then recompute**:\n",
    "    * tce_percent = 100 * (non_trad_aadt − traditional_aadt) / traditional_aadt, guarding divide-by-zero/NaN.\n",
    "\n",
    "* ** Confidence Interval**\n",
    "    * Build a clean 1-D series of tce_percent (drop NaN/±∞).\n",
    "    * Optional filters:\n",
    "        * cap_abs: drop rows with |tce| > cap_abs.\n",
    "        * winsor_pct: winsorize tails (e.g., 0.01 → 1% each tail).\n",
    "    * Compute mean TCE, std, SE, t-critical, 95% CI, t-statistic, p-value (two-sided), Cohen’s d.\n",
    "    * Returns a one-row summary (ci_summary) plus the collapsed detail (detail_df). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "885213e0-d79b-4ae0-8718-17e5b8e73ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# 4) Collapse to one row per location & Run the Confidence Interval\n",
    "# ------------------------------------------------------\n",
    "\n",
    "def collapse_and_ci(\n",
    "    cmp_df,\n",
    "    confidence=0.95,\n",
    "    tce_col=\"tce_percent\",\n",
    "    cap_abs=None,          # e.g., 500 caps |tce|\n",
    "    winsor_pct=None        # e.g., 0.01 winsorizes 1% tails\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Make a copy; derive base location name (strip trailing \"[ids]\") -> 'location_clean'.\n",
    "    2) Group by 'location_clean' with safe aggregations:\n",
    "       - traditional_* : mean (should match)\n",
    "       - non_trad_*_mean : SUM with min_count=1 (all-NaN -> NaN, not 0)\n",
    "       - non_trad_aadt : mean (prefer totals from Step 4; dups should agree)\n",
    "       - counts : sum; strings : union pipe-join; objectids : extract digits + dedupe\n",
    "    3) Recompute non_trad_aadt ONLY where missing:\n",
    "       - backfill from side sums: avg(ahead_sum, behind_sum) or lone side\n",
    "    4) Compute TCE = 100*(N - T)/T (guard divide-by-zero/NaN).\n",
    "    5) Build one-row CI summary over tce_col with optional cap/winsorization.\n",
    "    6) Return (detail_df, ci_summary_df).\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from scipy import stats\n",
    "\n",
    "    df = cmp_df.copy()\n",
    "\n",
    "    # -- 1) base location (defensive even if already clean) -------------------\n",
    "    def _base_location(s: str) -> str:\n",
    "        if not isinstance(s, str): return \"\"\n",
    "        return re.sub(r\"\\s*\\[[^\\]]+\\]\\s*$\", \"\", s).strip()\n",
    "\n",
    "    df[\"location_clean\"] = df[\"location\"].astype(str).map(_base_location)\n",
    "\n",
    "    # -- helpers ---------------------------------------------------------------\n",
    "    def uniq_join(series):\n",
    "        seen, out = set(), []\n",
    "        for s in series.dropna().astype(str):\n",
    "            for tok in s.split(\"|\"):\n",
    "                tok = tok.strip()\n",
    "                if tok and tok not in seen:\n",
    "                    seen.add(tok); out.append(tok)\n",
    "        return \"|\".join(out)\n",
    "\n",
    "    def join_objectids(series):\n",
    "        toks = []\n",
    "        for s in series.astype(str).fillna(\"\"):\n",
    "            toks.extend(re.findall(r\"\\d+\", s))\n",
    "        out, seen = [], set()\n",
    "        for t in toks:\n",
    "            if t not in seen:\n",
    "                seen.add(t); out.append(t)\n",
    "        return \"|\".join(out)\n",
    "\n",
    "    def sum_min1(s):\n",
    "        # numeric sum but return NaN if ALL are NaN\n",
    "        return pd.to_numeric(s, errors=\"coerce\").sum(min_count=1)\n",
    "\n",
    "    # -- 2) collapse to one row per location ----------------------------------\n",
    "    agg = {\n",
    "        # traditional: mean\n",
    "        \"traditional_ahead_mean\": \"mean\",\n",
    "        \"traditional_behind_mean\": \"mean\",\n",
    "        \"traditional_aadt\": \"mean\",\n",
    "\n",
    "        # non-trad side fields: sum with NaN safety\n",
    "        \"non_trad_ahead_mean\": sum_min1,\n",
    "        \"non_trad_behind_mean\": sum_min1,\n",
    "\n",
    "        # prefer totals from Step 4; dups should be equal → mean is fine\n",
    "        \"non_trad_aadt\": \"mean\",\n",
    "\n",
    "        # counts\n",
    "        \"stl_ahead_rows\": \"sum\",\n",
    "        \"stl_behind_rows\": \"sum\",\n",
    "        \"listed_ahead_segments\": \"sum\",\n",
    "        \"listed_behind_segments\": \"sum\",\n",
    "        \"present_ahead_segments\": \"sum\",\n",
    "        \"present_behind_segments\": \"sum\",\n",
    "\n",
    "        # strings/meta\n",
    "        \"ahead_zones\": uniq_join,\n",
    "        \"behind_zones\": uniq_join,\n",
    "        \"missing_ahead_zones\": uniq_join,\n",
    "        \"missing_behind_zones\": uniq_join,\n",
    "        \"daytype\": \"first\",\n",
    "        \"daytype_expected\": \"first\",\n",
    "        \"daytype_used\": \"first\",\n",
    "        \"daypart_used\": \"first\",\n",
    "        \"modeoftravel_used\": \"first\",\n",
    "\n",
    "        # ids\n",
    "        \"objectids\": join_objectids,\n",
    "        \"n_objectids\": \"sum\",\n",
    "        \"n_found_in_tc\": \"sum\",\n",
    "    }\n",
    "    agg = {k:v for k,v in agg.items() if k in df.columns}\n",
    "\n",
    "    detail = (df\n",
    "              .groupby(\"location_clean\", as_index=False)\n",
    "              .agg(agg)\n",
    "              .rename(columns={\"location_clean\":\"location\"}))\n",
    "\n",
    "    # recompute n_objectids from joined string (truth source)\n",
    "    if \"objectids\" in detail.columns:\n",
    "        detail[\"objectids\"] = detail[\"objectids\"].astype(str)\n",
    "        detail[\"n_objectids\"] = detail[\"objectids\"].str.split(r\"\\|\").apply(\n",
    "            lambda xs: len([t for t in xs if t and t.strip()])\n",
    "        )\n",
    "\n",
    "    # -- 3) backfill non_trad_aadt ONLY if missing ----------------------------\n",
    "    if {\"non_trad_ahead_mean\",\"non_trad_behind_mean\"}.issubset(detail.columns):\n",
    "        if \"non_trad_aadt\" not in detail.columns:\n",
    "            detail[\"non_trad_aadt\"] = np.nan\n",
    "        detail[\"non_trad_aadt\"] = pd.to_numeric(detail[\"non_trad_aadt\"], errors=\"coerce\")\n",
    "\n",
    "        a = pd.to_numeric(detail[\"non_trad_ahead_mean\"], errors=\"coerce\")\n",
    "        b = pd.to_numeric(detail[\"non_trad_behind_mean\"], errors=\"coerce\")\n",
    "        comp = np.where(a.notna() & b.notna(), (a + b) / 2.0,\n",
    "                        np.where(a.notna(), a, b))\n",
    "        detail[\"non_trad_aadt\"] = detail[\"non_trad_aadt\"].where(detail[\"non_trad_aadt\"].notna(), comp)\n",
    "\n",
    "    # -- 4) TCE ----------------------------------------------------------------\n",
    "    if {\"traditional_aadt\",\"non_trad_aadt\"}.issubset(detail.columns):\n",
    "        T = pd.to_numeric(detail[\"traditional_aadt\"], errors=\"coerce\")\n",
    "        N = pd.to_numeric(detail[\"non_trad_aadt\"], errors=\"coerce\")\n",
    "        detail[tce_col] = np.where(T.notna() & (T != 0) & N.notna(), 100.0*(N - T)/T, np.nan)\n",
    "\n",
    "    # -- 5) CI summary over tce_col -------------------------------------------\n",
    "    tces = pd.to_numeric(detail[tce_col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    # optional absolute cap\n",
    "    if cap_abs is not None and len(tces) > 0:\n",
    "        tces = tces[np.abs(tces) <= float(cap_abs)]\n",
    "\n",
    "    # optional winsorization\n",
    "    if winsor_pct is not None and 0 < winsor_pct < 0.5 and len(tces) > 0:\n",
    "        lo = float(tces.quantile(winsor_pct))\n",
    "        hi = float(tces.quantile(1 - winsor_pct))\n",
    "        tces = tces.clip(lower=lo, upper=hi)\n",
    "\n",
    "    n = int(tces.shape[0])\n",
    "    if n == 0:\n",
    "        ci_df = pd.DataFrame([{\n",
    "            \"confidence\": confidence,\n",
    "            \"tce_col\": tce_col,\n",
    "            \"n\": 0, \"dof\": None,\n",
    "            \"mean_tce\": None, \"std_tce\": None, \"se\": None,\n",
    "            \"t_critical\": None, \"margin_of_error\": None,\n",
    "            \"ci_lower\": None, \"ci_upper\": None,\n",
    "            \"t_statistic\": None, \"p_value_two_sided\": None, \"cohens_d\": None,\n",
    "            \"cap_abs\": cap_abs, \"winsor_pct\": winsor_pct\n",
    "        }])\n",
    "        return detail, ci_df\n",
    "\n",
    "    mean_tce = float(tces.mean())\n",
    "    if n > 1:\n",
    "        std_tce = float(tces.std(ddof=1))\n",
    "        se = std_tce / np.sqrt(n) if std_tce > 0 else 0.0\n",
    "        dof = n - 1\n",
    "        if se > 0:\n",
    "            tcrit = float(stats.t.ppf((1 + confidence) / 2.0, dof))\n",
    "            moe = tcrit * se\n",
    "            ci_lo, ci_hi = mean_tce - moe, mean_tce + moe\n",
    "            t_stat = mean_tce / se\n",
    "            p_val = float(2 * (1 - stats.t.cdf(abs(t_stat), dof)))\n",
    "            cohens_d = mean_tce / std_tce if std_tce > 0 else None\n",
    "        else:\n",
    "            tcrit = moe = ci_lo = ci_hi = t_stat = p_val = cohens_d = None\n",
    "    else:\n",
    "        std_tce = se = None\n",
    "        dof = None\n",
    "        tcrit = moe = ci_lo = ci_hi = t_stat = p_val = cohens_d = None\n",
    "\n",
    "    ci_df = pd.DataFrame([{\n",
    "        \"confidence\": confidence,\n",
    "        \"tce_col\": tce_col,\n",
    "        \"n\": n, \"dof\": dof,\n",
    "        \"mean_tce\": mean_tce, \"std_tce\": std_tce, \"se\": se,\n",
    "        \"t_critical\": tcrit, \"margin_of_error\": moe,\n",
    "        \"ci_lower\": ci_lo, \"ci_upper\": ci_hi,\n",
    "        \"t_statistic\": t_stat, \"p_value_two_sided\": p_val, \"cohens_d\": cohens_d,\n",
    "        \"cap_abs\": cap_abs, \"winsor_pct\": winsor_pct\n",
    "    }])\n",
    "\n",
    "    return detail, ci_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b57e76f8-cd1f-4d24-9403-2fff9d193dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1, Run the Confidence Interval\n",
    "detail_df, ci_summary = collapse_and_ci(cmp_df, confidence=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64359071-3d47-4266-9c37-440c71bb405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   confidence      tce_col   n  dof  mean_tce    std_tce        se  \\\n",
      "0        0.95  tce_percent  49   48 -2.758799  22.203273  3.171896   \n",
      "\n",
      "   t_critical  margin_of_error  ci_lower  ci_upper  t_statistic  \\\n",
      "0    2.010635         6.377525 -9.136323  3.618726    -0.869763   \n",
      "\n",
      "   p_value_two_sided  cohens_d cap_abs winsor_pct  \n",
      "0           0.388759 -0.124252    None       None  \n"
     ]
    }
   ],
   "source": [
    "# 4.2, View the results of the confidence interval\n",
    "print(ci_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28aa3c47-86df-4bd4-9cbc-5799b8b3f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Export to CSV \n",
    "detail_df.to_csv(\"step_4_summary_99_d3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff95b8b-74a8-4528-bc17-3997dc54e7a5",
   "metadata": {},
   "source": [
    "## Step 5, Analyst's Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9dd12ad9-02e5-4a93-b3dd-b96ab93205c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean TCE: -2.759\n",
      "95% Confidence Interval: ('-9.136', '3.619')\n",
      "t-test statistic: -0.870\n",
      "t-critical: 2.011\n"
     ]
    }
   ],
   "source": [
    "r = ci_summary.iloc[0]  # one-row summary\n",
    "\n",
    "mean_tce     = r[\"mean_tce\"]\n",
    "ci_lower     = r[\"ci_lower\"]\n",
    "ci_upper     = r[\"ci_upper\"]\n",
    "t_critical   = r[\"t_critical\"]\n",
    "t_statistic  = r[\"t_statistic\"]\n",
    "\n",
    "# pretty print (guard NaNs)\n",
    "fmt = lambda x: \"NA\" if pd.isna(x) else f\"{x:.3f}\"\n",
    "print(\"Mean TCE:\", fmt(mean_tce))\n",
    "print(\"95% Confidence Interval:\", (fmt(ci_lower), fmt(ci_upper)))\n",
    "print(\"t-test statistic:\", fmt(t_statistic))\n",
    "print(\"t-critical:\", fmt(t_critical))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7a389-e31e-43a2-9cf4-ccb7e7eec747",
   "metadata": {},
   "source": [
    "### Mean TCE: -2.134\n",
    "Traffic Count Error (TCE)\n",
    "* On average across SR-99 (District 3), Non-Traditional AADT (StreetLight) is about 2.1% lower than Traditional AADT (Traffic Census).\n",
    "\n",
    "### 95% Confidence Interval (-11.3%, 7.1%)\n",
    "* We’re 95% confident the true average difference lies between 11.3% lower and 7.1% higher.\n",
    "* Because this range includes zero, there’s no clear corridor-level bias—StreetLight isn’t consistently above or below Traffic Census on average.\n",
    "\n",
    "### T-Test Statistic (-0.524) \n",
    "* The observed average difference is ~0.52 standard errors below zero (t == −0.524), far from the ±2 threshold for 95% significance.\n",
    "\n",
    "### Summary\n",
    "* On SR-99 (District 3), StreetLight AADT averages about 3.6% lower than the Traffic Census.\n",
    "* With 95% confidence, the true average difference could be anywhere from 10.3% lower to 3.2% higher.\n",
    "* Because that range includes zero and the t-stat is −1.06 (well below the ±2 needed at 95%), the average difference is not statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9a728-020e-4224-850c-00b00eb03eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
