{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826e200c-086a-4cd6-962b-0a3d41ee4e11",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AADT Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f1a47-13bc-413e-bf96-bc0f40580be1",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## FHWA Links\n",
    "* Guidelines for Obtaining AADT Estimates from Non-Traditional Sources:\n",
    "    * https://www.fhwa.dot.gov/policyinformation/travel_monitoring/pubs/aadtnt/Guidelines_for_AADT_Estimates_Final.pdf\n",
    "\n",
    "---\n",
    "  \n",
    "## AADT Analysis Locations\n",
    "* 10 locations were used in the analysis\n",
    "* Locations were determined based on the location on installed & recording Traffic Operations cameras\n",
    "    * for additional information contact Zhenyu Zhu with Traffic Operations\n",
    "\n",
    "## Traffic Census Data\n",
    "* https://dot.ca.gov/programs/traffic-operations/census/traffic-volumes\n",
    "* Back AADT, Peak Month, and Peak Hour usually represents traffic South or West of the count location.  \n",
    "* Ahead AADT, Peak Month, and Peak Hour usually represents traffic North or East of the count location. Listing of routes with their designated  \n",
    "\n",
    "* Because the Back & Ahead counts are included at each location in the Traffic Census Data, (e.g., \"IRWINDALE, ARROW HIGHWAY\") only one [OBJECTID*] per location was pulled; for this analysis the North Bound Nodes were used for the analysis. \n",
    "    * for more information see the diagram: https://traffic.onramp.dot.ca.gov/downloads/traffic/files/performance/census/Back_and_Ahead_Leg_Traffic_Count_Diagram.pdf\n",
    "\n",
    "## StreetLight Analysis Data\n",
    "* Analysis Type == Network Performance\n",
    "* Segment Metrics\n",
    "* 2022 was used to match currently available Traffic Census Data (as of 8/27/2025)\n",
    "* pulled a variety of Day Types, but plan to just look at \"\"\"All Day Types\"\"\"\n",
    "* pulled a variety of Day Parts, but plan to just look at \"\"\"All Day Parts\"\"\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e4b06-1d4d-40f3-b913-f3664001d3bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How this notebook estimates StreetLight vs. Traffic Census differences\n",
    "\n",
    "**What we’re trying to answer:**  \n",
    "Across selected corridor locations, is the Non-Traditional AADT generally higher or lower than Traffic Census (aka Traditional) AADT, and by how much? We also show how certain we are about that average difference.\n",
    "\n",
    "---\n",
    "\n",
    "### The data we use\n",
    "- **Traffic Census (TC):** The official counts by location (`objectid`) with two directions: *ahead* and *back*.\n",
    "- **StreetLight (STL):** Volume by road segment (“**zonename**”) with tags like **daytype** (e.g., All Days) and **daypart** (e.g., All Day).\n",
    "- **Location mapping:** For each TC location, a list of the STL zosenames that represent the *ahead* side and the *behind* side of that location.\n",
    "\n",
    "---\n",
    "\n",
    "### How we build one number per location (AADT)\n",
    "1) **Pick the TC value** that matches the counter’s direction:  \n",
    "   - Even `objectid` → use the TC *back* value  \n",
    "   - Odd `objectid` → use the TC *ahead* value  \n",
    "   *(This mirrors the direction convention previously reviewed.)*\n",
    "\n",
    "2) **Filter StreetLight to the same conditions** you care about (usually **All Days** and **All Day**).\n",
    "\n",
    "3) **For each STL zonename**, take the average volume within that filter.  \n",
    "   *(This gives one “typical” value per segment under the chosen daytype/daypart.)*\n",
    "\n",
    "4) **Sum the STL segments for this location**:  \n",
    "   - Add up the “ahead” segments.  \n",
    "   - Add up the “behind” segments.  \n",
    "   - Then add those two sides together.  \n",
    "   *(Result = StreetLight AADT for that location.)*\n",
    "\n",
    "Now each location has:\n",
    "- **TC AADT** (the benchmark)  \n",
    "- **STL AADT** (the estimate from StreetLight)\n",
    "\n",
    "---\n",
    "\n",
    "### Turn those into apples-to-apples differences (TCE, in %)\n",
    "For every location with both numbers:\n",
    "- **Traffic Count Error (TCE)** = the percent difference between STL and TC.  \n",
    "  - Negative TCE → STL is lower than TC.  \n",
    "  - Positive TCE → STL is higher than TC.\n",
    "\n",
    "We collect one TCE value per location.\n",
    "\n",
    "---\n",
    "\n",
    "### Summarize and add a confidence band (CI)\n",
    "- **Average TCE**: the typical over/under across all locations.  \n",
    "- **95% Confidence Interval**: a “margin of error” around that average, based on how much the location-level TCEs vary and how many locations you have.  \n",
    "  - If the interval **crosses 0%**, the average difference isn’t statistically clear (could be slightly above or below zero).  \n",
    "  - If the interval is **entirely below 0%**, STL tends to be lower than TC.  \n",
    "  - If it’s **entirely above 0%**, STL tends to be higher.\n",
    "\n",
    "We also show a **t-statistic** and **p-value** for the “is the average difference basically zero?” question; lower p-values mean a clearer difference.\n",
    "\n",
    "---\n",
    "\n",
    "### What to look for\n",
    "- **The average TCE** (direction and size).  \n",
    "- **Whether the 95% CI includes 0%.**  \n",
    "- **Any locations with missing segments or mismatched data** (these are flagged so you can QA them)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e192bde-dff4-46cf-b2cd-da8347440ec5",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "171fa2b0-9b02-4947-a4d1-dd18ef86de42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import t as student_t  # if SciPy is not available, use a small lookup table\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6bce3a-11a1-4f2a-a7d9-eb7904946817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pull in the coordinates from the utils docs\n",
    "#from osow_frp_o_d_utils_v3 import origin_intersections, destination_intersections\n",
    "import shs_ct_tc_locations_utils as tc_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b47dc-41a0-41ed-9d63-8fe73c9c0549",
   "metadata": {},
   "source": [
    "### Identify the corridor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b6f8329-3a2d-49ec-ba88-cc69b26f6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available corridors\n",
    "    # \"interstate_605_d7_tc_aadt_locations\"\n",
    "    # \"sr_99_d3_tc_aadt_locations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "362e5fc0-0edf-460d-bc30-82e2647c7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the corridor to be analyzed\n",
    "CORRIDOR_VAR_NAME = \"sr_99_d3_tc_aadt_locations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6c6c3b7-87f4-4c6d-b468-50ae993dd498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resolve the object from the module by name\n",
    "try:\n",
    "    aadt_locations = getattr(tc_locs, CORRIDOR_VAR_NAME)\n",
    "except AttributeError:\n",
    "    raise KeyError(\n",
    "        f\"'{CORRIDOR_VAR_NAME}' not found in shs_ct_tc_locations_utils. \"\n",
    "        \"Double-check the variable name.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628fa29-7fdc-46d2-bc5a-84c271f150d9",
   "metadata": {},
   "source": [
    "## Step 0, Pull in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c01657-5051-44da-b79f-594359c06427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will pull in the data and clean the column headers in a way that will make them easier to work with\n",
    "def getdata_and_cleanheaders(path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Clean column headers: remove spaces, convert to lowercase, and strip trailing asterisks\n",
    "    cleaned_columns = []\n",
    "    for column in df.columns:\n",
    "        cleaned_column = column.replace(\" \", \"\").lower().rstrip(\"*\")\n",
    "        cleaned_columns.append(cleaned_column)\n",
    "\n",
    "    df.columns = cleaned_columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0a612-d979-43d3-87f3-e8d69df16d42",
   "metadata": {},
   "source": [
    "### import option 1: Identify the Google Cloud Storage path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce4826a9-4c1c-46e4-95c5-20c820351b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify the GCS path to the data\n",
    "# gcs_path = \"gs://calitp-analytics-data/data-analyses/big_data/compare_traffic_counts/0_2022/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "686ea444-0d68-415c-beda-7ecdfe887062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pull in the data & create dataframes\n",
    "#df_tc = getdata_and_cleanheaders(f\"{gcs_path}caltrans_traffic_census_2022.csv\")  # Traffic Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a890154-e2c9-435a-b136-48d932f3359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify the StreetLight Analysis to be used in the AADT comparison\n",
    "# df_stl = getdata_and_cleanheaders(f\"{gcs_path}streetlight_605_d7_all_vehicles_np_2022.csv\")  # StreetLight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f23b9-689a-4f72-a6a3-f3301ced1d71",
   "metadata": {},
   "source": [
    "### import option 2: Identify the local data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eec02ebb-e675-4ac5-947d-7103ae894902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base data folder: aadt_confidence_interval/aadt_data/2022\n",
    "LOCAL_DATA_DIR = Path.cwd() / \"aadt_data\" / \"2022\"\n",
    "if not LOCAL_DATA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Data folder not found: {LOCAL_DATA_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ef46a3b-f788-4083-8bec-336216431520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Traffic Census (traditional) — local CSV\n",
    "df_tc = getdata_and_cleanheaders(LOCAL_DATA_DIR / \"caltrans_traffic_census_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb87b2f6-ee56-4734-98f7-e7766c6abc31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# StreetLight (non-traditional) — local CSV\n",
    "df_stl = getdata_and_cleanheaders(LOCAL_DATA_DIR / \"streetlight_99_d3_all_vehicles_2022_np.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad4ac9-c278-4566-bc43-114cc6cef077",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ac917f3-c048-43cf-9c13-18907e39df11",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Export to a CSV for viewing/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b73b9a53-0066-4643-b8b7-ec90ceec2f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# comparing\n",
    "df_tc.to_csv(\"df_tc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59259345-ec3c-47e8-bd2d-cf5059babf9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# comparing\n",
    "df_stl.to_csv(\"df_stl.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa10f7c-2eb2-467c-8507-13362af70cc1",
   "metadata": {},
   "source": [
    "## Step 00: Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41ae164e-ae91-4f5d-9c9a-aac9d3cc730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_list(x):\n",
    "    if x is None: return []\n",
    "    if isinstance(x, (list, tuple, set)): return list(x)\n",
    "    return [x]\n",
    "\n",
    "def explode_locations_to_objectids(aadt_locs):\n",
    "    \"\"\"\n",
    "    Returns a list of dicts where each item is ONE objectid with:\n",
    "      name, daytype, objectids [list[str]], ahead_zones [list[str]], behind_zones [list[str]]\n",
    "    This shape is accepted by your existing traditional/non_traditional builders.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    # Case A: \"flat\" list like interstate_605_aadt_locations\n",
    "    if isinstance(aadt_locs, list) and aadt_locs and isinstance(aadt_locs[0], dict) and \"objectid\" in aadt_locs[0]:\n",
    "        for loc in aadt_locs:\n",
    "            oid = str(loc.get(\"objectid\"))\n",
    "            nm  = f\"{loc.get('location_description','UNKNOWN')} [{oid}]\"\n",
    "            day = loc.get(\"daytype\", \"0: All Days (M-Su)\")\n",
    "\n",
    "            ahead, behind = [], []\n",
    "            for k, v in loc.items():\n",
    "                if not k.startswith(\"zonename_\"):\n",
    "                    continue\n",
    "                idx = int(k.split(\"_\")[1])\n",
    "                # assume even indexes (0,2) are \"ahead\"/NB and odd (1,3) are \"behind\"/SB (matches your list)\n",
    "                if idx % 2 == 0: ahead.append(v)\n",
    "                else:            behind.append(v)\n",
    "\n",
    "            rows.append({\n",
    "                \"name\": nm,\n",
    "                \"daytype\": day,\n",
    "                \"objectids\": [oid],\n",
    "                \"ahead_zones\": [z for z in ahead if z],\n",
    "                \"behind_zones\": [z for z in behind if z],\n",
    "            })\n",
    "        return rows\n",
    "\n",
    "    # Case B: nested dict(s) like sr_605_d7_tc_aadt_locations\n",
    "    def _gather_objectids(node):\n",
    "        ids = []\n",
    "        if \"objectid\"  in node: ids.extend(_ensure_list(node[\"objectid\"]))\n",
    "        if \"objectids\" in node: ids.extend(_ensure_list(node[\"objectids\"]))\n",
    "        return [str(i) for i in ids if i is not None and str(i).strip() != \"\"]\n",
    "\n",
    "    if isinstance(aadt_locs, list):\n",
    "        iterable = []\n",
    "        for item in aadt_locs:\n",
    "            if isinstance(item, dict):\n",
    "                iterable.append(item)\n",
    "    elif isinstance(aadt_locs, dict):\n",
    "        iterable = [aadt_locs]\n",
    "    else:\n",
    "        iterable = []\n",
    "\n",
    "    for block in iterable:\n",
    "        for base_name, loc in block.items():\n",
    "            day = loc.get(\"daytype\", \"0: All Days (M-Su)\")\n",
    "            nodes = loc.get(\"nodes\", {}) or {}\n",
    "            for node_name, node in nodes.items():\n",
    "                oids = _gather_objectids(node)\n",
    "                if not oids: continue\n",
    "                nm = f\"{base_name} [{','.join(oids)}]\"\n",
    "\n",
    "                ahead = _ensure_list(node.get(\"zonename_ahead\", []))\n",
    "                behind = _ensure_list(node.get(\"zonename_behind\", []))\n",
    "\n",
    "                rows.append({\n",
    "                    \"name\": nm,\n",
    "                    \"daytype\": day,\n",
    "                    \"objectids\": oids,\n",
    "                    \"ahead_zones\": [z for z in ahead if z],\n",
    "                    \"behind_zones\": [z for z in behind if z],\n",
    "                })\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7480907-d484-4e8f-8e53-eeb4517487ce",
   "metadata": {},
   "source": [
    "## Step 1, Build a per-location summary of Traffic Census locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "316ff939-7384-4f6d-86bc-109a2a6a54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_aadt_by_location(aadt_locations, df_tc, as_df=True, use_parity=False):\n",
    "    \"\"\"\n",
    "    Build a per-location summary of *traditional* (Traffic Census) AADT.\n",
    "\n",
    "    Policy:\n",
    "      • If multiple objectids exist for a location, prefer those whose numeric value is ODD.\n",
    "        If no odd ids exist, fall back to all ids found.\n",
    "      • Default behavior (use_parity=False): for each kept objectid, compute (ahead_aadt + back_aadt)/2,\n",
    "        then average across kept objectids.\n",
    "      • If use_parity=True: even oid -> back_aadt; odd oid -> ahead_aadt (legacy behavior).\n",
    "\n",
    "    Output columns:\n",
    "      location, daytype, objectids, n_objectids, n_found_in_tc, missing_objectids,\n",
    "      traditional_ahead_mean, traditional_behind_mean, traditional_aadt\n",
    "    \"\"\"\n",
    "    # Requires: import pandas as pd; import numpy as np\n",
    "\n",
    "    def _ensure_list(x):\n",
    "        if x is None: return []\n",
    "        if isinstance(x, (list, tuple, set)): return list(x)\n",
    "        return [x]\n",
    "\n",
    "    def _gather_objectids(node_dict):\n",
    "        ids = []\n",
    "        if not isinstance(node_dict, dict): return ids\n",
    "        if \"objectid\"  in node_dict: ids.extend(_ensure_list(node_dict[\"objectid\"]))\n",
    "        if \"objectids\" in node_dict: ids.extend(_ensure_list(node_dict[\"objectids\"]))\n",
    "        return [str(i).strip() for i in ids if i is not None and str(i).strip() != \"\"]\n",
    "\n",
    "    def _dedup(seq):\n",
    "        seen=set(); out=[]\n",
    "        for x in seq:\n",
    "            if x not in seen:\n",
    "                out.append(x); seen.add(x)\n",
    "        return out\n",
    "\n",
    "    def _keep_odd_objectids(ids):\n",
    "        odds = [i for i in ids if i.isdigit() and (int(i) % 2 == 1)]\n",
    "        return odds if odds else ids\n",
    "\n",
    "    def _normalize_one_location(name, loc, include_oid_in_name=True):\n",
    "        nodes = (loc.get(\"nodes\") if isinstance(loc, dict) else None) or {}\n",
    "        all_ids=[]\n",
    "        for _, node in nodes.items():\n",
    "            all_ids.extend(_gather_objectids(node))\n",
    "        if not all_ids and isinstance(loc, dict) and \"objectid\" in loc:\n",
    "            all_ids = [str(loc[\"objectid\"])]\n",
    "\n",
    "        all_ids = _dedup([i for i in all_ids if i])\n",
    "        kept_ids = _keep_odd_objectids(all_ids)\n",
    "\n",
    "        name_out = name\n",
    "        if include_oid_in_name and kept_ids:\n",
    "            name_out = f\"{name} [{','.join(kept_ids)}]\"\n",
    "\n",
    "        return {\n",
    "            \"name\": name_out,\n",
    "            \"daytype\": (loc.get(\"daytype\") if isinstance(loc, dict) else None) or \"0: All Days (M-Su)\",\n",
    "            \"objectids\": kept_ids,\n",
    "        }\n",
    "\n",
    "    def _normalize_input(aadt_locs):\n",
    "        if isinstance(aadt_locs, pd.DataFrame) and {\"name\",\"daytype\",\"objectids\"}.issubset(aadt_locs.columns):\n",
    "            recs = aadt_locs.to_dict(orient=\"records\")\n",
    "            for r in recs:\n",
    "                r[\"objectids\"] = _keep_odd_objectids(_ensure_list(r.get(\"objectids\")))\n",
    "            return recs\n",
    "        if isinstance(aadt_locs, list) and aadt_locs and isinstance(aadt_locs[0], dict) and \\\n",
    "           {\"name\",\"daytype\",\"objectids\"}.issubset(aadt_locs[0].keys()):\n",
    "            recs = []\n",
    "            for r in aadt_locs:\n",
    "                r = dict(r)\n",
    "                r[\"objectids\"] = _keep_odd_objectids(_ensure_list(r.get(\"objectids\")))\n",
    "                recs.append(r)\n",
    "            return recs\n",
    "\n",
    "        recs = []\n",
    "        if isinstance(aadt_locs, dict):\n",
    "            for nm, loc in aadt_locs.items():\n",
    "                recs.append(_normalize_one_location(nm, loc))\n",
    "            return recs\n",
    "\n",
    "        if isinstance(aadt_locs, list):\n",
    "            for item in aadt_locs:\n",
    "                if not isinstance(item, dict):\n",
    "                    continue\n",
    "                if \"nodes\" in item:\n",
    "                    nm = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "                    recs.append(_normalize_one_location(nm, item))\n",
    "                elif \"objectid\" in item:\n",
    "                    oid = str(item.get(\"objectid\")).strip()\n",
    "                    nm  = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "                    kept = _keep_odd_objectids([oid])\n",
    "                    recs.append({\n",
    "                        \"name\": f\"{nm} [{','.join(kept)}]\" if kept else nm,\n",
    "                        \"daytype\": item.get(\"daytype\", \"0: All Days (M-Su)\"),\n",
    "                        \"objectids\": kept,\n",
    "                    })\n",
    "                else:\n",
    "                    for nm, loc in item.items():\n",
    "                        recs.append(_normalize_one_location(nm, loc))\n",
    "        return recs\n",
    "\n",
    "    def _traditional_aadt_for_ids(df_tc_in, obj_ids):\n",
    "        \"\"\"\n",
    "        Default (use_parity=False): per-oid average of (ahead_aadt, back_aadt), then mean across oids.\n",
    "        If use_parity=True: even->back_aadt, odd->ahead_aadt.\n",
    "        \"\"\"\n",
    "        obj_ids = [str(x).strip() for x in (obj_ids or []) if str(x).strip()]\n",
    "        if not obj_ids:\n",
    "            return np.nan, np.nan, np.nan, 0\n",
    "\n",
    "        sub = df_tc_in[df_tc_in[\"objectid\"].astype(str).str.strip().isin(obj_ids)].copy()\n",
    "        if sub.empty:\n",
    "            return np.nan, np.nan, np.nan, 0\n",
    "\n",
    "        if use_parity:\n",
    "            vals = []\n",
    "            for oid in obj_ids:\n",
    "                row = sub[sub[\"objectid\"].astype(str).str.strip() == oid]\n",
    "                if row.empty:\n",
    "                    continue\n",
    "                v = row.iloc[0][\"back_aadt\"] if (oid.isdigit() and int(oid) % 2 == 0) else row.iloc[0][\"ahead_aadt\"]\n",
    "                vals.append(pd.to_numeric(v, errors=\"coerce\"))\n",
    "            vals = pd.Series(vals, dtype=\"float64\").dropna()\n",
    "            if vals.empty: return np.nan, np.nan, np.nan, 0\n",
    "            overall = float(vals.mean())\n",
    "            return overall, np.nan, np.nan, int(vals.shape[0])\n",
    "\n",
    "        # --- average ahead/back per objectid, then average across objectids ---\n",
    "        sub[\"ahead_aadt\"] = pd.to_numeric(sub.get(\"ahead_aadt\"), errors=\"coerce\")\n",
    "        sub[\"back_aadt\"]  = pd.to_numeric(sub.get(\"back_aadt\"),  errors=\"coerce\")\n",
    "\n",
    "        # per-oid average: mean of available sides (ignore NaN)\n",
    "        per_oid_avg = sub[[\"ahead_aadt\",\"back_aadt\"]].mean(axis=1, skipna=True)\n",
    "        per_oid_avg = per_oid_avg.dropna()\n",
    "\n",
    "        if per_oid_avg.empty:\n",
    "            return np.nan, np.nan, np.nan, 0\n",
    "\n",
    "        overall = float(per_oid_avg.mean())\n",
    "\n",
    "        # side means (for reporting only)\n",
    "        ahead_vals = sub[\"ahead_aadt\"].dropna()\n",
    "        back_vals  = sub[\"back_aadt\"].dropna()\n",
    "        mean_ahead = float(ahead_vals.mean()) if not ahead_vals.empty else np.nan\n",
    "        mean_back  = float(back_vals.mean())  if not back_vals.empty  else np.nan\n",
    "        count_used = int(per_oid_avg.shape[0])\n",
    "\n",
    "        return overall, mean_ahead, mean_back, count_used\n",
    "\n",
    "    # ---- main ----\n",
    "    norm = _normalize_input(aadt_locations)\n",
    "    tc_ids_all = set(df_tc[\"objectid\"].astype(str).str.strip().unique())\n",
    "\n",
    "    rows = []\n",
    "    for loc in norm:\n",
    "        obj_ids = [str(x).strip() for x in (loc.get(\"objectids\") or []) if str(x).strip()]\n",
    "        overall, mean_ahead, mean_back, n_found = _traditional_aadt_for_ids(df_tc, obj_ids)\n",
    "        missing = [x for x in obj_ids if x not in tc_ids_all]\n",
    "\n",
    "        rows.append({\n",
    "            \"location\": loc.get(\"name\"),\n",
    "            \"daytype\":  loc.get(\"daytype\"),\n",
    "            \"objectids\": \"|\".join(obj_ids),\n",
    "            \"n_objectids\": len(obj_ids),\n",
    "            \"n_found_in_tc\": int(n_found),\n",
    "            \"missing_objectids\": \"|\".join(missing) if missing else \"\",\n",
    "            \"traditional_ahead_mean\": mean_ahead,\n",
    "            \"traditional_behind_mean\": mean_back,\n",
    "            \"traditional_aadt\": overall,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows) if as_df else rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db115308-4010-4c5a-a51c-49db69dfaa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run step 1 - traditional aadt counts\n",
    "trad_df = traditional_aadt_by_location(aadt_locations, df_tc, as_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e23b26a-69a2-4889-b04b-14cc250a504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trad_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69ea7086-b4d3-47eb-bf6e-bae9c86c506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Step 1 as a CSV to take a look\n",
    "trad_df.to_csv(\"step_1_traditional_aadt_by_location.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30ac04-cfd4-45bc-a85f-16437dbafdd2",
   "metadata": {},
   "source": [
    "## Step 2 Identify Traffic Census location names for the StreetLight segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69e68861-59c8-4426-bf8c-16db1ea1ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# def non_traditional_aadt_by_location(\n",
    "#     aadt_locations,\n",
    "#     df_stl,\n",
    "#     daytype_filter=\"0: All Days (M-Su)\",\n",
    "#     daypart_filter=\"0: All Day (12am-12am)\",\n",
    "#     modeoftravel_filter=None,\n",
    "#     zonename_col=\"zonename\",\n",
    "#     stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\",\n",
    "#     as_df=True,\n",
    "#     agg=\"sum\",  # \"sum\" mirrors reviewed pipeline; \"mean\" = average across segments\n",
    "#     segment_count_mode=\"unique\",  # \"unique\" counts deduped zonenames; set \"all\" to count before dedup\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Build a per-location summary of *non-traditional* (StreetLight) AADT.\n",
    "\n",
    "#     Output columns (one row per location):\n",
    "#       location, daytype_expected, daytype_used, daypart_used, modeoftravel_used,\n",
    "#       ahead_zones, behind_zones,\n",
    "#       non_trad_ahead_mean, non_trad_behind_mean, non_trad_aadt,\n",
    "#       stl_ahead_rows, stl_behind_rows, missing_ahead_zones, missing_behind_zones\n",
    "\n",
    "#     NEW columns added to align with 'count each segment listed':\n",
    "#       listed_ahead_segments, listed_behind_segments,\n",
    "#       present_ahead_segments, present_behind_segments\n",
    "#     \"\"\"\n",
    "#     # ---- helpers ----\n",
    "#     def _ensure_list(x):\n",
    "#         if x is None: return []\n",
    "#         if isinstance(x, (list, tuple, set)): return list(x)\n",
    "#         return [x]\n",
    "\n",
    "#     def _gather_zones(node_dict):\n",
    "#         ahead  = _ensure_list(node_dict.get(\"zonename_ahead\", []))\n",
    "#         behind = _ensure_list(node_dict.get(\"zonename_behind\", []))\n",
    "#         return ahead, behind\n",
    "\n",
    "#     def _gather_objectids(node_dict):\n",
    "#         ids = []\n",
    "#         if not isinstance(node_dict, dict): return ids\n",
    "#         if \"objectid\"  in node_dict: ids.extend(_ensure_list(node_dict[\"objectid\"]))\n",
    "#         if \"objectids\" in node_dict: ids.extend(_ensure_list(node_dict[\"objectids\"]))\n",
    "#         return [str(i) for i in ids if i is not None and str(i).strip() != \"\"]\n",
    "\n",
    "#     def _dedup(seq):\n",
    "#         seen=set(); out=[]\n",
    "#         for x in seq:\n",
    "#             if x not in seen:\n",
    "#                 out.append(x); seen.add(x)\n",
    "#         return out\n",
    "\n",
    "#     def _normalize_one_location(name, loc, include_oid_in_name=True):\n",
    "#         \"\"\"Nested 'nodes' format -> collect objectids and zonenames; append [oids] to name for merge alignment.\"\"\"\n",
    "#         nodes = loc.get(\"nodes\", {}) or {}\n",
    "#         ahead_all, behind_all, all_oids = [], [], []\n",
    "#         for _, node in nodes.items():\n",
    "#             a, b = _gather_zones(node)\n",
    "#             ahead_all.extend([z for z in a if z])\n",
    "#             behind_all.extend([z for z in b if z])\n",
    "#             all_oids.extend(_gather_objectids(node))\n",
    "\n",
    "#         name_out = name\n",
    "#         if include_oid_in_name and all_oids:\n",
    "#             name_out = f\"{name} [{','.join(_dedup(all_oids))}]\"\n",
    "\n",
    "#         # record both pre-dedup and dedup lists so we can count segments the way you prefer\n",
    "#         ahead_dedup  = _dedup(ahead_all)\n",
    "#         behind_dedup = _dedup(behind_all)\n",
    "\n",
    "#         return {\n",
    "#             \"name\": name_out,\n",
    "#             \"daytype\": loc.get(\"daytype\", \"0: All Days (M-Su)\"),\n",
    "#             \"ahead_zones_all\": ahead_all,\n",
    "#             \"behind_zones_all\": behind_all,\n",
    "#             \"ahead_zones\": ahead_dedup,\n",
    "#             \"behind_zones\": behind_dedup,\n",
    "#         }\n",
    "\n",
    "#     def _normalize_input(aadt_locs):\n",
    "#         # Already normalized DataFrame?\n",
    "#         if isinstance(aadt_locs, pd.DataFrame) and \\\n",
    "#            {\"name\",\"daytype\",\"ahead_zones\",\"behind_zones\"}.issubset(aadt_locs.columns):\n",
    "#             recs = aadt_locs.to_dict(orient=\"records\")\n",
    "#             # ensure *_zones_all exist for counting\n",
    "#             for r in recs:\n",
    "#                 r.setdefault(\"ahead_zones_all\", r.get(\"ahead_zones\", []))\n",
    "#                 r.setdefault(\"behind_zones_all\", r.get(\"behind_zones\", []))\n",
    "#             return recs\n",
    "\n",
    "#         # Already normalized list[dict]?\n",
    "#         if isinstance(aadt_locs, list) and aadt_locs and isinstance(aadt_locs[0], dict) and \\\n",
    "#            {\"name\",\"daytype\",\"ahead_zones\",\"behind_zones\"}.issubset(aadt_locs[0].keys()):\n",
    "#             recs = aadt_locs\n",
    "#             for r in recs:\n",
    "#                 r.setdefault(\"ahead_zones_all\", r.get(\"ahead_zones\", []))\n",
    "#                 r.setdefault(\"behind_zones_all\", r.get(\"behind_zones\", []))\n",
    "#             return recs\n",
    "\n",
    "#         recs = []\n",
    "#         # Dict keyed by name (nested format)\n",
    "#         if isinstance(aadt_locs, dict):\n",
    "#             for nm, loc in aadt_locs.items():\n",
    "#                 recs.append(_normalize_one_location(nm, loc))\n",
    "#             return recs\n",
    "\n",
    "#         # List of locations (mixed formats)\n",
    "#         if isinstance(aadt_locs, list):\n",
    "#             for item in aadt_locs:\n",
    "#                 if not isinstance(item, dict):\n",
    "#                     continue\n",
    "#                 if \"nodes\" in item:\n",
    "#                     nm = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "#                     recs.append(_normalize_one_location(nm, item))\n",
    "#                 elif \"objectid\" in item:\n",
    "#                     # flat I-605 row (objectid + zonename_0..3)\n",
    "#                     oid = str(item.get(\"objectid\"))\n",
    "#                     nm  = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "#                     day = item.get(\"daytype\", \"0: All Days (M-Su)\")\n",
    "\n",
    "#                     ahead_all, behind_all = [], []\n",
    "#                     for k, v in item.items():\n",
    "#                         if not (isinstance(k, str) and k.startswith(\"zonename_\")):\n",
    "#                             continue\n",
    "#                         try:\n",
    "#                             idx = int(k.split(\"_\")[1])\n",
    "#                         except Exception:\n",
    "#                             idx = None\n",
    "#                         # Convention: even 0/2 -> ahead; odd 1/3 -> behind\n",
    "#                         if idx is not None and idx % 2 == 0:\n",
    "#                             ahead_all.append(v)\n",
    "#                         else:\n",
    "#                             behind_all.append(v)\n",
    "\n",
    "#                     recs.append({\n",
    "#                         \"name\": f\"{nm} [{oid}]\",\n",
    "#                         \"daytype\": day,\n",
    "#                         \"ahead_zones_all\": ahead_all,\n",
    "#                         \"behind_zones_all\": behind_all,\n",
    "#                         \"ahead_zones\": _dedup([z for z in ahead_all if z]),\n",
    "#                         \"behind_zones\": _dedup([z for z in behind_all if z]),\n",
    "#                     })\n",
    "#                 else:\n",
    "#                     for nm, loc in item.items():\n",
    "#                         recs.append(_normalize_one_location(nm, loc))\n",
    "#         return recs\n",
    "\n",
    "#     # ---- filter & precompute per-zone means ----\n",
    "#     must_cols = [zonename_col, stl_volume_col, \"daytype\", \"daypart\"]\n",
    "#     for c in must_cols:\n",
    "#         if c not in df_stl.columns:\n",
    "#             raise KeyError(f\"df_stl is missing required column: {c}\")\n",
    "\n",
    "#     filt = (df_stl[\"daytype\"] == daytype_filter) & (df_stl[\"daypart\"] == daypart_filter)\n",
    "#     if modeoftravel_filter and (\"modeoftravel\" in df_stl.columns):\n",
    "#         filt = filt & (df_stl[\"modeoftravel\"] == modeoftravel_filter)\n",
    "\n",
    "#     stl_filtered = df_stl.loc[filt, [zonename_col, stl_volume_col]].copy()\n",
    "\n",
    "#     # Clean types\n",
    "#     stl_filtered[zonename_col] = stl_filtered[zonename_col].astype(str).str.strip()\n",
    "#     stl_filtered[stl_volume_col] = pd.to_numeric(stl_filtered[stl_volume_col], errors=\"coerce\")\n",
    "\n",
    "#     # Compute per-zonename averages (handles duplicates safely)\n",
    "#     zone_group = stl_filtered.groupby(zonename_col)[stl_volume_col]\n",
    "#     zone_mean = zone_group.mean()   # pd.Series: index=zonename, value=mean volume\n",
    "#     zone_rows = zone_group.size()   # pd.Series: index=zonename, value=row count backing the mean\n",
    "#     present_zones = set(zone_mean.index)\n",
    "\n",
    "#     def _zone_stats(zones_list, agg_local=\"sum\"):\n",
    "#         \"\"\"\n",
    "#         Return:\n",
    "#           aggregated_value,\n",
    "#           backing_row_count_sum,\n",
    "#           missing_list,\n",
    "#           present_segment_count,\n",
    "#           listed_segment_count\n",
    "#         \"\"\"\n",
    "#         # choose counting base (dedup vs all)\n",
    "#         zones_all = [z for z in _ensure_list(zones_list) if z and str(z).strip() != \"\"]\n",
    "#         zones_all = [str(z).strip() for z in zones_all]\n",
    "#         zones_for_agg = _dedup(zones_all) if segment_count_mode == \"unique\" else zones_all\n",
    "\n",
    "#         if not zones_for_agg:\n",
    "#             return np.nan, 0, [], 0, 0\n",
    "\n",
    "#         present = [z for z in zones_for_agg if z in present_zones]\n",
    "#         missing = [z for z in zones_for_agg if z not in present_zones]\n",
    "\n",
    "#         vals = zone_mean.reindex(present).dropna()\n",
    "#         if agg_local == \"sum\":\n",
    "#             val = float(vals.sum()) if len(vals) else np.nan\n",
    "#         else:  # \"mean\"\n",
    "#             val = float(vals.mean()) if len(vals) else np.nan\n",
    "\n",
    "#         n_rows = int(zone_rows.reindex(present).fillna(0).sum())\n",
    "#         present_seg_ct = len(present)\n",
    "#         listed_seg_ct = len(zones_for_agg)\n",
    "#         return val, n_rows, missing, present_seg_ct, listed_seg_ct\n",
    "\n",
    "#     # ---- build rows ----\n",
    "#     norm = _normalize_input(aadt_locations)\n",
    "#     rows = []\n",
    "#     for loc in norm:\n",
    "#         ahead_all  = loc.get(\"ahead_zones_all\", [])\n",
    "#         behind_all = loc.get(\"behind_zones_all\", [])\n",
    "#         ahead_ded  = loc.get(\"ahead_zones\", [])\n",
    "#         behind_ded = loc.get(\"behind_zones\", [])\n",
    "\n",
    "#         # choose which list drives counts/aggregation\n",
    "#         ahead_for_counts  = ahead_ded if segment_count_mode == \"unique\" else ahead_all\n",
    "#         behind_for_counts = behind_ded if segment_count_mode == \"unique\" else behind_all\n",
    "\n",
    "#         val_ahead, ahead_n, miss_a, present_ahead_ct, listed_ahead_ct = _zone_stats(ahead_for_counts, agg_local=agg)\n",
    "#         val_behind, behind_n, miss_b, present_behind_ct, listed_behind_ct = _zone_stats(behind_for_counts, agg_local=agg)\n",
    "\n",
    "#         # Sum ahead + behind to mirror reviewed pipeline\n",
    "#         overall = np.nansum([val_ahead, val_behind])\n",
    "\n",
    "#         rows.append({\n",
    "#             \"location\": loc.get(\"name\"),\n",
    "#             \"daytype_expected\": loc.get(\"daytype\"),\n",
    "#             \"daytype_used\": daytype_filter,\n",
    "#             \"daypart_used\": daypart_filter,\n",
    "#             \"modeoftravel_used\": modeoftravel_filter if modeoftravel_filter else \"\",\n",
    "\n",
    "#             # keep original zone strings (deduped for readability)\n",
    "#             \"ahead_zones\": \"|\".join(ahead_ded),\n",
    "#             \"behind_zones\": \"|\".join(behind_ded),\n",
    "\n",
    "#             # values\n",
    "#             \"non_trad_ahead_mean\": val_ahead,\n",
    "#             \"non_trad_behind_mean\": val_behind,\n",
    "#             \"non_trad_aadt\": overall,\n",
    "\n",
    "#             # backing row counts in df_stl (unchanged)\n",
    "#             \"stl_ahead_rows\": ahead_n,\n",
    "#             \"stl_behind_rows\": behind_n,\n",
    "\n",
    "#             # NEW: segment counts (how many segments were listed vs present in df_stl)\n",
    "#             \"listed_ahead_segments\": listed_ahead_ct,\n",
    "#             \"listed_behind_segments\": listed_behind_ct,\n",
    "#             \"present_ahead_segments\": present_ahead_ct,\n",
    "#             \"present_behind_segments\": present_behind_ct,\n",
    "\n",
    "#             # missing lists\n",
    "#             \"missing_ahead_zones\": \"|\".join(miss_a) if miss_a else \"\",\n",
    "#             \"missing_behind_zones\": \"|\".join(miss_b) if miss_b else \"\",\n",
    "#         })\n",
    "\n",
    "#     return pd.DataFrame(rows) if as_df else rows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def non_traditional_aadt_by_location(\n",
    "#     aadt_locations,\n",
    "#     df_stl,\n",
    "#     daytype_filter=\"0: All Days (M-Su)\",\n",
    "#     daypart_filter=\"0: All Day (12am-12am)\",\n",
    "#     modeoftravel_filter=None,\n",
    "#     zonename_col=\"zonename\",\n",
    "#     stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\",\n",
    "#     as_df=True,\n",
    "#     agg=\"sum\",                 # \"sum\" mirrors reviewed pipeline; \"mean\" = average across segments\n",
    "#     segment_count_mode=\"unique\",  # \"unique\" counts deduped zonenames; \"all\" counts before dedup\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Build a per-location summary of *non-traditional* (StreetLight) AADT.\n",
    "\n",
    "#     Output columns (one row per location):\n",
    "#       location, daytype_expected, daytype_used, daypart_used, modeoftravel_used,\n",
    "#       ahead_zones, behind_zones,\n",
    "#       non_trad_ahead_mean, non_trad_behind_mean, non_trad_aadt,\n",
    "#       stl_ahead_rows, stl_behind_rows, missing_ahead_zones, missing_behind_zones\n",
    "\n",
    "#     NEW columns added to align with 'count each segment listed':\n",
    "#       listed_ahead_segments, listed_behind_segments,\n",
    "#       present_ahead_segments, present_behind_segments\n",
    "\n",
    "#     Notes:\n",
    "#       - non_trad_aadt = average of (non_trad_ahead_mean, non_trad_behind_mean) [NaN-safe]\n",
    "#     \"\"\"\n",
    "#     # ---- helpers ----\n",
    "#     def _ensure_list(x):\n",
    "#         if x is None: return []\n",
    "#         if isinstance(x, (list, tuple, set)): return list(x)\n",
    "#         return [x]\n",
    "\n",
    "#     def _gather_zones(node_dict):\n",
    "#         ahead  = _ensure_list(node_dict.get(\"zonename_ahead\", []))\n",
    "#         behind = _ensure_list(node_dict.get(\"zonename_behind\", []))\n",
    "#         return ahead, behind\n",
    "\n",
    "#     def _gather_objectids(node_dict):\n",
    "#         ids = []\n",
    "#         if not isinstance(node_dict, dict): return ids\n",
    "#         if \"objectid\"  in node_dict: ids.extend(_ensure_list(node_dict[\"objectid\"]))\n",
    "#         if \"objectids\" in node_dict: ids.extend(_ensure_list(node_dict[\"objectids\"]))\n",
    "#         return [str(i) for i in ids if i is not None and str(i).strip() != \"\"]\n",
    "\n",
    "#     def _dedup(seq):\n",
    "#         seen=set(); out=[]\n",
    "#         for x in seq:\n",
    "#             if x not in seen:\n",
    "#                 out.append(x); seen.add(x)\n",
    "#         return out\n",
    "\n",
    "#     def _normalize_one_location(name, loc, include_oid_in_name=True):\n",
    "#         \"\"\"Nested 'nodes' -> collect objectids and zonenames; append [oids] to name for merge alignment.\"\"\"\n",
    "#         nodes = loc.get(\"nodes\", {}) or {}\n",
    "#         ahead_all, behind_all, all_oids = [], [], []\n",
    "#         for _, node in nodes.items():\n",
    "#             a, b = _gather_zones(node)\n",
    "#             ahead_all.extend([z for z in a if z])\n",
    "#             behind_all.extend([z for z in b if z])\n",
    "#             all_oids.extend(_gather_objectids(node))\n",
    "\n",
    "#         name_out = name\n",
    "#         if include_oid_in_name and all_oids:\n",
    "#             name_out = f\"{name} [{','.join(_dedup(all_oids))}]\"\n",
    "\n",
    "#         # keep both pre-dedup and dedup lists so we can count segments as requested\n",
    "#         ahead_ded  = _dedup(ahead_all)\n",
    "#         behind_ded = _dedup(behind_all)\n",
    "\n",
    "#         return {\n",
    "#             \"name\": name_out,\n",
    "#             \"daytype\": loc.get(\"daytype\", \"0: All Days (M-Su)\"),\n",
    "#             \"ahead_zones_all\": ahead_all,\n",
    "#             \"behind_zones_all\": behind_all,\n",
    "#             \"ahead_zones\": ahead_ded,\n",
    "#             \"behind_zones\": behind_ded,\n",
    "#         }\n",
    "\n",
    "#     def _normalize_input(aadt_locs):\n",
    "#         # Already normalized DataFrame?\n",
    "#         if isinstance(aadt_locs, pd.DataFrame) and \\\n",
    "#            {\"name\",\"daytype\",\"ahead_zones\",\"behind_zones\"}.issubset(aadt_locs.columns):\n",
    "#             recs = aadt_locs.to_dict(orient=\"records\")\n",
    "#             for r in recs:\n",
    "#                 r.setdefault(\"ahead_zones_all\", r.get(\"ahead_zones\", []))\n",
    "#                 r.setdefault(\"behind_zones_all\", r.get(\"behind_zones\", []))\n",
    "#             return recs\n",
    "\n",
    "#         # Already normalized list[dict]?\n",
    "#         if isinstance(aadt_locs, list) and aadt_locs and isinstance(aadt_locs[0], dict) and \\\n",
    "#            {\"name\",\"daytype\",\"ahead_zones\",\"behind_zones\"}.issubset(aadt_locs[0].keys()):\n",
    "#             recs = aadt_locs\n",
    "#             for r in recs:\n",
    "#                 r.setdefault(\"ahead_zones_all\", r.get(\"ahead_zones\", []))\n",
    "#                 r.setdefault(\"behind_zones_all\", r.get(\"behind_zones\", []))\n",
    "#             return recs\n",
    "\n",
    "#         recs = []\n",
    "#         # Dict keyed by name (nested format)\n",
    "#         if isinstance(aadt_locs, dict):\n",
    "#             for nm, loc in aadt_locs.items():\n",
    "#                 recs.append(_normalize_one_location(nm, loc))\n",
    "#             return recs\n",
    "\n",
    "#         # List of locations (mixed formats)\n",
    "#         if isinstance(aadt_locs, list):\n",
    "#             for item in aadt_locs:\n",
    "#                 if not isinstance(item, dict):\n",
    "#                     continue\n",
    "#                 if \"nodes\" in item:\n",
    "#                     nm = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "#                     recs.append(_normalize_one_location(nm, item))\n",
    "#                 elif \"objectid\" in item:\n",
    "#                     # flat row (objectid + zonename_0..3)\n",
    "#                     oid = str(item.get(\"objectid\"))\n",
    "#                     nm  = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "#                     day = item.get(\"daytype\", \"0: All Days (M-Su)\")\n",
    "\n",
    "#                     ahead_all, behind_all = [], []\n",
    "#                     for k, v in item.items():\n",
    "#                         if not (isinstance(k, str) and k.startswith(\"zonename_\")):\n",
    "#                             continue\n",
    "#                         try:\n",
    "#                             idx = int(k.split(\"_\")[1])\n",
    "#                         except Exception:\n",
    "#                             idx = None\n",
    "#                         # even 0/2 -> ahead; odd 1/3 -> behind\n",
    "#                         if idx is not None and idx % 2 == 0:\n",
    "#                             ahead_all.append(v)\n",
    "#                         else:\n",
    "#                             behind_all.append(v)\n",
    "\n",
    "#                     recs.append({\n",
    "#                         \"name\": f\"{nm} [{oid}]\",\n",
    "#                         \"daytype\": day,\n",
    "#                         \"ahead_zones_all\": ahead_all,\n",
    "#                         \"behind_zones_all\": behind_all,\n",
    "#                         \"ahead_zones\": _dedup([z for z in ahead_all if z]),\n",
    "#                         \"behind_zones\": _dedup([z for z in behind_all if z]),\n",
    "#                     })\n",
    "#                 else:\n",
    "#                     for nm, loc in item.items():\n",
    "#                         recs.append(_normalize_one_location(nm, loc))\n",
    "#         return recs\n",
    "\n",
    "#     # ---- filter & per-zone means ----\n",
    "#     must_cols = [zonename_col, stl_volume_col, \"daytype\", \"daypart\"]\n",
    "#     for c in must_cols:\n",
    "#         if c not in df_stl.columns:\n",
    "#             raise KeyError(f\"df_stl is missing required column: {c}\")\n",
    "\n",
    "#     filt = (df_stl[\"daytype\"] == daytype_filter) & (df_stl[\"daypart\"] == daypart_filter)\n",
    "#     if modeoftravel_filter and (\"modeoftravel\" in df_stl.columns):\n",
    "#         filt = filt & (df_stl[\"modeoftravel\"] == modeoftravel_filter)\n",
    "\n",
    "#     stl_filtered = df_stl.loc[filt, [zonename_col, stl_volume_col]].copy()\n",
    "#     stl_filtered[zonename_col] = stl_filtered[zonename_col].astype(str).str.strip()\n",
    "#     stl_filtered[stl_volume_col] = pd.to_numeric(stl_filtered[stl_volume_col], errors=\"coerce\")\n",
    "\n",
    "#     zone_group = stl_filtered.groupby(zonename_col)[stl_volume_col]\n",
    "#     zone_mean = zone_group.mean()    # index=zonename, value=mean volume\n",
    "#     zone_rows = zone_group.size()    # index=zonename, value=row count\n",
    "#     present_zones = set(zone_mean.index)\n",
    "\n",
    "#     def _zone_stats(zones_list, agg_local=\"sum\"):\n",
    "#         \"\"\"\n",
    "#         Return:\n",
    "#           aggregated_value,\n",
    "#           backing_row_count_sum,\n",
    "#           missing_list,\n",
    "#           present_segment_count,\n",
    "#           listed_segment_count\n",
    "#         \"\"\"\n",
    "#         zones_all = [z for z in _ensure_list(zones_list) if z and str(z).strip() != \"\"]\n",
    "#         zones_all = [str(z).strip() for z in zones_all]\n",
    "#         zones_for_agg = _dedup(zones_all) if segment_count_mode == \"unique\" else zones_all\n",
    "\n",
    "#         if not zones_for_agg:\n",
    "#             return np.nan, 0, [], 0, 0\n",
    "\n",
    "#         present = [z for z in zones_for_agg if z in present_zones]\n",
    "#         missing = [z for z in zones_for_agg if z not in present_zones]\n",
    "\n",
    "#         vals = zone_mean.reindex(present).dropna()\n",
    "#         if agg_local == \"sum\":\n",
    "#             val = float(vals.sum()) if len(vals) else np.nan\n",
    "#         else:  # \"mean\"\n",
    "#             val = float(vals.mean()) if len(vals) else np.nan\n",
    "\n",
    "#         n_rows = int(zone_rows.reindex(present).fillna(0).sum())\n",
    "#         present_seg_ct = len(present)\n",
    "#         listed_seg_ct = len(zones_for_agg)\n",
    "#         return val, n_rows, missing, present_seg_ct, listed_seg_ct\n",
    "\n",
    "#     # ---- build rows ----\n",
    "#     norm = _normalize_input(aadt_locations)\n",
    "#     rows = []\n",
    "#     for loc in norm:\n",
    "#         ahead_all  = loc.get(\"ahead_zones_all\", [])\n",
    "#         behind_all = loc.get(\"behind_zones_all\", [])\n",
    "#         ahead_ded  = loc.get(\"ahead_zones\", [])\n",
    "#         behind_ded = loc.get(\"behind_zones\", [])\n",
    "\n",
    "#         ahead_for_counts  = ahead_ded if segment_count_mode == \"unique\" else ahead_all\n",
    "#         behind_for_counts = behind_ded if segment_count_mode == \"unique\" else behind_all\n",
    "\n",
    "#         val_ahead,  ahead_n,  miss_a, present_ahead_ct,  listed_ahead_ct  = _zone_stats(ahead_for_counts,  agg_local=agg)\n",
    "#         val_behind, behind_n, miss_b, present_behind_ct, listed_behind_ct = _zone_stats(behind_for_counts, agg_local=agg)\n",
    "\n",
    "#         # >>> CHANGE HERE: average the two direction aggregates (NaN-safe)\n",
    "        \n",
    "#         overall = np.nanmean([val_ahead, val_behind])\n",
    "        \n",
    "        \n",
    "#         rows.append({\n",
    "#             \"location\": loc.get(\"name\"),\n",
    "#             \"daytype_expected\": loc.get(\"daytype\"),\n",
    "#             \"daytype_used\": daytype_filter,\n",
    "#             \"daypart_used\": daypart_filter,\n",
    "#             \"modeoftravel_used\": modeoftravel_filter if modeoftravel_filter else \"\",\n",
    "\n",
    "#             \"ahead_zones\": \"|\".join(ahead_ded),\n",
    "#             \"behind_zones\": \"|\".join(behind_ded),\n",
    "\n",
    "#             \"non_trad_ahead_mean\": val_ahead,\n",
    "#             \"non_trad_behind_mean\": val_behind,\n",
    "#             \"non_trad_aadt\": overall,\n",
    "\n",
    "#             \"stl_ahead_rows\": ahead_n,\n",
    "#             \"stl_behind_rows\": behind_n,\n",
    "\n",
    "#             \"listed_ahead_segments\": listed_ahead_ct,\n",
    "#             \"listed_behind_segments\": listed_behind_ct,\n",
    "#             \"present_ahead_segments\": present_ahead_ct,\n",
    "#             \"present_behind_segments\": present_behind_ct,\n",
    "\n",
    "#             \"missing_ahead_zones\": \"|\".join(miss_a) if miss_a else \"\",\n",
    "#             \"missing_behind_zones\": \"|\".join(miss_b) if miss_b else \"\",\n",
    "#         })\n",
    "\n",
    "#     return pd.DataFrame(rows) if as_df else rows\n",
    "\n",
    "\n",
    "\n",
    "def non_traditional_aadt_by_location(\n",
    "    aadt_locations,\n",
    "    df_stl,\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=None,\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\",\n",
    "    as_df=True,\n",
    "    agg=\"sum\",                # \"sum\" mirrors reviewed pipeline at the per-side level\n",
    "    segment_count_mode=\"unique\",  # \"unique\" counts deduped zonenames; \"all\" counts before dedup\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a per-location summary of *non-traditional* (StreetLight) AADT.\n",
    "\n",
    "    Output columns (one row per location):\n",
    "      location, daytype_expected, daytype_used, daypart_used, modeoftravel_used,\n",
    "      ahead_zones, behind_zones,\n",
    "      non_trad_ahead_mean, non_trad_behind_mean, non_trad_aadt,\n",
    "      stl_ahead_rows, stl_behind_rows, missing_ahead_zones, missing_behind_zones,\n",
    "      listed_ahead_segments, listed_behind_segments, present_ahead_segments, present_behind_segments\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- helpers ----\n",
    "    def _ensure_list(x):\n",
    "        if x is None:\n",
    "            return []\n",
    "        if isinstance(x, (list, tuple, set)):\n",
    "            return list(x)\n",
    "        return [x]\n",
    "\n",
    "    def _gather_zones(node_dict):\n",
    "        ahead  = _ensure_list(node_dict.get(\"zonename_ahead\", []))\n",
    "        behind = _ensure_list(node_dict.get(\"zonename_behind\", []))\n",
    "        return ahead, behind\n",
    "\n",
    "    def _gather_objectids(node_dict):\n",
    "        ids = []\n",
    "        if not isinstance(node_dict, dict):\n",
    "            return ids\n",
    "        if \"objectid\"  in node_dict: ids.extend(_ensure_list(node_dict[\"objectid\"]))\n",
    "        if \"objectids\" in node_dict: ids.extend(_ensure_list(node_dict[\"objectids\"]))\n",
    "        return [str(i) for i in ids if i is not None and str(i).strip() != \"\"]\n",
    "\n",
    "    def _dedup(seq):\n",
    "        seen=set(); out=[]\n",
    "        for x in seq:\n",
    "            if x not in seen:\n",
    "                out.append(x); seen.add(x)\n",
    "        return out\n",
    "\n",
    "    def _normalize_one_location(name, loc, include_oid_in_name=True):\n",
    "        \"\"\"Collect objectids and zonenames; append [oids] to name for merge alignment.\"\"\"\n",
    "        nodes = loc.get(\"nodes\", {}) or {}\n",
    "        ahead_all, behind_all, all_oids = [], [], []\n",
    "        for _, node in nodes.items():\n",
    "            a, b = _gather_zones(node)\n",
    "            ahead_all.extend([z for z in a if z])\n",
    "            behind_all.extend([z for z in b if z])\n",
    "            all_oids.extend(_gather_objectids(node))\n",
    "\n",
    "        name_out = name\n",
    "        if include_oid_in_name and all_oids:\n",
    "            name_out = f\"{name} [{','.join(_dedup(all_oids))}]\"\n",
    "\n",
    "        return {\n",
    "            \"name\": name_out,\n",
    "            \"daytype\": loc.get(\"daytype\", \"0: All Days (M-Su)\"),\n",
    "            \"ahead_zones_all\": ahead_all,\n",
    "            \"behind_zones_all\": behind_all,\n",
    "            \"ahead_zones\": _dedup(ahead_all),\n",
    "            \"behind_zones\": _dedup(behind_all),\n",
    "        }\n",
    "\n",
    "    def _normalize_input(aadt_locs):\n",
    "        # Already normalized DataFrame?\n",
    "        if isinstance(aadt_locs, pd.DataFrame) and \\\n",
    "           {\"name\",\"daytype\",\"ahead_zones\",\"behind_zones\"}.issubset(aadt_locs.columns):\n",
    "            recs = aadt_locs.to_dict(orient=\"records\")\n",
    "            for r in recs:\n",
    "                r.setdefault(\"ahead_zones_all\", r.get(\"ahead_zones\", []))\n",
    "                r.setdefault(\"behind_zones_all\", r.get(\"behind_zones\", []))\n",
    "            return recs\n",
    "\n",
    "        # Already normalized list[dict]?\n",
    "        if isinstance(aadt_locs, list) and aadt_locs and isinstance(aadt_locs[0], dict) and \\\n",
    "           {\"name\",\"daytype\",\"ahead_zones\",\"behind_zones\"}.issubset(aadt_locs[0].keys()):\n",
    "            recs = aadt_locs\n",
    "            for r in recs:\n",
    "                r.setdefault(\"ahead_zones_all\", r.get(\"ahead_zones\", []))\n",
    "                r.setdefault(\"behind_zones_all\", r.get(\"behind_zones\", []))\n",
    "            return recs\n",
    "\n",
    "        recs = []\n",
    "        # Dict keyed by name (nested format)\n",
    "        if isinstance(aadt_locs, dict):\n",
    "            for nm, loc in aadt_locs.items():\n",
    "                recs.append(_normalize_one_location(nm, loc))\n",
    "            return recs\n",
    "\n",
    "        # List of locations (mixed formats)\n",
    "        if isinstance(aadt_locs, list):\n",
    "            for item in aadt_locs:\n",
    "                if not isinstance(item, dict):\n",
    "                    continue\n",
    "                if \"nodes\" in item:\n",
    "                    nm = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "                    recs.append(_normalize_one_location(nm, item))\n",
    "                elif \"objectid\" in item:\n",
    "                    # flat row (objectid + zonename_0..3)\n",
    "                    oid = str(item.get(\"objectid\"))\n",
    "                    nm  = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "                    day = item.get(\"daytype\", \"0: All Days (M-Su)\")\n",
    "\n",
    "                    ahead_all, behind_all = [], []\n",
    "                    for k, v in item.items():\n",
    "                        if not (isinstance(k, str) and k.startswith(\"zonename_\")):\n",
    "                            continue\n",
    "                        try:\n",
    "                            idx = int(k.split(\"_\")[1])\n",
    "                        except Exception:\n",
    "                            idx = None\n",
    "                        # even 0/2 -> ahead; odd 1/3 -> behind\n",
    "                        if idx is not None and idx % 2 == 0:\n",
    "                            ahead_all.append(v)\n",
    "                        else:\n",
    "                            behind_all.append(v)\n",
    "\n",
    "                    recs.append({\n",
    "                        \"name\": f\"{nm} [{oid}]\",\n",
    "                        \"daytype\": day,\n",
    "                        \"ahead_zones_all\": ahead_all,\n",
    "                        \"behind_zones_all\": behind_all,\n",
    "                        \"ahead_zones\": _dedup([z for z in ahead_all if z]),\n",
    "                        \"behind_zones\": _dedup([z for z in behind_all if z]),\n",
    "                    })\n",
    "                else:\n",
    "                    for nm, loc in item.items():\n",
    "                        recs.append(_normalize_one_location(nm, loc))\n",
    "        return recs\n",
    "\n",
    "    # ---- filter & precompute per-zone means ----\n",
    "    must_cols = [zonename_col, stl_volume_col, \"daytype\", \"daypart\"]\n",
    "    for c in must_cols:\n",
    "        if c not in df_stl.columns:\n",
    "            raise KeyError(f\"df_stl is missing required column: {c}\")\n",
    "\n",
    "    filt = (df_stl[\"daytype\"] == daytype_filter) & (df_stl[\"daypart\"] == daypart_filter)\n",
    "    if modeoftravel_filter and (\"modeoftravel\" in df_stl.columns):\n",
    "        filt = filt & (df_stl[\"modeoftravel\"] == modeoftravel_filter)\n",
    "\n",
    "    stl_filtered = df_stl.loc[filt, [zonename_col, stl_volume_col]].copy()\n",
    "\n",
    "    # Clean types\n",
    "    stl_filtered[zonename_col] = stl_filtered[zonename_col].astype(str).str.strip()\n",
    "    stl_filtered[stl_volume_col] = pd.to_numeric(stl_filtered[stl_volume_col], errors=\"coerce\")\n",
    "\n",
    "    # Per-zonename averages (handles duplicates safely)\n",
    "    zone_group = stl_filtered.groupby(zonename_col)[stl_volume_col]\n",
    "    zone_mean = zone_group.mean()   # pd.Series: index=zonename, value=mean volume\n",
    "    zone_rows = zone_group.size()   # pd.Series: index=zonename, value=row count backing the mean\n",
    "    present_zones = set(zone_mean.index)\n",
    "\n",
    "    def _zone_stats(zones_list, agg_local=\"sum\"):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "          aggregated_value,\n",
    "          backing_row_count_sum,\n",
    "          missing_list,\n",
    "          present_segment_count,\n",
    "          listed_segment_count\n",
    "        \"\"\"\n",
    "        zones_all = [z for z in _ensure_list(zones_list) if z and str(z).strip() != \"\"]\n",
    "        zones_all = [str(z).strip() for z in zones_all]\n",
    "        zones_for_agg = _dedup(zones_all) if segment_count_mode == \"unique\" else zones_all\n",
    "\n",
    "        if not zones_for_agg:\n",
    "            return np.nan, 0, [], 0, 0\n",
    "\n",
    "        present = [z for z in zones_for_agg if z in present_zones]\n",
    "        missing = [z for z in zones_for_agg if z not in present_zones]\n",
    "\n",
    "        vals = zone_mean.reindex(present).dropna()\n",
    "        if agg_local == \"sum\":\n",
    "            val = float(vals.sum()) if len(vals) else np.nan\n",
    "        else:  # \"mean\" across segments within a side\n",
    "            val = float(vals.mean()) if len(vals) else np.nan\n",
    "\n",
    "        n_rows = int(zone_rows.reindex(present).fillna(0).sum())\n",
    "        present_seg_ct = len(present)\n",
    "        listed_seg_ct = len(zones_for_agg)\n",
    "        return val, n_rows, missing, present_seg_ct, listed_seg_ct\n",
    "\n",
    "    def _combine_dirs(a, b):\n",
    "        \"\"\"Average the two directions if both exist; otherwise use the one that exists.\"\"\"\n",
    "        a = np.nan if a is None else a\n",
    "        b = np.nan if b is None else b\n",
    "        if pd.notna(a) and pd.notna(b):\n",
    "            return (float(a) + float(b)) / 2.0\n",
    "        if pd.notna(a):\n",
    "            return float(a)\n",
    "        if pd.notna(b):\n",
    "            return float(b)\n",
    "        return np.nan\n",
    "\n",
    "    # ---- build rows ----\n",
    "    norm = _normalize_input(aadt_locations)\n",
    "    rows = []\n",
    "    for loc in norm:\n",
    "        ahead_all  = loc.get(\"ahead_zones_all\", [])\n",
    "        behind_all = loc.get(\"behind_zones_all\", [])\n",
    "        ahead_ded  = loc.get(\"ahead_zones\", [])\n",
    "        behind_ded = loc.get(\"behind_zones\", [])\n",
    "\n",
    "        # choose which list drives counts/aggregation\n",
    "        ahead_for_counts  = ahead_ded if segment_count_mode == \"unique\" else ahead_all\n",
    "        behind_for_counts = behind_ded if segment_count_mode == \"unique\" else behind_all\n",
    "\n",
    "        val_ahead, ahead_n, miss_a, present_ahead_ct, listed_ahead_ct = _zone_stats(ahead_for_counts, agg_local=agg)\n",
    "        val_behind, behind_n, miss_b, present_behind_ct, listed_behind_ct = _zone_stats(behind_for_counts, agg_local=agg)\n",
    "\n",
    "        # Average across directions (NOT mean of means twice; just combine the two sides)\n",
    "        overall = _combine_dirs(val_ahead, val_behind)\n",
    "\n",
    "        rows.append({\n",
    "            \"location\": loc.get(\"name\"),\n",
    "            \"daytype_expected\": loc.get(\"daytype\"),\n",
    "            \"daytype_used\": daytype_filter,\n",
    "            \"daypart_used\": daypart_filter,\n",
    "            \"modeoftravel_used\": modeoftravel_filter if modeoftravel_filter else \"\",\n",
    "\n",
    "            # Keep deduped zone strings for readability\n",
    "            \"ahead_zones\": \"|\".join(ahead_ded),\n",
    "            \"behind_zones\": \"|\".join(behind_ded),\n",
    "\n",
    "            # Values\n",
    "            \"non_trad_ahead_mean\": val_ahead,\n",
    "            \"non_trad_behind_mean\": val_behind,\n",
    "            \"non_trad_aadt\": overall,\n",
    "\n",
    "            # Backing row counts in df_stl\n",
    "            \"stl_ahead_rows\": ahead_n,\n",
    "            \"stl_behind_rows\": behind_n,\n",
    "\n",
    "            # Segment counts\n",
    "            \"listed_ahead_segments\": listed_ahead_ct,\n",
    "            \"listed_behind_segments\": listed_behind_ct,\n",
    "            \"present_ahead_segments\": present_ahead_ct,\n",
    "            \"present_behind_segments\": present_behind_ct,\n",
    "\n",
    "            # Missing lists\n",
    "            \"missing_ahead_zones\": \"|\".join(miss_a) if miss_a else \"\",\n",
    "            \"missing_behind_zones\": \"|\".join(miss_b) if miss_b else \"\",\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows) if as_df else rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2bffe53-019a-424d-99bd-6880d47e4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will run the \"non_traditional_aadt_by_location\" function if  you have the raw nested structure:\n",
    "stl_df = non_traditional_aadt_by_location(\n",
    "    aadt_locations,\n",
    "    df_stl,\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=\"All Vehicles - StL All Vehicles Volume\",  # or None\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\",\n",
    "    as_df=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1054eca1-bde9-4b77-881f-3faf0f052fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export step 2 to a CSV\n",
    "stl_df.to_csv(\"step_2_non_traditional_aadt_by_location.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11dbbc8-84a8-4a7c-bca5-5b89aee3259a",
   "metadata": {},
   "source": [
    "### Step 3, Build the per-location comparison DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec234022-6cff-4b8f-90c7-ed0f67a0bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------------------------------\n",
    "# # 3) Build the per-location comparison DataFrame\n",
    "# # ------------------------------------------------------\n",
    "\n",
    "def _base_location(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    return re.sub(r\"\\s*\\[.*\\]\\s*$\", \"\", s).strip()\n",
    "\n",
    "def _pick_col(df: pd.DataFrame, base: str):\n",
    "    if base in df.columns: \n",
    "        return base\n",
    "    for suf in (\"_trad\", \"_nt\", \"_x\", \"_y\"):\n",
    "        c = f\"{base}{suf}\"\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def _uniq_join(series, sep=\"|\"):\n",
    "    vals = []\n",
    "    for x in series.dropna().astype(str):\n",
    "        if not x:\n",
    "            continue\n",
    "        vals.extend([t.strip() for t in x.split(sep) if t.strip()])\n",
    "    # preserve order while deduping\n",
    "    seen=set(); out=[]\n",
    "    for v in vals:\n",
    "        if v not in seen:\n",
    "            out.append(v); seen.add(v)\n",
    "    return sep.join(out)\n",
    "\n",
    "def _uniq_join_commas(series):\n",
    "    # for objectids like \"7817\" etc.\n",
    "    vals = []\n",
    "    for x in series.dropna().astype(str):\n",
    "        for t in re.split(r\"[,\\s]+\", x.strip()):\n",
    "            if t:\n",
    "                vals.append(t)\n",
    "    seen=set(); out=[]\n",
    "    for v in vals:\n",
    "        if v not in seen:\n",
    "            out.append(v); seen.add(v)\n",
    "    return \",\".join(out)\n",
    "\n",
    "# def build_aadt_comparison_df(\n",
    "#     aadt_locations,\n",
    "#     df_tc,\n",
    "#     df_stl,\n",
    "#     daytype_filter=\"0: All Days (M-Su)\",\n",
    "#     daypart_filter=\"0: All Day (12am-12am)\",\n",
    "#     modeoftravel_filter=None,\n",
    "#     zonename_col=\"zonename\",\n",
    "#     stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\",\n",
    "#     agg=\"sum\",\n",
    "#     segment_count_mode=\"unique\",\n",
    "#     collapse_to_one_per_location=True  # NEW\n",
    "# ) -> pd.DataFrame:\n",
    "\n",
    "#     # --- 1) Traditional side ---\n",
    "#     trad_df = traditional_aadt_by_location(\n",
    "#         aadt_locations=aadt_locations, df_tc=df_tc, as_df=True\n",
    "#     ).copy()\n",
    "#     trad_df[\"location_base\"] = trad_df[\"location\"].apply(_base_location)\n",
    "\n",
    "#     # --- 2) Non-traditional side ---\n",
    "#     nt_df = non_traditional_aadt_by_location(\n",
    "#         aadt_locations=aadt_locations,\n",
    "#         df_stl=df_stl,\n",
    "#         daytype_filter=daytype_filter,\n",
    "#         daypart_filter=daypart_filter,\n",
    "#         modeoftravel_filter=modeoftravel_filter,\n",
    "#         zonename_col=zonename_col,\n",
    "#         stl_volume_col=stl_volume_col,\n",
    "#         as_df=True,\n",
    "#         agg=agg,\n",
    "#         segment_count_mode=segment_count_mode\n",
    "#     ).copy()\n",
    "#     nt_df[\"location_base\"] = nt_df[\"location\"].apply(_base_location)\n",
    "\n",
    "#     # --- 3) Merge on base name ---\n",
    "#     merged = pd.merge(trad_df, nt_df, how=\"inner\", on=\"location_base\", suffixes=(\"_trad\", \"_nt\"))\n",
    "\n",
    "#     # Helper: safely fetch a possibly-suffixed column from `merged`\n",
    "#     def G(name):\n",
    "#         c = _pick_col(merged, name)\n",
    "#         return merged[c] if c is not None else pd.Series([np.nan]*len(merged), index=merged.index)\n",
    "      \n",
    "#     if collapse_to_one_per_location:\n",
    "#         grp = merged.groupby(\"location_base\", dropna=False)\n",
    "\n",
    "#         def _first(series):\n",
    "#             return series.iloc[0] if len(series) else np.nan\n",
    "\n",
    "#         out = pd.DataFrame({\n",
    "#             \"location\": grp[\"location_base\"].first(),\n",
    "\n",
    "#             # IDs & zones\n",
    "#             \"objectids\": grp[_pick_col(merged, \"objectids_trad\") or _pick_col(merged, \"objectids\")].apply(\n",
    "#                 lambda s: \",\".join(dict.fromkeys([x for v in s.dropna().astype(str) for x in v.replace(\"|\",\",\").split(\",\") if x]))\n",
    "#             ),\n",
    "#             \"ahead_zones\": grp[_pick_col(merged, \"ahead_zones_nt\") or _pick_col(merged, \"ahead_zones\")].apply(\n",
    "#                 lambda s: \"|\".join(dict.fromkeys([x.strip() for v in s.dropna().astype(str) for x in v.split(\"|\") if x.strip()]))\n",
    "#             ),\n",
    "#             \"behind_zones\": grp[_pick_col(merged, \"behind_zones_nt\") or _pick_col(merged, \"behind_zones\")].apply(\n",
    "#                 lambda s: \"|\".join(dict.fromkeys([x.strip() for v in s.dropna().astype(str) for x in v.split(\"|\") if x.strip()]))\n",
    "#             ),\n",
    "\n",
    "#             # Traditional metrics: duplicates are identical -> mean == first\n",
    "#             \"traditional_ahead_mean\": grp[_pick_col(merged, \"traditional_ahead_mean\")].mean(),\n",
    "#             \"traditional_behind_mean\": grp[_pick_col(merged, \"traditional_behind_mean\")].mean(),\n",
    "#             \"traditional_aadt\":       grp[_pick_col(merged, \"traditional_aadt\")].mean(),\n",
    "\n",
    "#             # Non-traditional metrics: DO NOT sum across duplicate merge rows\n",
    "#             # Use mean (same as first, since NT is repeated on each duplicate row)\n",
    "#             \"non_trad_ahead_mean\": grp[_pick_col(merged, \"non_trad_ahead_mean\")].mean(),\n",
    "#             \"non_trad_behind_mean\":grp[_pick_col(merged, \"non_trad_behind_mean\")].mean(),\n",
    "#             \"non_trad_aadt\":       grp[_pick_col(merged, \"non_trad_aadt\")].mean(),\n",
    "\n",
    "#             # Counts / diagnostics: use max (not sum) to avoid doubling\n",
    "#             \"stl_ahead_rows\":          grp[_pick_col(merged, \"stl_ahead_rows\")].max(),\n",
    "#             \"stl_behind_rows\":         grp[_pick_col(merged, \"stl_behind_rows\")].max(),\n",
    "#             \"listed_ahead_segments\":   grp[_pick_col(merged, \"listed_ahead_segments\")].max(),\n",
    "#             \"listed_behind_segments\":  grp[_pick_col(merged, \"listed_behind_segments\")].max(),\n",
    "#             \"present_ahead_segments\":  grp[_pick_col(merged, \"present_ahead_segments\")].max(),\n",
    "#             \"present_behind_segments\": grp[_pick_col(merged, \"present_behind_segments\")].max(),\n",
    "\n",
    "#             # Missing lists: unique-join\n",
    "#             \"missing_ahead_zones\": grp[_pick_col(merged, \"missing_ahead_zones\")].apply(\n",
    "#                 lambda s: \"|\".join(dict.fromkeys([x.strip() for v in s.dropna().astype(str) for x in v.split(\"|\") if x.strip()]))\n",
    "#             ),\n",
    "#             \"missing_behind_zones\": grp[_pick_col(merged, \"missing_behind_zones\")].apply(\n",
    "#                 lambda s: \"|\".join(dict.fromkeys([x.strip() for v in s.dropna().astype(str) for x in v.split(\"|\") if x.strip()]))\n",
    "#             ),\n",
    "\n",
    "#             # Filters / metadata: stable representative\n",
    "#             \"daytype\":           grp[_pick_col(merged, \"daytype_trad\") or _pick_col(merged, \"daytype\")].first(),\n",
    "#             \"daytype_expected\":  grp[_pick_col(merged, \"daytype_expected\")].first(),\n",
    "#             \"daytype_used\":      grp[_pick_col(merged, \"daytype_used\")].first(),\n",
    "#             \"daypart_used\":      grp[_pick_col(merged, \"daypart_used\")].first(),\n",
    "#             \"modeoftravel_used\": grp[_pick_col(merged, \"modeoftravel_used\")].first(),\n",
    "#         }).reset_index(drop=True)\n",
    "\n",
    "#         # recompute counts and TCE\n",
    "#         out[\"n_objectids\"] = out[\"objectids\"].apply(lambda s: 0 if not isinstance(s, str) or not s.strip()\n",
    "#                                                     else len([x for x in s.split(\",\") if x]))\n",
    "#         # n_found_in_tc: take max (avoid doubling)\n",
    "#         if _pick_col(merged, \"n_found_in_tc\"):\n",
    "#             out[\"n_found_in_tc\"] = grp[_pick_col(merged, \"n_found_in_tc\")].max().values\n",
    "\n",
    "#         def _tce_row(row):\n",
    "#             t, n = row.get(\"traditional_aadt\"), row.get(\"non_trad_aadt\")\n",
    "#             return 100.0 * (n - t) / t if pd.notna(t) and t != 0 and pd.notna(n) else np.nan\n",
    "#         out[\"tce_percent\"] = out.apply(_tce_row, axis=1)\n",
    "\n",
    "#         preferred_cols = [\n",
    "#             \"location\",\"objectids\",\"n_objectids\",\"n_found_in_tc\",\n",
    "#             \"ahead_zones\",\"behind_zones\",\n",
    "#             \"traditional_ahead_mean\",\"traditional_behind_mean\",\"traditional_aadt\",\n",
    "#             \"non_trad_ahead_mean\",\"non_trad_behind_mean\",\"non_trad_aadt\",\n",
    "#             \"tce_percent\",\n",
    "#             \"daytype\",\"daytype_expected\",\"daytype_used\",\"daypart_used\",\"modeoftravel_used\",\n",
    "#             \"stl_ahead_rows\",\"stl_behind_rows\",\n",
    "#             \"missing_ahead_zones\",\"missing_behind_zones\",\n",
    "#             \"listed_ahead_segments\",\"listed_behind_segments\",\n",
    "#             \"present_ahead_segments\",\"present_behind_segments\",\n",
    "#         ]\n",
    "#         cols = [c for c in preferred_cols if c in out.columns]\n",
    "#         return out[cols].copy()\n",
    "\n",
    "\n",
    "def build_aadt_comparison_df(\n",
    "    aadt_locations,\n",
    "    df_tc,\n",
    "    df_stl,\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=None,\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust build that:\n",
    "      • Works with nested locations OR exploded objectid rows\n",
    "      • Merges Traditional + Non-Traditional\n",
    "      • Collapses duplicates to one row per physical location\n",
    "      • Sums STL per direction across dup rows, then sets non_trad_aadt = avg(ahead_sum, behind_sum)\n",
    "      • Keeps objectids as a '|'-joined string to avoid numeric/CSV formatting issues\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Build the two sides ---\n",
    "    trad_df = traditional_aadt_by_location(\n",
    "        aadt_locations=aadt_locations,\n",
    "        df_tc=df_tc,\n",
    "        as_df=True\n",
    "    )\n",
    "    nt_df = non_traditional_aadt_by_location(\n",
    "        aadt_locations=aadt_locations,\n",
    "        df_stl=df_stl,\n",
    "        daytype_filter=daytype_filter,\n",
    "        daypart_filter=daypart_filter,\n",
    "        modeoftravel_filter=modeoftravel_filter,\n",
    "        zonename_col=zonename_col,\n",
    "        stl_volume_col=stl_volume_col,\n",
    "        as_df=True\n",
    "    )\n",
    "\n",
    "    merged = pd.merge(\n",
    "        trad_df,\n",
    "        nt_df,\n",
    "        how=\"inner\",\n",
    "        on=\"location\",\n",
    "        suffixes=(\"_trad\", \"_nt\")\n",
    "    )\n",
    "\n",
    "    # --- 2) Normalize to a single location name (strip trailing \" [objectids]\") ---\n",
    "    merged[\"location_base\"] = merged[\"location\"].str.replace(r\"\\s*\\[[^\\]]+\\]\\s*$\", \"\", regex=True)\n",
    "\n",
    "    # Helpers\n",
    "    def uniq_join(series):\n",
    "        seen=set(); out=[]\n",
    "        for s in series.dropna().astype(str):\n",
    "            for tok in str(s).split(\"|\"):\n",
    "                tok = tok.strip()\n",
    "                if tok and tok not in seen:\n",
    "                    seen.add(tok); out.append(tok)\n",
    "        return \"|\".join(out)\n",
    "\n",
    "    def join_objectids(series):\n",
    "        toks=[]\n",
    "        for s in series.astype(str).fillna(\"\"):\n",
    "            # split anything that looks like a delimiter, keep digit runs\n",
    "            parts = re.findall(r\"\\d{1,}\", s)\n",
    "            toks.extend([p for p in parts if p])\n",
    "        # stable-unique\n",
    "        out=[]; seen=set()\n",
    "        for t in toks:\n",
    "            if t not in seen:\n",
    "                seen.add(t); out.append(t)\n",
    "        return \"|\".join(out)\n",
    "\n",
    "    # --- 3) Collapse duplicates BY location_base ---\n",
    "    agg = {\n",
    "        # Traditional (should agree across dup rows; mean is fine)\n",
    "        \"traditional_ahead_mean\": \"mean\",\n",
    "        \"traditional_behind_mean\": \"mean\",\n",
    "        \"traditional_aadt\": \"mean\",\n",
    "\n",
    "        # StreetLight directions: sum across dup rows\n",
    "        \"non_trad_ahead_mean\": \"sum\",\n",
    "        \"non_trad_behind_mean\": \"sum\",\n",
    "\n",
    "        # Segment/row counts: sum\n",
    "        \"stl_ahead_rows\": \"sum\",\n",
    "        \"stl_behind_rows\": \"sum\",\n",
    "        \"listed_ahead_segments\": \"sum\",\n",
    "        \"listed_behind_segments\": \"sum\",\n",
    "        \"present_ahead_segments\": \"sum\",\n",
    "        \"present_behind_segments\": \"sum\",\n",
    "\n",
    "        # String unions\n",
    "        \"ahead_zones\": uniq_join,\n",
    "        \"behind_zones\": uniq_join,\n",
    "        \"missing_ahead_zones\": uniq_join,\n",
    "        \"missing_behind_zones\": uniq_join,\n",
    "\n",
    "        # IDs / metadata\n",
    "        \"objectids\": join_objectids,\n",
    "        \"n_objectids\": \"sum\",\n",
    "        \"n_found_in_tc\": \"sum\",\n",
    "        \"daytype\": \"first\",\n",
    "        \"daytype_expected\": \"first\",\n",
    "        \"daytype_used\": \"first\",\n",
    "        \"daypart_used\": \"first\",\n",
    "        \"modeoftravel_used\": \"first\",\n",
    "    }\n",
    "    # only keep aggregations for columns we actually have\n",
    "    agg = {k:v for k,v in agg.items() if k in merged.columns}\n",
    "\n",
    "    out = (merged\n",
    "           .groupby(\"location_base\", as_index=False)\n",
    "           .agg(agg)\n",
    "           .rename(columns={\"location_base\":\"location\"}))\n",
    "\n",
    "    # Harden objectids as strings; recompute counts from the pipe-joined string\n",
    "    if \"objectids\" in out.columns:\n",
    "        out[\"objectids\"] = out[\"objectids\"].astype(str)\n",
    "        out[\"n_objectids\"] = out[\"objectids\"].str.split(r\"\\|\").apply(lambda xs: len([t for t in xs if t]))\n",
    "\n",
    "    # --- 4) Recompute STL combined AADT as the avg of the summed directions (or lone side if missing) ---\n",
    "    if {\"non_trad_ahead_mean\",\"non_trad_behind_mean\"}.issubset(out.columns):\n",
    "        a = pd.to_numeric(out[\"non_trad_ahead_mean\"], errors=\"coerce\")\n",
    "        b = pd.to_numeric(out[\"non_trad_behind_mean\"], errors=\"coerce\")\n",
    "        out[\"non_trad_aadt\"] = np.where(\n",
    "            a.notna() & b.notna(),\n",
    "            (a + b) / 2.0,\n",
    "            np.where(a.notna(), a, b)\n",
    "        )\n",
    "\n",
    "    # --- 5) Recompute TCE ---\n",
    "    if {\"traditional_aadt\",\"non_trad_aadt\"}.issubset(out.columns):\n",
    "        T = pd.to_numeric(out[\"traditional_aadt\"], errors=\"coerce\")\n",
    "        N = pd.to_numeric(out[\"non_trad_aadt\"], errors=\"coerce\")\n",
    "        out[\"tce_percent\"] = np.where(T.notna() & (T != 0) & N.notna(), 100.0*(N - T)/T, np.nan)\n",
    "\n",
    "    # Keep a clean, stable column order (only columns that exist)\n",
    "    preferred_cols = [\n",
    "        \"location\",\n",
    "        \"objectids\",\"n_objectids\",\"n_found_in_tc\",\n",
    "        \"ahead_zones\",\"behind_zones\",\n",
    "        \"traditional_ahead_mean\",\"traditional_behind_mean\",\"traditional_aadt\",\n",
    "        \"non_trad_ahead_mean\",\"non_trad_behind_mean\",\"non_trad_aadt\",\n",
    "        \"tce_percent\",\n",
    "        \"daytype\",\"daytype_expected\",\"daytype_used\",\"daypart_used\",\"modeoftravel_used\",\n",
    "        \"stl_ahead_rows\",\"stl_behind_rows\",\n",
    "        \"listed_ahead_segments\",\"listed_behind_segments\",\n",
    "        \"present_ahead_segments\",\"present_behind_segments\",\n",
    "        \"missing_ahead_zones\",\"missing_behind_zones\",\n",
    "    ]\n",
    "    cols = [c for c in preferred_cols if c in out.columns]\n",
    "    return out[cols].copy()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#     if collapse_to_one_per_location:\n",
    "#         # Build an aggregation map keyed on location_base\n",
    "#         grp = merged.groupby(\"location_base\", dropna=False)\n",
    "\n",
    "#         out = pd.DataFrame({\n",
    "#             # display name = base (OID-less)\n",
    "#             \"location\": grp[\"location_base\"].first(),\n",
    "\n",
    "#             # IDs & zones (unique-join)\n",
    "#             \"objectids\": grp.apply(lambda g: _uniq_join_commas(G(\"objectids_trad\").loc[g.index] if \"objectids_trad\" in merged.columns else G(\"objectids\").loc[g.index])),\n",
    "#             \"ahead_zones\": grp.apply(lambda g: _uniq_join(G(\"ahead_zones_nt\").loc[g.index] if \"ahead_zones_nt\" in merged.columns else G(\"ahead_zones\").loc[g.index])),\n",
    "#             \"behind_zones\": grp.apply(lambda g: _uniq_join(G(\"behind_zones_nt\").loc[g.index] if \"behind_zones_nt\" in merged.columns else G(\"behind_zones\").loc[g.index])),\n",
    "\n",
    "#             # Traditional metrics: use mean (equivalent to first if identical)\n",
    "#             \"traditional_ahead_mean\": grp[G(\"traditional_ahead_mean\").name].mean(),\n",
    "#             \"traditional_behind_mean\": grp[G(\"traditional_behind_mean\").name].mean(),\n",
    "#             \"traditional_aadt\": grp[G(\"traditional_aadt\").name].mean(),\n",
    "\n",
    "#             # Non-traditional metrics: sum across split rows (recombines directions)\n",
    "#             \"non_trad_ahead_mean\": grp[G(\"non_trad_ahead_mean\").name].sum(min_count=1),\n",
    "#             \"non_trad_behind_mean\": grp[G(\"non_trad_behind_mean\").name].sum(min_count=1),\n",
    "#             \"non_trad_aadt\": grp[G(\"non_trad_aadt\").name].sum(min_count=1),\n",
    "\n",
    "#             # Row counts / diagnostics: sum\n",
    "#             \"stl_ahead_rows\": grp[G(\"stl_ahead_rows\").name].sum(min_count=1),\n",
    "#             \"stl_behind_rows\": grp[G(\"stl_behind_rows\").name].sum(min_count=1),\n",
    "#             \"listed_ahead_segments\": grp[G(\"listed_ahead_segments\").name].sum(min_count=1),\n",
    "#             \"listed_behind_segments\": grp[G(\"listed_behind_segments\").name].sum(min_count=1),\n",
    "#             \"present_ahead_segments\": grp[G(\"present_ahead_segments\").name].sum(min_count=1),\n",
    "#             \"present_behind_segments\": grp[G(\"present_behind_segments\").name].sum(min_count=1),\n",
    "\n",
    "#             # Missing lists: unique-join\n",
    "#             \"missing_ahead_zones\": grp.apply(lambda g: _uniq_join(G(\"missing_ahead_zones\").loc[g.index])),\n",
    "#             \"missing_behind_zones\": grp.apply(lambda g: _uniq_join(G(\"missing_behind_zones\").loc[g.index])),\n",
    "\n",
    "#             # Filters / metadata: pick something consistent\n",
    "#             \"daytype\": grp[_pick_col(merged, \"daytype_trad\") or _pick_col(merged, \"daytype\")].first(),\n",
    "#             \"daytype_expected\": grp[_pick_col(merged, \"daytype_expected\")].first(),\n",
    "#             \"daytype_used\": grp[_pick_col(merged, \"daytype_used\")].first(),\n",
    "#             \"daypart_used\": grp[_pick_col(merged, \"daypart_used\")].first(),\n",
    "#             \"modeoftravel_used\": grp[_pick_col(merged, \"modeoftravel_used\")].first(),\n",
    "#         }).reset_index(drop=True)\n",
    "\n",
    "#         # n_objectids + n_found_in_tc after consolidation\n",
    "#         out[\"n_objectids\"] = out[\"objectids\"].apply(lambda s: 0 if not isinstance(s, str) or not s.strip() else len([x for x in s.split(\",\") if x]))\n",
    "#         if \"n_found_in_tc\" in merged.columns:\n",
    "#             # sum then clip to n_objectids (safety)\n",
    "#             n_found = grp[_pick_col(merged, \"n_found_in_tc\")].sum().values\n",
    "#             out[\"n_found_in_tc\"] = np.minimum(n_found, out[\"n_objectids\"].values)\n",
    "#         else:\n",
    "#             out[\"n_found_in_tc\"] = np.nan\n",
    "\n",
    "#         # Recompute TCE on the collapsed totals\n",
    "#         def _tce_row(row):\n",
    "#             t = row.get(\"traditional_aadt\")\n",
    "#             n = row.get(\"non_trad_aadt\")\n",
    "#             if pd.notna(t) and t != 0 and pd.notna(n):\n",
    "#                 return 100.0 * (n - t) / t\n",
    "#             return np.nan\n",
    "#         out[\"tce_percent\"] = out.apply(_tce_row, axis=1)\n",
    "\n",
    "#         # Final column order\n",
    "#         preferred_cols = [\n",
    "#             \"location\",\n",
    "#             \"objectids\", \"n_objectids\", \"n_found_in_tc\",\n",
    "#             \"ahead_zones\", \"behind_zones\",\n",
    "#             \"traditional_ahead_mean\", \"traditional_behind_mean\", \"traditional_aadt\",\n",
    "#             \"non_trad_ahead_mean\", \"non_trad_behind_mean\", \"non_trad_aadt\",\n",
    "#             \"tce_percent\",\n",
    "#             \"daytype\", \"daytype_expected\", \"daytype_used\", \"daypart_used\", \"modeoftravel_used\",\n",
    "#             \"stl_ahead_rows\", \"stl_behind_rows\",\n",
    "#             \"missing_ahead_zones\", \"missing_behind_zones\",\n",
    "#             \"listed_ahead_segments\", \"listed_behind_segments\",\n",
    "#             \"present_ahead_segments\", \"present_behind_segments\",\n",
    "#         ]\n",
    "#         cols = [c for c in preferred_cols if c in out.columns]\n",
    "#         return out[cols].copy()\n",
    "\n",
    "#     # --- fallback: no collapse requested ---\n",
    "#     # (kept from the previous version, but now with base-name merge)\n",
    "#     def _tce(row):\n",
    "#         t = row.get(_pick_col(merged, \"traditional_aadt\"))\n",
    "#         n = row.get(_pick_col(merged, \"non_trad_aadt\"))\n",
    "#         if pd.notna(t) and t != 0 and pd.notna(n):\n",
    "#             return 100.0 * (n - t) / t\n",
    "#         return np.nan\n",
    "#     merged[\"tce_percent\"] = merged.apply(_tce, axis=1)\n",
    "\n",
    "#     # prefer NT label, else trad; otherwise base\n",
    "#     loc_nt = _pick_col(merged, \"location_nt\")\n",
    "#     loc_tr = _pick_col(merged, \"location_trad\")\n",
    "#     if loc_nt:\n",
    "#         merged[\"location\"] = merged[loc_nt].apply(_base_location)  # strip OIDs if you still want one line per row\n",
    "#     elif loc_tr:\n",
    "#         merged[\"location\"] = merged[loc_tr].apply(_base_location)\n",
    "#     else:\n",
    "#         merged[\"location\"] = merged[\"location_base\"]\n",
    "\n",
    "#     preferred_cols = [\n",
    "#         \"location\",\n",
    "#         \"objectids\", \"n_objectids\", \"n_found_in_tc\", \"missing_objectids\",\n",
    "#         \"ahead_zones\", \"behind_zones\",\n",
    "#         \"traditional_ahead_mean\", \"traditional_behind_mean\", \"traditional_aadt\",\n",
    "#         \"non_trad_ahead_mean\", \"non_trad_behind_mean\", \"non_trad_aadt\",\n",
    "#         \"tce_percent\",\n",
    "#         \"daytype\", \"daytype_expected\", \"daytype_used\", \"daypart_used\", \"modeoftravel_used\",\n",
    "#         \"stl_ahead_rows\", \"stl_behind_rows\",\n",
    "#         \"missing_ahead_zones\", \"missing_behind_zones\",\n",
    "#         \"listed_ahead_segments\", \"listed_behind_segments\",\n",
    "#         \"present_ahead_segments\", \"present_behind_segments\",\n",
    "#     ]\n",
    "#     cols = [c for c in preferred_cols if c in merged.columns]\n",
    "#     return merged[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f183dac9-4407-491a-8e88-76339267e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1) Build the combined comparison DataFrame\n",
    "cmp_df = build_aadt_comparison_df(\n",
    "    aadt_locations=aadt_locations,  # your dict/list structure\n",
    "    df_tc=df_tc,                                 # Traffic Census dataframe\n",
    "    df_stl=df_stl,                               # StreetLight dataframe\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=None,                    # e.g., \"0: All Modes\" if you need it\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab6d0c76-b4a0-41a8-8ecd-ae3b2a05fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2) Quick peek\n",
    "#cmp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b5789ef-bea8-4369-9f79-b8ff23aef61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3) (Optional) sort by absolute TCE to see big deltas first\n",
    "cmp_df = cmp_df.sort_values(\"tce_percent\", key=lambda s: s.abs(), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfd90d29-5647-4cf8-ac6a-911e4cea503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4) Export to CSV \n",
    "cmp_df.to_csv(\"step_3_comparison_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0c8ed-8d3b-41d4-a68e-77daf218d4fc",
   "metadata": {},
   "source": [
    "### Step 3.5 Collapse to one row per location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd7cd1fe-0a6d-4c4c-9500-fe24d5d5dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collapse_to_one_row_per_location(cmp_df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     df = cmp_df.copy()\n",
    "\n",
    "#     # strip the trailing \" [.....]\" objectid tags from the location\n",
    "#     df[\"location_clean\"] = df[\"location\"].str.replace(r\"\\s*\\[[^\\]]+\\]\\s*$\", \"\", regex=True)\n",
    "\n",
    "#     # helper: union pipe-joined tokens in order\n",
    "#     def uniq_join(series):\n",
    "#         seen = set()\n",
    "#         out = []\n",
    "#         for s in series.dropna().astype(str):\n",
    "#             for tok in s.split(\"|\"):\n",
    "#                 tok = tok.strip()\n",
    "#                 if tok and tok not in seen:\n",
    "#                     seen.add(tok); out.append(tok)\n",
    "#         return \"|\".join(out)\n",
    "\n",
    "#     # helper: join unique objectids\n",
    "#     def join_objectids(series):\n",
    "#         toks = []\n",
    "#         for s in series.dropna().astype(str):\n",
    "#             toks.extend([t.strip() for t in s.split(\",\") if t.strip()])\n",
    "#         uniq = sorted(set(toks), key=lambda x: (len(x), x))  # stable-ish ordering\n",
    "#         return \",\".join(uniq)\n",
    "\n",
    "#     agg = {\n",
    "#         # Traffic Census (keep representative value; rows should agree)\n",
    "#         \"traditional_ahead_mean\": \"mean\",\n",
    "#         \"traditional_behind_mean\": \"mean\",\n",
    "#         \"traditional_aadt\": \"mean\",\n",
    "\n",
    "#         # StreetLight directions: **SUM across duplicate rows**\n",
    "#         \"non_trad_ahead_mean\": \"sum\",\n",
    "#         \"non_trad_behind_mean\": \"sum\",\n",
    "\n",
    "#         # Counts should sum\n",
    "#         \"stl_ahead_rows\": \"sum\",\n",
    "#         \"stl_behind_rows\": \"sum\",\n",
    "#         \"listed_ahead_segments\": \"sum\",\n",
    "#         \"listed_behind_segments\": \"sum\",\n",
    "#         \"present_ahead_segments\": \"sum\",\n",
    "#         \"present_behind_segments\": \"sum\",\n",
    "\n",
    "#         # Metadata / strings\n",
    "#         \"ahead_zones\": uniq_join,\n",
    "#         \"behind_zones\": uniq_join,\n",
    "#         \"missing_ahead_zones\": uniq_join,\n",
    "#         \"missing_behind_zones\": uniq_join,\n",
    "#         \"daytype\": \"first\",\n",
    "#         \"daytype_expected\": \"first\",\n",
    "#         \"daytype_used\": \"first\",\n",
    "#         \"daypart_used\": \"first\",\n",
    "#         \"modeoftravel_used\": \"first\",\n",
    "\n",
    "#         # IDs\n",
    "#         \"objectids\": join_objectids,\n",
    "#         \"n_objectids\": \"sum\",      # temporary; we’ll recompute below from objectids\n",
    "#         \"n_found_in_tc\": \"sum\",    # sum across duplicates\n",
    "#     }\n",
    "\n",
    "#     # only aggregate columns that exist\n",
    "#     agg = {k: v for k, v in agg.items() if k in df.columns}\n",
    "\n",
    "#     out = df.groupby(\"location_clean\", as_index=False).agg(agg)\n",
    "#     out = out.rename(columns={\"location_clean\": \"location\"})\n",
    "\n",
    "#     # Recompute n_objectids from the joined ID list (unique count)\n",
    "#     if \"objectids\" in out.columns:\n",
    "#         out[\"n_objectids\"] = out[\"objectids\"].apply(\n",
    "#             lambda s: len({t for t in str(s).split(\",\") if t})\n",
    "#         )\n",
    "\n",
    "#     # Recompute non_trad_aadt as the average of the two (summed) directions\n",
    "#     if {\"non_trad_ahead_mean\", \"non_trad_behind_mean\"}.issubset(out.columns):\n",
    "#         a = pd.to_numeric(out[\"non_trad_ahead_mean\"], errors=\"coerce\")\n",
    "#         b = pd.to_numeric(out[\"non_trad_behind_mean\"], errors=\"coerce\")\n",
    "#         out[\"non_trad_aadt\"] = np.where(\n",
    "#             a.notna() & b.notna(),\n",
    "#             (a + b) / 2.0,\n",
    "#             np.where(a.notna(), a, b)\n",
    "#         )\n",
    "\n",
    "#     # Recompute TCE with the updated non_trad_aadt\n",
    "#     if {\"traditional_aadt\", \"non_trad_aadt\"}.issubset(out.columns):\n",
    "#         T = pd.to_numeric(out[\"traditional_aadt\"], errors=\"coerce\")\n",
    "#         N = pd.to_numeric(out[\"non_trad_aadt\"], errors=\"coerce\")\n",
    "#         out[\"tce_percent\"] = np.where(\n",
    "#             T.notna() & (T != 0) & N.notna(),\n",
    "#             100.0 * (N - T) / T,\n",
    "#             np.nan\n",
    "#         )\n",
    "\n",
    "#     return out\n",
    "\n",
    "\n",
    "def collapse_to_one_row_per_location(cmp_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = cmp_df.copy()\n",
    "\n",
    "    # strip trailing \" [.....]\" tag\n",
    "    df[\"location_clean\"] = df[\"location\"].str.replace(r\"\\s*\\[[^\\]]+\\]\\s*$\", \"\", regex=True)\n",
    "\n",
    "    # union helper for pipe-joined strings\n",
    "    def uniq_join(series):\n",
    "        seen = set(); out = []\n",
    "        for s in series.dropna().astype(str):\n",
    "            for tok in s.split(\"|\"):\n",
    "                tok = tok.strip()\n",
    "                if tok and tok not in seen:\n",
    "                    seen.add(tok); out.append(tok)\n",
    "        return \"|\".join(out)\n",
    "\n",
    "    # join objectids with a PIPE to avoid CSV/number formatting issues\n",
    "    def join_objectids(series):\n",
    "        toks = []\n",
    "        for s in series.astype(str).fillna(\"\"):\n",
    "            # split on commas/pipes/spaces, keep digits\n",
    "            for t in re.split(r\"[,\\|\\s]+\", str(s)):\n",
    "                t = t.strip()\n",
    "                if t:\n",
    "                    toks.append(t)\n",
    "        # unique but stable-ish\n",
    "        uniq = []\n",
    "        seen = set()\n",
    "        for t in toks:\n",
    "            if t not in seen:\n",
    "                seen.add(t); uniq.append(t)\n",
    "        return \"|\".join(uniq)\n",
    "\n",
    "    agg = {\n",
    "        # TC side (rows should agree; mean is fine)\n",
    "        \"traditional_ahead_mean\": \"mean\",\n",
    "        \"traditional_behind_mean\": \"mean\",\n",
    "        \"traditional_aadt\": \"mean\",\n",
    "\n",
    "        # StreetLight directions: **SUM across duplicate rows**\n",
    "        \"non_trad_ahead_mean\": \"sum\",\n",
    "        \"non_trad_behind_mean\": \"sum\",\n",
    "\n",
    "        # Counts should sum\n",
    "        \"stl_ahead_rows\": \"sum\",\n",
    "        \"stl_behind_rows\": \"sum\",\n",
    "        \"listed_ahead_segments\": \"sum\",\n",
    "        \"listed_behind_segments\": \"sum\",\n",
    "        \"present_ahead_segments\": \"sum\",\n",
    "        \"present_behind_segments\": \"sum\",\n",
    "\n",
    "        # Strings/metadata\n",
    "        \"ahead_zones\": uniq_join,\n",
    "        \"behind_zones\": uniq_join,\n",
    "        \"missing_ahead_zones\": uniq_join,\n",
    "        \"missing_behind_zones\": uniq_join,\n",
    "        \"daytype\": \"first\",\n",
    "        \"daytype_expected\": \"first\",\n",
    "        \"daytype_used\": \"first\",\n",
    "        \"daypart_used\": \"first\",\n",
    "        \"modeoftravel_used\": \"first\",\n",
    "\n",
    "        # IDs\n",
    "        \"objectids\": join_objectids,\n",
    "        \"n_objectids\": \"sum\",\n",
    "        \"n_found_in_tc\": \"sum\",\n",
    "    }\n",
    "    agg = {k: v for k, v in agg.items() if k in df.columns}\n",
    "\n",
    "    out = df.groupby(\"location_clean\", as_index=False).agg(agg).rename(columns={\"location_clean\":\"location\"})\n",
    "\n",
    "    # Harden objectids: keep as strings and recompute counts\n",
    "    if \"objectids\" in out.columns:\n",
    "        out[\"objectids\"] = out[\"objectids\"].astype(str)\n",
    "        out[\"n_objectids\"] = out[\"objectids\"].str.split(r\"\\|\").apply(lambda xs: len([t for t in xs if t]))\n",
    "\n",
    "    # Recompute non_trad_aadt as the avg of the two summed directions\n",
    "    if {\"non_trad_ahead_mean\",\"non_trad_behind_mean\"}.issubset(out.columns):\n",
    "        a = pd.to_numeric(out[\"non_trad_ahead_mean\"], errors=\"coerce\")\n",
    "        b = pd.to_numeric(out[\"non_trad_behind_mean\"], errors=\"coerce\")\n",
    "        out[\"non_trad_aadt\"] = np.where(a.notna() & b.notna(), (a+b)/2.0, np.where(a.notna(), a, b))\n",
    "\n",
    "    # Recompute TCE with updated non_trad_aadt\n",
    "    if {\"traditional_aadt\",\"non_trad_aadt\"}.issubset(out.columns):\n",
    "        T = pd.to_numeric(out[\"traditional_aadt\"], errors=\"coerce\")\n",
    "        N = pd.to_numeric(out[\"non_trad_aadt\"], errors=\"coerce\")\n",
    "        out[\"tce_percent\"] = np.where(T.notna() & (T!=0) & N.notna(), 100.0*(N-T)/T, np.nan)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "978cfa4a-9d42-4017-af43-0f445285898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the collapse to one row function\n",
    "cmp_df = collapse_to_one_row_per_location(cmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b80a7faa-00a2-421a-b960-86257838ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4) Export to CSV \n",
    "cmp_df.to_csv(\"step_3_5_collapse_to_one_row.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaade07-ead0-4958-ac76-cf4a98169217",
   "metadata": {},
   "source": [
    "## Step 4 Confidence Interval over TCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c453dbb8-0d46-4847-b296-00aee2b97787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------------------------------\n",
    "# # 4) Confidence interval over TCE\n",
    "# # ------------------------------------------------------\n",
    "\n",
    "def _prep_tces(detail_df, tce_col=\"tce_percent\", cap_abs=None, winsor_pct=None):\n",
    "    # Extract, coerce, and clean\n",
    "    s = pd.to_numeric(detail_df[tce_col], errors=\"coerce\").replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    dropped = 0\n",
    "    if cap_abs is not None:\n",
    "        mask = s.abs() <= float(cap_abs)\n",
    "        dropped = int((~mask).sum())\n",
    "        s = s[mask]\n",
    "\n",
    "    # Optional winsorization\n",
    "    if winsor_pct is not None and 0 < winsor_pct < 0.5 and len(s) > 0:\n",
    "        lo = s.quantile(winsor_pct)\n",
    "        hi = s.quantile(1 - winsor_pct)\n",
    "        s = s.clip(lower=lo, upper=hi)\n",
    "\n",
    "    return s.astype(float), dropped\n",
    "\n",
    "def tce_confidence_interval(\n",
    "    detail_df,\n",
    "    confidence=0.95,\n",
    "    tce_col=\"tce_percent\",\n",
    "    cap_abs=None,          # e.g., 500 trims extreme %s\n",
    "    winsor_pct=None        # e.g., 0.01 winsorizes 1% tails\n",
    "):\n",
    "    \"\"\"\n",
    "    One-sample t CI on TCE (%) vs 0.\n",
    "    Returns: (mean_tce, ci_lo, ci_hi, tcrit, t_stat)\n",
    "    \"\"\"\n",
    "    tces, dropped = _prep_tces(detail_df, tce_col=tce_col, cap_abs=cap_abs, winsor_pct=winsor_pct)\n",
    "    n = int(tces.shape[0])\n",
    "    if n == 0:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    mean_tce = float(tces.mean())\n",
    "\n",
    "    if n > 1:\n",
    "        std_tce = float(tces.std(ddof=1))\n",
    "        se = std_tce / np.sqrt(n) if std_tce > 0 else 0.0\n",
    "        if se > 0:\n",
    "            dof = n - 1\n",
    "            tcrit = float(stats.t.ppf((1 + confidence) / 2.0, dof))\n",
    "            ci_lo = mean_tce - tcrit * se\n",
    "            ci_hi = mean_tce + tcrit * se\n",
    "            t_stat = mean_tce / se\n",
    "        else:\n",
    "            tcrit = ci_lo = ci_hi = t_stat = None\n",
    "    else:\n",
    "        tcrit = ci_lo = ci_hi = t_stat = None\n",
    "\n",
    "    return mean_tce, ci_lo, ci_hi, tcrit, t_stat\n",
    "\n",
    "def tce_confidence_interval_df(\n",
    "    detail_df,\n",
    "    confidence=0.95,\n",
    "    tce_col=\"tce_percent\",\n",
    "    cap_abs=None,          # drop rows with |tce| > cap_abs\n",
    "    winsor_pct=None        # winsorize tails by this fraction\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Same as tce_confidence_interval, with a one-row DataFrame and diagnostics.\n",
    "    \"\"\"\n",
    "    tces, dropped = _prep_tces(detail_df, tce_col=tce_col, cap_abs=cap_abs, winsor_pct=winsor_pct)\n",
    "    n = int(tces.shape[0])\n",
    "\n",
    "    if n == 0:\n",
    "        return pd.DataFrame([{\n",
    "            \"confidence\": confidence,\n",
    "            \"tce_col\": tce_col,\n",
    "            \"n\": 0,\n",
    "            \"dof\": None,\n",
    "            \"mean_tce\": None,\n",
    "            \"std_tce\": None,\n",
    "            \"se\": None,\n",
    "            \"t_critical\": None,\n",
    "            \"margin_of_error\": None,\n",
    "            \"ci_lower\": None,\n",
    "            \"ci_upper\": None,\n",
    "            \"t_statistic\": None,\n",
    "            \"p_value_two_sided\": None,\n",
    "            \"cohens_d\": None,\n",
    "            \"count_dropped\": int(dropped),\n",
    "            \"cap_abs\": cap_abs,\n",
    "            \"winsor_pct\": winsor_pct\n",
    "        }])\n",
    "\n",
    "    mean_tce = float(tces.mean())\n",
    "\n",
    "    if n > 1:\n",
    "        std_tce = float(tces.std(ddof=1))\n",
    "        se = std_tce / np.sqrt(n) if std_tce > 0 else 0.0\n",
    "        dof = n - 1\n",
    "\n",
    "        if se > 0:\n",
    "            tcrit = float(stats.t.ppf((1 + confidence) / 2.0, dof))\n",
    "            moe = tcrit * se\n",
    "            ci_lo = mean_tce - moe\n",
    "            ci_hi = mean_tce + moe\n",
    "            t_stat = mean_tce / se\n",
    "            p_val = float(2 * (1 - stats.t.cdf(abs(t_stat), dof)))\n",
    "            cohens_d = mean_tce / std_tce if std_tce > 0 else None\n",
    "        else:\n",
    "            tcrit = moe = ci_lo = ci_hi = t_stat = p_val = cohens_d = None\n",
    "    else:\n",
    "        std_tce = None\n",
    "        se = None\n",
    "        dof = None\n",
    "        tcrit = moe = ci_lo = ci_hi = t_stat = p_val = cohens_d = None\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        \"confidence\": confidence,\n",
    "        \"tce_col\": tce_col,\n",
    "        \"n\": n,\n",
    "        \"dof\": dof,\n",
    "        \"mean_tce\": mean_tce,\n",
    "        \"std_tce\": std_tce,\n",
    "        \"se\": se,\n",
    "        \"t_critical\": tcrit,\n",
    "        \"margin_of_error\": moe,\n",
    "        \"ci_lower\": ci_lo,\n",
    "        \"ci_upper\": ci_hi,\n",
    "        \"t_statistic\": t_stat,\n",
    "        \"p_value_two_sided\": p_val,\n",
    "        \"cohens_d\": cohens_d,\n",
    "        \"count_dropped\": int(dropped),\n",
    "        \"cap_abs\": cap_abs,\n",
    "        \"winsor_pct\": winsor_pct\n",
    "    }])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "241190c1-880c-4346-89d8-16d9f93e2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0) Normalize to objectid rows (works for either of your location formats)\n",
    "norm_rows = explode_locations_to_objectids(aadt_locations)  # or sr_605_d7_tc_aadt_locations\n",
    "\n",
    "# 4.1) Build comparison\n",
    "cmp_df = build_aadt_comparison_df(\n",
    "    aadt_locations=norm_rows,\n",
    "    df_tc=df_tc,\n",
    "    df_stl=df_stl,\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=None,\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44b695fb-0e44-4645-ae8f-6fcb17175f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2) Get the CI summary as a DataFrame\n",
    "# tce_summary_df = tce_confidence_interval_df(cmp_df, confidence=0.95)\n",
    "tce_summary_df = tce_confidence_interval_df(cmp_df, confidence=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d1a1f62-dec4-4836-8a54-6b25852def26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   confidence      tce_col   n  dof  mean_tce    std_tce        se  \\\n",
      "0        0.95  tce_percent  49   48 -3.703764  23.388965  3.341281   \n",
      "\n",
      "   t_critical  margin_of_error   ci_lower  ci_upper  t_statistic  \\\n",
      "0    2.010635         6.718095 -10.421859  3.014331    -1.108486   \n",
      "\n",
      "   p_value_two_sided  cohens_d  count_dropped cap_abs winsor_pct  \n",
      "0           0.273175 -0.158355              0    None       None  \n"
     ]
    }
   ],
   "source": [
    "# 4.3) Quick peek\n",
    "print(tce_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28aa3c47-86df-4bd4-9cbc-5799b8b3f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4) Export to CSV \n",
    "cmp_df.to_csv(\"step_4_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b665779a-9ff5-4a08-8a83-788ab9b90caa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean TCE: -3.703764130718093\n",
      "95% Confidence Interval: (-10.42185922932066, 3.014330967884474)\n",
      "t-test statistic: -1.1084863768619073\n",
      "t-critical: 2.010634757624232\n"
     ]
    }
   ],
   "source": [
    "mean_tce, ci_lower, ci_upper, t_critical, t_statistic = tce_confidence_interval(\n",
    "    cmp_df, confidence=0.95\n",
    ")\n",
    "\n",
    "print(\"Mean TCE:\", mean_tce)\n",
    "print(\"95% Confidence Interval:\", (ci_lower, ci_upper))\n",
    "print(\"t-test statistic:\", t_statistic)\n",
    "print(\"t-critical:\", t_critical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a515f0f-d033-4e3f-97e0-c2174789a864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87708bdc-d5d9-4da5-8259-aea807e17beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ece7a389-e31e-43a2-9cf4-ccb7e7eec747",
   "metadata": {},
   "source": [
    "### Mean TCE: -3.62\n",
    "Traffic Census Error (TCE)\n",
    "* A negative TCE of -3.62% means that on average, the StreetLight AADT estimates are about 3.62% lower than the official Caltrans Traffic Census counts.\n",
    "\n",
    "### 95% Confidence Interval (-10.78%, 3.54%)\n",
    "* Based on the sample of locations, the results suggest 95% confidence that the true average TCE (i.e., the average percent difference between StreetLight and Census across the entire population) falls somewhere between -10.78% and +3.54%.\n",
    "    * Since this interval includes zero, it's possible that the true average error is zero, meaning StreetLight might not be significantly over- or underestimating, on average.\n",
    "    * But the range is quite wide (~14 percentage points), which indicates some variability in the data or a small sample size.\n",
    "\n",
    "### T-Test Statistic  \n",
    "* **-1.059**: This means your observed sample mean is about **1.059 standard errors** below the expected population mean. Since it's not far enough from the threshold (2.093), the result is **not significant**.\n",
    "\n",
    "### Summary\n",
    "* On average, StreetLight data is underestimating AADT by about 3.6% on this subset of locations.\n",
    "* But with 95% confidence, the actual average error could be as much as 10.8% under or 3.5% over the true value.\n",
    "* Because zero is in that range, you can't definitively say it's underestimating — the difference might not be statistically significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84bef9d-85b4-44d0-aa13-ebfa9ed7ef73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d35c3b37-f7e4-41e8-abf2-083155ea8641",
   "metadata": {},
   "source": [
    "# AADT Confidence Interval - Interstate 605, District 7\n",
    "\n",
    "## FHWA Links\n",
    "* Guidelines for Obtaining AADT Estimates from Non-Traditional Sources:\n",
    "    * https://www.fhwa.dot.gov/policyinformation/travel_monitoring/pubs/aadtnt/Guidelines_for_AADT_Estimates_Final.pdf\n",
    "\n",
    "## AADT Analysis Locations\n",
    "* Locations were determined based on the location on installed & recording Traffic Operations cameras\n",
    "    * for additional information contact Zhenyu Zhu with Traffic Operations\n",
    "\n",
    "## Traffic Census Data\n",
    "* https://dot.ca.gov/programs/traffic-operations/census/traffic-volumes\n",
    "* Back AADT, Peak Month, and Peak Hour usually represents traffic South or West of the count location.  \n",
    "* Ahead AADT, Peak Month, and Peak Hour usually represents traffic North or East of the count location. Listing of routes with their designated  \n",
    "\n",
    "* Because the Back & Ahead counts are included at each location in the Traffic Census Data, (e.g., \"IRWINDALE, ARROW HIGHWAY\") only one [OBJECTID*] per location was pulled; for this analysis the North Bound Nodes were used for the analysis. \n",
    "    * for more information see the diagram: https://traffic.onramp.dot.ca.gov/downloads/traffic/files/performance/census/Back_and_Ahead_Leg_Traffic_Count_Diagram.pdf\n",
    "\n",
    "## StreetLight Analysis Data\n",
    "* Analysis Type == Network Performance\n",
    "* Segment Metrics\n",
    "* 2022 was used to match currently available Traffic Census Data (as of 8/27/2025)\n",
    "* pulled a variety of Day Types, but plan to just look at \"\"\"All Day Types\"\"\"\n",
    "* pulled a variety of Day Parts, but plan to just look at \"\"\"All Day Parts\"\"\"\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
