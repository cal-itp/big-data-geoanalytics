{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d35c3b37-f7e4-41e8-abf2-083155ea8641",
   "metadata": {},
   "source": [
    "# AADT Confidence Interval - State Route 99, District 3\n",
    "\n",
    "## FHWA Links\n",
    "* Guidelines for Obtaining AADT Estimates from Non-Traditional Sources:\n",
    "    * https://www.fhwa.dot.gov/policyinformation/travel_monitoring/pubs/aadtnt/Guidelines_for_AADT_Estimates_Final.pdf\n",
    "  \n",
    "  \n",
    "## AADT Analysis Locations\n",
    "* 10 locations were used in the analysis\n",
    "* Locations were determined based on the location on installed & recording Traffic Operations cameras\n",
    "    * for additional information contact Zhenyu Zhu with Traffic Operations\n",
    "\n",
    "## Traffic Census Data\n",
    "* https://dot.ca.gov/programs/traffic-operations/census/traffic-volumes\n",
    "* Back AADT, Peak Month, and Peak Hour usually represents traffic South or West of the count location.  \n",
    "* Ahead AADT, Peak Month, and Peak Hour usually represents traffic North or East of the count location. Listing of routes with their designated  \n",
    "\n",
    "* Because the Back & Ahead counts are included at each location in the Traffic Census Data, (e.g., \"IRWINDALE, ARROW HIGHWAY\") only one [OBJECTID*] per location was pulled; for this analysis the North Bound Nodes were used for the analysis. \n",
    "    * for more information see the diagram: https://traffic.onramp.dot.ca.gov/downloads/traffic/files/performance/census/Back_and_Ahead_Leg_Traffic_Count_Diagram.pdf\n",
    "\n",
    "## StreetLight Analysis Data\n",
    "* StreetLight Locations on Interstate 99 are one-direction, each location will contain two points: northbound and southbound\n",
    "    * Analysis Type == Network Performance\n",
    "    * Segment Metrics\n",
    "    * 2022 was used to match currently available Traffic Census Data (as of 8/27/2025)\n",
    "    * pulled a variety of Day Types, but plan to just look at \"\"\"All Day Types\"\"\"\n",
    "    * pulled a variety of Day Parts, but plan to just look at \"\"\"All Day Parts\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e192bde-dff4-46cf-b2cd-da8347440ec5",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "171fa2b0-9b02-4947-a4d1-dd18ef86de42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5394f0e-d012-42ad-9286-c95948da43aa",
   "metadata": {},
   "source": [
    "### Pull in the Location Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6bce3a-11a1-4f2a-a7d9-eb7904946817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pull in the coordinates from the utils docs\n",
    "#from osow_frp_o_d_utils_v3 import origin_intersections, destination_intersections\n",
    "from shs_ct_tc_locations_utils import sr_99_d3_tc_aadt_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0a612-d979-43d3-87f3-e8d69df16d42",
   "metadata": {},
   "source": [
    "### Identify the Google Cloud Storage path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce4826a9-4c1c-46e4-95c5-20c820351b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the GCS path to the data\n",
    "gcs_path = \"gs://calitp-analytics-data/data-analyses/big_data/compare_traffic_counts/sr99_d3/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628fa29-7fdc-46d2-bc5a-84c271f150d9",
   "metadata": {},
   "source": [
    "## Step 0, Pull in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40c01657-5051-44da-b79f-594359c06427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will pull in the data and clean the column headers in a way that will make them easier to work with\n",
    "def getdata_and_cleanheaders(path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Clean column headers: remove spaces, convert to lowercase, and strip trailing asterisks\n",
    "    cleaned_columns = []\n",
    "    for column in df.columns:\n",
    "        cleaned_column = column.replace(\" \", \"\").lower().rstrip(\"*\")\n",
    "        cleaned_columns.append(cleaned_column)\n",
    "\n",
    "    df.columns = cleaned_columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "686ea444-0d68-415c-beda-7ecdfe887062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pull in the data & create dataframes\n",
    "df_tc = getdata_and_cleanheaders(f\"{gcs_path}caltrans_traffic_census_2022.csv\")  # Traffic Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a890154-e2c9-435a-b136-48d932f3359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the StreetLight Analysis to be used in the AADT comparison\n",
    "df_stl = getdata_and_cleanheaders(f\"{gcs_path}streetlight_sr99_d3_all_vehicles_2022_np.csv\")  # StreetLight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b73b9a53-0066-4643-b8b7-ec90ceec2f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# comparing\n",
    "df_tc.to_csv(\"df_tc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59259345-ec3c-47e8-bd2d-cf5059babf9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# comparing\n",
    "df_stl.to_csv(\"df_stl.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7480907-d484-4e8f-8e53-eeb4517487ce",
   "metadata": {},
   "source": [
    "## Step 1, Build a per-location summary of Traffic Census locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "316ff939-7384-4f6d-86bc-109a2a6a54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def traditional_aadt_by_location(aadt_locations, df_tc, as_df=True):\n",
    "    \"\"\"\n",
    "    Build a per-location summary of *traditional* (Traffic Census) AADT.\n",
    "    Output columns:\n",
    "      location, daytype, objectids, n_objectids, n_found_in_tc, missing_objectids,\n",
    "      traditional_ahead_mean, traditional_behind_mean, traditional_aadt\n",
    "    \"\"\"\n",
    "\n",
    "    def _ensure_list(x):\n",
    "        if x is None: return []\n",
    "        if isinstance(x, (list, tuple, set)): return list(x)\n",
    "        return [x]\n",
    "\n",
    "    def _gather_objectids(node_dict):\n",
    "        ids = []\n",
    "        if not isinstance(node_dict, dict): return ids\n",
    "        if \"objectid\"  in node_dict: ids.extend(_ensure_list(node_dict[\"objectid\"]))\n",
    "        if \"objectids\" in node_dict: ids.extend(_ensure_list(node_dict[\"objectids\"]))\n",
    "        return [str(i) for i in ids if i is not None and str(i).strip() != \"\"]\n",
    "\n",
    "    def _dedup(seq):\n",
    "        seen=set(); out=[]\n",
    "        for x in seq:\n",
    "            if x not in seen:\n",
    "                out.append(x); seen.add(x)\n",
    "        return out\n",
    "\n",
    "    def _normalize_one_location(name, loc):\n",
    "        nodes = loc.get(\"nodes\", {}) or {}\n",
    "        all_ids=[]\n",
    "        for _, node in nodes.items():\n",
    "            all_ids.extend(_gather_objectids(node))\n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"daytype\": loc.get(\"daytype\", \"0: All Days (M-Su)\"),\n",
    "            \"objectids\": _dedup(all_ids),\n",
    "        }\n",
    "\n",
    "    def _normalize_input(aadt_locs):\n",
    "        if isinstance(aadt_locs, pd.DataFrame) and \\\n",
    "           {\"name\",\"daytype\",\"objectids\"}.issubset(aadt_locs.columns):\n",
    "            return aadt_locs.to_dict(orient=\"records\")\n",
    "        if isinstance(aadt_locs, list) and aadt_locs and isinstance(aadt_locs[0], dict) and \\\n",
    "           {\"name\",\"daytype\",\"objectids\"}.issubset(aadt_locs[0].keys()):\n",
    "            return aadt_locs\n",
    "\n",
    "        recs = []\n",
    "        if isinstance(aadt_locs, dict):\n",
    "            for nm, loc in aadt_locs.items():\n",
    "                recs.append(_normalize_one_location(nm, loc))\n",
    "        elif isinstance(aadt_locs, list):\n",
    "            for item in aadt_locs:\n",
    "                if isinstance(item, dict) and \"nodes\" in item:  # single location dict\n",
    "                    nm = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "                    recs.append(_normalize_one_location(nm, item))\n",
    "                elif isinstance(item, dict):  # dict keyed by name\n",
    "                    for nm, loc in item.items():\n",
    "                        recs.append(_normalize_one_location(nm, loc))\n",
    "        return recs\n",
    "\n",
    "    def _traditional_aadt_for_ids(df_tc_in, obj_ids):\n",
    "        if not obj_ids:\n",
    "            return np.nan, np.nan, np.nan, 0\n",
    "        sub = df_tc_in[df_tc_in[\"objectid\"].astype(str).isin(obj_ids)]\n",
    "        if sub.empty:\n",
    "            return np.nan, np.nan, np.nan, 0\n",
    "        ahead_vals = pd.to_numeric(sub.get(\"ahead_aadt\"), errors=\"coerce\").dropna()\n",
    "        back_vals  = pd.to_numeric(sub.get(\"back_aadt\"),  errors=\"coerce\").dropna()\n",
    "        mean_ahead = ahead_vals.mean() if not ahead_vals.empty else np.nan\n",
    "        mean_back  = back_vals.mean()  if not back_vals.empty  else np.nan\n",
    "        overall = np.nanmean([mean_ahead, mean_back])\n",
    "        count_used = len(sub)\n",
    "        return overall, mean_ahead, mean_back, count_used\n",
    "\n",
    "    norm = _normalize_input(aadt_locations)\n",
    "    tc_ids_all = set(df_tc[\"objectid\"].astype(str).unique())\n",
    "\n",
    "    rows = []\n",
    "    for loc in norm:\n",
    "        obj_ids = [str(x) for x in (loc.get(\"objectids\") or [])]\n",
    "        overall, mean_ahead, mean_back, n_found = _traditional_aadt_for_ids(df_tc, obj_ids)\n",
    "        missing = [x for x in obj_ids if x not in tc_ids_all]\n",
    "\n",
    "        # >>> CHANGE #1: pipe-join for display/export\n",
    "        objectids_display = \"|\".join(obj_ids)\n",
    "\n",
    "        rows.append({\n",
    "            \"location\": loc.get(\"name\"),\n",
    "            \"daytype\":  loc.get(\"daytype\"),\n",
    "            \"objectids\": objectids_display,   # pipe-separated string\n",
    "            \"n_objectids\": len(obj_ids),\n",
    "            \"n_found_in_tc\": int(n_found),\n",
    "            \"missing_objectids\": \"|\".join(missing) if missing else \"\",\n",
    "            \"traditional_ahead_mean\": mean_ahead,\n",
    "            \"traditional_behind_mean\": mean_back,\n",
    "            \"traditional_aadt\": overall,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows) if as_df else rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db115308-4010-4c5a-a51c-49db69dfaa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run step 1 - traditional aadt counts\n",
    "trad_df = traditional_aadt_by_location(sr_99_d3_tc_aadt_locations, df_tc, as_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e23b26a-69a2-4889-b04b-14cc250a504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trad_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69ea7086-b4d3-47eb-bf6e-bae9c86c506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Step 1 as a CSV to take a look\n",
    "trad_df.to_csv(\"step_1_traditional_aadt_by_location.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30ac04-cfd4-45bc-a85f-16437dbafdd2",
   "metadata": {},
   "source": [
    "## Step 2 Identify Traffic Census location names for the StreetLight segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69e68861-59c8-4426-bf8c-16db1ea1ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# Identify the Traffic Census Location name for the StreetLight segments\n",
    "    # Traffic Census locations can either be bi-directional or uni-directional. \n",
    "        # If they are bi-directional they will only have one node per location, \n",
    "        # if they are uni-directional there will be two nodes per location. \n",
    "    # Each Traffic Census location has one an [AHEAD_AADT] and one [BEHIND_AADT] count per location node - so each Traffic Census location also has at least two StreetLight locations, \n",
    "        # but each location typically has four StreetLight segments (two Traffic Census nodes (e.g., Northbound & Southbound, each node has two StreetLight segments)\n",
    "\n",
    "def non_traditional_aadt_by_location(\n",
    "    aadt_locations,\n",
    "    df_stl,\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=None,\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\",\n",
    "    as_df=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a per-location summary of *non-traditional* (StreetLight) AADT.\n",
    "\n",
    "    Output columns (one row per location):\n",
    "      location, daytype_expected, daytype_used, daypart_used, modeoftravel_used,\n",
    "      ahead_zones, behind_zones,\n",
    "      non_trad_ahead_mean, non_trad_behind_mean, non_trad_aadt,\n",
    "      stl_ahead_rows, stl_behind_rows, missing_ahead_zones, missing_behind_zones\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- helpers ----\n",
    "    def _ensure_list(x):\n",
    "        if x is None: return []\n",
    "        if isinstance(x, (list, tuple, set)): return list(x)\n",
    "        return [x]\n",
    "\n",
    "    def _gather_zones(node_dict):\n",
    "        ahead  = _ensure_list(node_dict.get(\"zonename_ahead\", []))\n",
    "        behind = _ensure_list(node_dict.get(\"zonename_behind\", []))\n",
    "        return ahead, behind\n",
    "\n",
    "    def _dedup(seq):\n",
    "        seen=set(); out=[]\n",
    "        for x in seq:\n",
    "            if x not in seen:\n",
    "                out.append(x); seen.add(x)\n",
    "        return out\n",
    "\n",
    "    def _normalize_one_location(name, loc):\n",
    "        nodes = loc.get(\"nodes\", {}) or {}\n",
    "        ahead, behind = [], []\n",
    "        for _, node in nodes.items():\n",
    "            a, b = _gather_zones(node)\n",
    "            ahead.extend([z for z in a if z])\n",
    "            behind.extend([z for z in b if z])\n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"daytype\": loc.get(\"daytype\", \"0: All Days (M-Su)\"),\n",
    "            \"ahead_zones\": _dedup(ahead),\n",
    "            \"behind_zones\": _dedup(behind),\n",
    "        }\n",
    "\n",
    "    def _normalize_input(aadt_locs):\n",
    "        # already normalized DataFrame?\n",
    "        if isinstance(aadt_locs, pd.DataFrame) and \\\n",
    "           {\"name\",\"daytype\",\"ahead_zones\",\"behind_zones\"}.issubset(aadt_locs.columns):\n",
    "            return aadt_locs.to_dict(orient=\"records\")\n",
    "        # already normalized list[dict]?\n",
    "        if isinstance(aadt_locs, list) and aadt_locs and isinstance(aadt_locs[0], dict) and \\\n",
    "           {\"name\",\"daytype\",\"ahead_zones\",\"behind_zones\"}.issubset(aadt_locs[0].keys()):\n",
    "            return aadt_locs\n",
    "\n",
    "        recs = []\n",
    "        if isinstance(aadt_locs, dict):\n",
    "            for nm, loc in aadt_locs.items():\n",
    "                recs.append(_normalize_one_location(nm, loc))\n",
    "        elif isinstance(aadt_locs, list):\n",
    "            for item in aadt_locs:\n",
    "                if isinstance(item, dict) and \"nodes\" in item:  # single location dict\n",
    "                    nm = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "                    recs.append(_normalize_one_location(nm, item))\n",
    "                elif isinstance(item, dict):  # dict keyed by name\n",
    "                    for nm, loc in item.items():\n",
    "                        recs.append(_normalize_one_location(nm, loc))\n",
    "        return recs\n",
    "\n",
    "    # ---- filter & precompute per-zone means ----\n",
    "    must_cols = [zonename_col, stl_volume_col, \"daytype\", \"daypart\"]\n",
    "    for c in must_cols:\n",
    "        if c not in df_stl.columns:\n",
    "            raise KeyError(f\"df_stl is missing required column: {c}\")\n",
    "\n",
    "    filt = (df_stl[\"daytype\"] == daytype_filter) & (df_stl[\"daypart\"] == daypart_filter)\n",
    "    if modeoftravel_filter and (\"modeoftravel\" in df_stl.columns):\n",
    "        filt = filt & (df_stl[\"modeoftravel\"] == modeoftravel_filter)\n",
    "\n",
    "    stl_filtered = df_stl.loc[filt, [zonename_col, stl_volume_col]].copy()\n",
    "    # Clean types\n",
    "    stl_filtered[zonename_col] = stl_filtered[zonename_col].astype(str).str.strip()\n",
    "    stl_filtered[stl_volume_col] = pd.to_numeric(stl_filtered[stl_volume_col], errors=\"coerce\")\n",
    "\n",
    "    # Compute per-zonename averages (handles duplicates safely)\n",
    "    zone_group = stl_filtered.groupby(zonename_col)[stl_volume_col]\n",
    "    zone_mean = zone_group.mean()   # pd.Series: index=zonename, value=mean volume\n",
    "    zone_rows = zone_group.size()   # pd.Series: index=zonename, value=row count backing the mean\n",
    "    present_zones = set(zone_mean.index)\n",
    "\n",
    "    def _zone_stats(zones):\n",
    "        \"\"\"Return (mean_of_zone_means, backing_row_count_sum, missing_list) for a list of zonenames.\"\"\"\n",
    "        zones = [str(z).strip() for z in _ensure_list(zones) if z and str(z).strip() != \"\"]\n",
    "        if not zones:\n",
    "            return np.nan, 0, []\n",
    "        present = [z for z in zones if z in present_zones]\n",
    "        missing = [z for z in zones if z not in present_zones]\n",
    "        means = zone_mean.reindex(present).dropna()\n",
    "        mean_val = means.mean() if len(means) else np.nan\n",
    "        # how many filtered rows contributed across these zones (debug)\n",
    "        n_rows = int(zone_rows.reindex(present).fillna(0).sum())\n",
    "        return mean_val, n_rows, missing\n",
    "\n",
    "    # ---- build rows ----\n",
    "    norm = _normalize_input(aadt_locations)\n",
    "    rows = []\n",
    "    for loc in norm:\n",
    "        ahead = _ensure_list(loc.get(\"ahead_zones\", []))\n",
    "        behind = _ensure_list(loc.get(\"behind_zones\", []))\n",
    "\n",
    "        mean_ahead, ahead_n, miss_a = _zone_stats(ahead)\n",
    "        mean_behind, behind_n, miss_b = _zone_stats(behind)\n",
    "        overall = np.nanmean([mean_ahead, mean_behind])\n",
    "\n",
    "        rows.append({\n",
    "            \"location\": loc.get(\"name\"),\n",
    "            \"daytype_expected\": loc.get(\"daytype\"),\n",
    "            \"daytype_used\": daytype_filter,\n",
    "            \"daypart_used\": daypart_filter,\n",
    "            \"modeoftravel_used\": modeoftravel_filter if modeoftravel_filter else \"\",\n",
    "            # Pipe-joined for spreadsheet safety (mirrors Step 1)\n",
    "            \"ahead_zones\": \"|\".join(ahead),\n",
    "            \"behind_zones\": \"|\".join(behind),\n",
    "            \"non_trad_ahead_mean\": mean_ahead,\n",
    "            \"non_trad_behind_mean\": mean_behind,\n",
    "            \"non_trad_aadt\": overall,\n",
    "            \"stl_ahead_rows\": ahead_n,\n",
    "            \"stl_behind_rows\": behind_n,\n",
    "            \"missing_ahead_zones\": \"|\".join(miss_a) if miss_a else \"\",\n",
    "            \"missing_behind_zones\": \"|\".join(miss_b) if miss_b else \"\",\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows) if as_df else rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2bffe53-019a-424d-99bd-6880d47e4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will run the \"non_traditional_aadt_by_location\" function if  you have the raw nested structure:\n",
    "stl_df = non_traditional_aadt_by_location(\n",
    "    sr_99_d3_tc_aadt_locations,\n",
    "    df_stl,\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=\"All Vehicles - StL All Vehicles Volume\",  # or None\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\",\n",
    "    as_df=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1054eca1-bde9-4b77-881f-3faf0f052fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export step 2 to a CSV\n",
    "stl_df.to_csv(\"step_2_non_traditional_aadt_by_location.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11dbbc8-84a8-4a7c-bca5-5b89aee3259a",
   "metadata": {},
   "source": [
    "### Step 3, Build the per-location comparison DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec234022-6cff-4b8f-90c7-ed0f67a0bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------------------------------\n",
    "# # 3) Build the per-location comparison DataFrame\n",
    "# # ------------------------------------------------------\n",
    "\n",
    "def build_aadt_comparison_df(\n",
    "    aadt_locations,\n",
    "    df_tc,\n",
    "    df_stl,\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=None,\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a per-location comparison combining:\n",
    "      - Traditional (Traffic Census) AADT\n",
    "      - Non-traditional (StreetLight) AADT\n",
    "      - TCE (%) = 100 * (non_trad_aadt - traditional_aadt) / traditional_aadt\n",
    "\n",
    "    Returns a pandas DataFrame (one row per location).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Build the two sides using your updated functions\n",
    "    trad_df = traditional_aadt_by_location(\n",
    "        aadt_locations=aadt_locations,\n",
    "        df_tc=df_tc,\n",
    "        as_df=True\n",
    "    )\n",
    "\n",
    "    nt_df = non_traditional_aadt_by_location(\n",
    "        aadt_locations=aadt_locations,\n",
    "        df_stl=df_stl,\n",
    "        daytype_filter=daytype_filter,\n",
    "        daypart_filter=daypart_filter,\n",
    "        modeoftravel_filter=modeoftravel_filter,\n",
    "        zonename_col=zonename_col,\n",
    "        stl_volume_col=stl_volume_col,\n",
    "        as_df=True\n",
    "    )\n",
    "\n",
    "    # 2) Merge on 'location'\n",
    "    merged = pd.merge(\n",
    "        trad_df,\n",
    "        nt_df,\n",
    "        how=\"inner\",\n",
    "        on=\"location\",\n",
    "        suffixes=(\"_trad\", \"_nt\")\n",
    "    )\n",
    "\n",
    "    # 3) Compute TCE (%), guarding against zero / NaN\n",
    "    def _tce(row):\n",
    "        t = row.get(\"traditional_aadt\")\n",
    "        n = row.get(\"non_trad_aadt\")\n",
    "        if pd.notna(t) and t != 0 and pd.notna(n):\n",
    "            return 100.0 * (n - t) / t\n",
    "        return np.nan\n",
    "\n",
    "    merged[\"tce_percent\"] = merged.apply(_tce, axis=1)\n",
    "\n",
    "    # 4) Stable, readable column order (only keep those that exist)\n",
    "    preferred_cols = [\n",
    "        \"location\",\n",
    "        # IDs & zones (pipe-joined for spreadsheet safety)\n",
    "        \"objectids\", \"n_objectids\", \"n_found_in_tc\", \"missing_objectids\",\n",
    "        \"ahead_zones\", \"behind_zones\",\n",
    "        # AADT metrics\n",
    "        \"traditional_ahead_mean\", \"traditional_behind_mean\", \"traditional_aadt\",\n",
    "        \"non_trad_ahead_mean\", \"non_trad_behind_mean\", \"non_trad_aadt\",\n",
    "        \"tce_percent\",\n",
    "        # Filters / metadata\n",
    "        \"daytype\",            # from Step 1\n",
    "        \"daytype_expected\",   # from Step 2 (original location metadata)\n",
    "        \"daytype_used\", \"daypart_used\", \"modeoftravel_used\",\n",
    "        # Debug / row counts\n",
    "        \"stl_ahead_rows\", \"stl_behind_rows\",\n",
    "        \"missing_ahead_zones\", \"missing_behind_zones\",\n",
    "    ]\n",
    "    cols = [c for c in preferred_cols if c in merged.columns]\n",
    "    merged = merged[cols].copy()\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f183dac9-4407-491a-8e88-76339267e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1) Build the combined comparison DataFrame\n",
    "cmp_df = build_aadt_comparison_df(\n",
    "    aadt_locations=sr_99_d3_tc_aadt_locations,  # your dict/list structure\n",
    "    df_tc=df_tc,                                 # Traffic Census dataframe\n",
    "    df_stl=df_stl,                               # StreetLight dataframe\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=None,                    # e.g., \"0: All Modes\" if you need it\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab6d0c76-b4a0-41a8-8ecd-ae3b2a05fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2) Quick peek\n",
    "#cmp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b5789ef-bea8-4369-9f79-b8ff23aef61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3) (Optional) sort by absolute TCE to see big deltas first\n",
    "cmp_df = cmp_df.sort_values(\"tce_percent\", key=lambda s: s.abs(), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfd90d29-5647-4cf8-ac6a-911e4cea503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4) Export to CSV \n",
    "cmp_df.to_csv(\"step_3_comparison_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaade07-ead0-4958-ac76-cf4a98169217",
   "metadata": {},
   "source": [
    "## Step 4 Confidence Interval over TCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c453dbb8-0d46-4847-b296-00aee2b97787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------------------------------\n",
    "# # 4) Confidence interval over TCE\n",
    "# # ------------------------------------------------------\n",
    "\n",
    "def tce_confidence_interval(detail_df, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Compute summary stats over `detail_df[\"tce_percent\"]`.\n",
    "    Returns: (mean_tce, ci_lo, ci_hi, tcrit, t_stat)\n",
    "    \"\"\"\n",
    "    # Clean and extract\n",
    "    tces = pd.to_numeric(detail_df[\"tce_percent\"], errors=\"coerce\") \\\n",
    "             .replace([np.inf, -np.inf], np.nan) \\\n",
    "             .dropna().values\n",
    "    n = len(tces)\n",
    "    if n == 0:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    mean_tce = float(np.mean(tces))\n",
    "    if n > 1:\n",
    "        std_tce = float(np.std(tces, ddof=1))\n",
    "        se = std_tce / np.sqrt(n)\n",
    "        if se > 0:\n",
    "            dof = n - 1\n",
    "            tcrit = float(stats.t.ppf((1 + confidence) / 2, dof))\n",
    "            ci_lo = mean_tce - tcrit * se\n",
    "            ci_hi = mean_tce + tcrit * se\n",
    "            t_stat = mean_tce / se\n",
    "        else:\n",
    "            tcrit = ci_lo = ci_hi = t_stat = None\n",
    "    else:\n",
    "        std_tce = 0.0\n",
    "        se = 0.0\n",
    "        tcrit = ci_lo = ci_hi = t_stat = None\n",
    "\n",
    "    return mean_tce, ci_lo, ci_hi, tcrit, t_stat\n",
    "\n",
    "def tce_confidence_interval_df(detail_df, confidence=0.95) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Same as tce_confidence_interval, but returns a one-row DataFrame with\n",
    "    extra fields useful for reporting/export.\n",
    "    \"\"\"\n",
    "    tces = pd.to_numeric(detail_df[\"tce_percent\"], errors=\"coerce\") \\\n",
    "             .replace([np.inf, -np.inf], np.nan) \\\n",
    "             .dropna()\n",
    "    n = int(tces.shape[0])\n",
    "    if n == 0:\n",
    "        return pd.DataFrame([{\n",
    "            \"confidence\": confidence,\n",
    "            \"n\": 0,\n",
    "            \"dof\": None,\n",
    "            \"mean_tce\": None,\n",
    "            \"std_tce\": None,\n",
    "            \"se\": None,\n",
    "            \"t_critical\": None,\n",
    "            \"margin_of_error\": None,\n",
    "            \"ci_lower\": None,\n",
    "            \"ci_upper\": None,\n",
    "            \"t_statistic\": None,\n",
    "            \"p_value_two_sided\": None\n",
    "        }])\n",
    "\n",
    "    mean_tce = float(tces.mean())\n",
    "    if n > 1:\n",
    "        std_tce = float(tces.std(ddof=1))\n",
    "        se = std_tce / np.sqrt(n)\n",
    "        dof = n - 1\n",
    "        if se > 0:\n",
    "            tcrit = float(stats.t.ppf((1 + confidence) / 2, dof))\n",
    "            moe = tcrit * se\n",
    "            ci_lo = mean_tce - moe\n",
    "            ci_hi = mean_tce + moe\n",
    "            t_stat = mean_tce / se\n",
    "            p_val = float(2 * (1 - stats.t.cdf(abs(t_stat), dof)))\n",
    "        else:\n",
    "            tcrit = moe = ci_lo = ci_hi = t_stat = p_val = None\n",
    "    else:\n",
    "        std_tce = 0.0\n",
    "        se = 0.0\n",
    "        dof = None\n",
    "        tcrit = moe = ci_lo = ci_hi = t_stat = p_val = None\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        \"confidence\": confidence,\n",
    "        \"n\": n,\n",
    "        \"dof\": dof,\n",
    "        \"mean_tce\": mean_tce,\n",
    "        \"std_tce\": std_tce if n > 1 else None,\n",
    "        \"se\": se if n > 1 else None,\n",
    "        \"t_critical\": tcrit,\n",
    "        \"margin_of_error\": moe,\n",
    "        \"ci_lower\": ci_lo,\n",
    "        \"ci_upper\": ci_hi,\n",
    "        \"t_statistic\": t_stat,\n",
    "        \"p_value_two_sided\": p_val\n",
    "    }])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "241190c1-880c-4346-89d8-16d9f93e2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1) Build your comparison DataFrame first (from prior step)\n",
    "cmp_df = build_aadt_comparison_df(\n",
    "    aadt_locations=sr_99_d3_tc_aadt_locations,\n",
    "    df_tc=df_tc,\n",
    "    df_stl=df_stl,\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=None,\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44b695fb-0e44-4645-ae8f-6fcb17175f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2) Get the CI summary as a DataFrame\n",
    "tce_summary_df = tce_confidence_interval_df(cmp_df, confidence=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d1a1f62-dec4-4836-8a54-6b25852def26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   confidence   n  dof   mean_tce    std_tce        se  t_critical  \\\n",
      "0        0.95  49   48 -35.124376  27.905644  3.986521    2.010635   \n",
      "\n",
      "   margin_of_error   ci_lower   ci_upper  t_statistic  p_value_two_sided  \n",
      "0         8.015437 -43.139813 -27.108939    -8.810785       1.348721e-11  \n"
     ]
    }
   ],
   "source": [
    "# 4.3) Quick peek\n",
    "print(tce_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28aa3c47-86df-4bd4-9cbc-5799b8b3f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4) Export to CSV \n",
    "cmp_df.to_csv(\"step_4_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b665779a-9ff5-4a08-8a83-788ab9b90caa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean TCE: -35.12437585771558\n",
      "95% Confidence Interval: (-43.13981265550753, -27.10893905992363)\n",
      "t-test statistic: -8.810785066136184\n",
      "t-critical: 2.0106347546964454\n"
     ]
    }
   ],
   "source": [
    "mean_tce, ci_lower, ci_upper, t_critical, t_statistic = tce_confidence_interval(\n",
    "    cmp_df, confidence=0.95\n",
    ")\n",
    "\n",
    "print(\"Mean TCE:\", mean_tce)\n",
    "print(\"95% Confidence Interval:\", (ci_lower, ci_upper))\n",
    "print(\"t-test statistic:\", t_statistic)\n",
    "print(\"t-critical:\", t_critical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7a389-e31e-43a2-9cf4-ccb7e7eec747",
   "metadata": {},
   "source": [
    "### Mean TCE: -35.12\n",
    "Traffic Census Error (TCE)\n",
    "* Across SR-99 in District 3, StreetLight AADT underestimates Traffic Census AADT by about 35% on average. This appears systematic rather than random, so we’ll (1) fix any mapping/coverage gaps, (2) stratify and weight the results, and (3) apply a corridor-specific calibration before using StreetLight AADT operationally.\n",
    "\n",
    "### 95% Confidence Interval (-43.14%, -27.11%)\n",
    "* On SR-99 in District 3, StreetLight AADT underestimates Traffic Census by ~35% on average (95% CI: –43% to –27%), a statistically and operationally significant gap that warrants calibration before use.\n",
    "\n",
    "### T-Test Statistic (-8.81)\n",
    "* The corridor-average underestimation is highly significant (t = –8.81, p ≪ 0.001): StreetLight AADT is materially lower than Traffic Census on SR-99 (D3), warranting data QA and corridor-specific calibration.\n",
    "\n",
    "### t-critical (2.01)\n",
    "* At 95% confidence, the decision threshold is t-critical = 2.01; our observed t = −8.81 easily exceeds this in magnitude, confirming a statistically significant corridor-level underestimation of StreetLight AADT relative to Traffic Census on SR-99 (D3).\n",
    "\n",
    "### Summary\n",
    "* SR-99 (D3): StreetLight AADT is ~35% lower than Traffic Census on average (95% CI –43% to –27%, t = –8.81), indicating a statistically significant corridor-level underestimation that requires QA and calibration before operational use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71be92d-e371-47d8-9cf8-5c3da056db6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ee5909-ec3c-4c6b-ab29-e93293f9f224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
