{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826e200c-086a-4cd6-962b-0a3d41ee4e11",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AADT Confidence Interval - State Route (SR) 99, District 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f1a47-13bc-413e-bf96-bc0f40580be1",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## FHWA Links\n",
    "* Guidelines for Obtaining AADT Estimates from Non-Traditional Sources:\n",
    "    * https://www.fhwa.dot.gov/policyinformation/travel_monitoring/pubs/aadtnt/Guidelines_for_AADT_Estimates_Final.pdf\n",
    "\n",
    "---\n",
    "  \n",
    "## AADT Analysis Locations\n",
    "* 10 locations were used in the analysis\n",
    "* Locations were determined based on the location on installed & recording Traffic Operations cameras\n",
    "    * for additional information contact Zhenyu Zhu with Traffic Operations\n",
    "\n",
    "## Traffic Census Data\n",
    "* https://dot.ca.gov/programs/traffic-operations/census/traffic-volumes\n",
    "* Back AADT, Peak Month, and Peak Hour usually represents traffic South or West of the count location.  \n",
    "* Ahead AADT, Peak Month, and Peak Hour usually represents traffic North or East of the count location. Listing of routes with their designated  \n",
    "\n",
    "* Because the Back & Ahead counts are included at each location in the Traffic Census Data, (e.g., \"IRWINDALE, ARROW HIGHWAY\") only one [OBJECTID*] per location was pulled; for this analysis the North Bound Nodes were used for the analysis. \n",
    "    * for more information see the diagram: https://traffic.onramp.dot.ca.gov/downloads/traffic/files/performance/census/Back_and_Ahead_Leg_Traffic_Count_Diagram.pdf\n",
    "\n",
    "## StreetLight Analysis Data\n",
    "* Analysis Type == Network Performance\n",
    "* Segment Metrics\n",
    "* 2022 was used to match currently available Traffic Census Data (as of 8/27/2025)\n",
    "* pulled a variety of Day Types, but plan to just look at \"\"\"All Day Types\"\"\"\n",
    "* pulled a variety of Day Parts, but plan to just look at \"\"\"All Day Parts\"\"\"\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e4b06-1d4d-40f3-b913-f3664001d3bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## How this notebook estimates StreetLight vs. Traffic Census differences\n",
    "\n",
    "**What we’re trying to answer:**  \n",
    "Across selected corridor locations, is the Non-Traditional AADT generally higher or lower than Traffic Census (aka Traditional) AADT, and by how much? We also show how certain we are about that average difference.\n",
    "\n",
    "---\n",
    "\n",
    "### The data we use\n",
    "- **Traffic Census (TC):** The official counts by location (`objectid`) with two directions: *ahead* and *back*.\n",
    "- **StreetLight (STL):** Volume by road segment (“**zonename**”) with tags like **daytype** (e.g., All Days) and **daypart** (e.g., All Day).\n",
    "- **Location mapping:** For each TC location, a list of the STL zosenames that represent the *ahead* side and the *behind* side of that location.\n",
    "\n",
    "---\n",
    "\n",
    "### How we build one number per location (AADT)\n",
    "1) **Pick the TC value** that matches the counter’s direction:  \n",
    "   - Even `objectid` → use the TC *back* value  \n",
    "   - Odd `objectid` → use the TC *ahead* value  \n",
    "   *(This mirrors the direction convention previously reviewed.)*\n",
    "\n",
    "2) **Filter StreetLight to the same conditions** you care about (usually **All Days** and **All Day**).\n",
    "\n",
    "3) **For each STL zonename**, take the average volume within that filter.  \n",
    "   *(This gives one “typical” value per segment under the chosen daytype/daypart.)*\n",
    "\n",
    "4) **Sum the STL segments for this location**:  \n",
    "   - Add up the “ahead” segments.  \n",
    "   - Add up the “behind” segments.  \n",
    "   - Then add those two sides together.  \n",
    "   *(Result = StreetLight AADT for that location.)*\n",
    "\n",
    "Now each location has:\n",
    "- **TC AADT** (the benchmark)  \n",
    "- **STL AADT** (the estimate from StreetLight)\n",
    "\n",
    "---\n",
    "\n",
    "### Turn those into apples-to-apples differences (TCE, in %)\n",
    "For every location with both numbers:\n",
    "- **Traffic Count Error (TCE)** = the percent difference between STL and TC.  \n",
    "  - Negative TCE → STL is lower than TC.  \n",
    "  - Positive TCE → STL is higher than TC.\n",
    "\n",
    "We collect one TCE value per location.\n",
    "\n",
    "---\n",
    "\n",
    "### Summarize and add a confidence band (CI)\n",
    "- **Average TCE**: the typical over/under across all locations.  \n",
    "- **95% Confidence Interval**: a “margin of error” around that average, based on how much the location-level TCEs vary and how many locations you have.  \n",
    "  - If the interval **crosses 0%**, the average difference isn’t statistically clear (could be slightly above or below zero).  \n",
    "  - If the interval is **entirely below 0%**, STL tends to be lower than TC.  \n",
    "  - If it’s **entirely above 0%**, STL tends to be higher.\n",
    "\n",
    "We also show a **t-statistic** and **p-value** for the “is the average difference basically zero?” question; lower p-values mean a clearer difference.\n",
    "\n",
    "---\n",
    "\n",
    "### What to look for\n",
    "- **The average TCE** (direction and size).  \n",
    "- **Whether the 95% CI includes 0%.**  \n",
    "- **Any locations with missing segments or mismatched data** (these are flagged so you can QA them)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e192bde-dff4-46cf-b2cd-da8347440ec5",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "171fa2b0-9b02-4947-a4d1-dd18ef86de42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6bce3a-11a1-4f2a-a7d9-eb7904946817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pull in the coordinates from the utils docs\n",
    "#from osow_frp_o_d_utils_v3 import origin_intersections, destination_intersections\n",
    "import shs_ct_tc_locations_utils as tc_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b47dc-41a0-41ed-9d63-8fe73c9c0549",
   "metadata": {},
   "source": [
    "### Identify the corridor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362e5fc0-0edf-460d-bc30-82e2647c7be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the corridor to be analyzed\n",
    "CORRIDOR_VAR_NAME = \"sr_99_d3_tc_aadt_locations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6c6c3b7-87f4-4c6d-b468-50ae993dd498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Resolve the object from the module by name\n",
    "try:\n",
    "    aadt_locations = getattr(tc_locs, CORRIDOR_VAR_NAME)\n",
    "except AttributeError:\n",
    "    raise KeyError(\n",
    "        f\"'{CORRIDOR_VAR_NAME}' not found in shs_ct_tc_locations_utils. \"\n",
    "        \"Double-check the variable name.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0a612-d979-43d3-87f3-e8d69df16d42",
   "metadata": {},
   "source": [
    "### Identify the Google Cloud Storage path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce4826a9-4c1c-46e4-95c5-20c820351b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the GCS path to the data\n",
    "gcs_path = \"gs://calitp-analytics-data/data-analyses/big_data/compare_traffic_counts/0_2022/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628fa29-7fdc-46d2-bc5a-84c271f150d9",
   "metadata": {},
   "source": [
    "## Step 0, Pull in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c01657-5051-44da-b79f-594359c06427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will pull in the data and clean the column headers in a way that will make them easier to work with\n",
    "def getdata_and_cleanheaders(path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Clean column headers: remove spaces, convert to lowercase, and strip trailing asterisks\n",
    "    cleaned_columns = []\n",
    "    for column in df.columns:\n",
    "        cleaned_column = column.replace(\" \", \"\").lower().rstrip(\"*\")\n",
    "        cleaned_columns.append(cleaned_column)\n",
    "\n",
    "    df.columns = cleaned_columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "686ea444-0d68-415c-beda-7ecdfe887062",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_request non-retriable exception: ('Error code invalid_grant: Refresh token has expired', '{\"error\":\"invalid_grant\",\"error_description\":\"Refresh token has expired\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/gcsfs/retry.py\", line 135, in retry_request\n",
      "    return await func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/gcsfs/core.py\", line 467, in _request\n",
      "    headers=self._get_headers(headers),\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/gcsfs/core.py\", line 444, in _get_headers\n",
      "    self.credentials.apply(out)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/gcsfs/credentials.py\", line 223, in apply\n",
      "    self.maybe_refresh()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/gcsfs/credentials.py\", line 211, in maybe_refresh\n",
      "    self.credentials.refresh(req)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/google/auth/external_account_authorized_user.py\", line 281, in refresh\n",
      "    response_data = self._make_sts_request(request)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/google/auth/external_account_authorized_user.py\", line 292, in _make_sts_request\n",
      "    return self._sts_client.refresh_token(request, self._refresh_token)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/google/oauth2/sts.py\", line 172, in refresh_token\n",
      "    return self._make_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/google/oauth2/sts.py\", line 88, in _make_request\n",
      "    utils.handle_error_response(response_body)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/google/oauth2/utils.py\", line 168, in handle_error_response\n",
      "    raise exceptions.OAuthError(error_details, response_body)\n",
      "google.auth.exceptions.OAuthError: ('Error code invalid_grant: Refresh token has expired', '{\"error\":\"invalid_grant\",\"error_description\":\"Refresh token has expired\"}')\n"
     ]
    },
    {
     "ename": "OAuthError",
     "evalue": "('Error code invalid_grant: Refresh token has expired', '{\"error\":\"invalid_grant\",\"error_description\":\"Refresh token has expired\"}')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOAuthError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pull in the data & create dataframes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_tc \u001b[38;5;241m=\u001b[39m \u001b[43mgetdata_and_cleanheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgcs_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mcaltrans_traffic_census_2022.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Traffic Census\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mgetdata_and_cleanheaders\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgetdata_and_cleanheaders\u001b[39m(path):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Read the CSV file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Clean column headers: remove spaces, convert to lowercase, and strip trailing asterisks\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     cleaned_columns \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:713\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    710\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    722\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:411\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    409\u001b[0m     file_obj \u001b[38;5;241m=\u001b[39m \u001b[43mfsspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsspec_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# GH 34626 Reads from Public Buckets without Credentials needs anon=True\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(err_types_to_retry_with_anon):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/core.py:147\u001b[0m, in \u001b[0;36mOpenFile.open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Materialise this as a real open file without context\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    The OpenFile object should be explicitly closed to avoid enclosed file\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    instances persisting. You must, therefore, keep a reference to the OpenFile\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    during the life of the file-like it generates.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__enter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/core.py:105\u001b[0m, in \u001b[0;36mOpenFile.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_magic(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/spec.py:1338\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1337\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1338\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:1678\u001b[0m, in \u001b[0;36mGCSFileSystem._open\u001b[0;34m(self, path, mode, block_size, cache_options, acl, consistency, metadata, autocommit, fixed_key_metadata, generation, **kwargs)\u001b[0m\n\u001b[1;32m   1676\u001b[0m     block_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_block_size\n\u001b[1;32m   1677\u001b[0m const \u001b[38;5;241m=\u001b[39m consistency \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsistency\n\u001b[0;32m-> 1678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGCSFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconsistency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m    \u001b[49m\u001b[43macl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_key_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_key_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:1850\u001b[0m, in \u001b[0;36mGCSFile.__init__\u001b[0;34m(self, gcsfs, path, mode, block_size, autocommit, cache_type, cache_options, acl, consistency, metadata, content_type, timeout, fixed_key_metadata, generation, kms_key_name, **kwargs)\u001b[0m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to open a bucket\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration \u001b[38;5;241m=\u001b[39m _coalesce_generation(generation, path_generation)\n\u001b[0;32m-> 1850\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgcsfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcsfs \u001b[38;5;241m=\u001b[39m gcsfs\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbucket \u001b[38;5;241m=\u001b[39m bucket\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/spec.py:1912\u001b[0m, in \u001b[0;36mAbstractBufferedFile.__init__\u001b[0;34m(self, fs, path, mode, block_size, autocommit, cache_type, cache_options, size, **kwargs)\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m   1911\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1912\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetails\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m caches[cache_type](\n\u001b[1;32m   1914\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_range, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcache_options\n\u001b[1;32m   1915\u001b[0m     )\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:1894\u001b[0m, in \u001b[0;36mGCSFile.details\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1891\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdetails\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1893\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1894\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/asyn.py:103\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     54\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     58\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:1032\u001b[0m, in \u001b[0;36mGCSFileSystem._info\u001b[0;34m(self, path, generation, **kwargs)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;66;03m# Check exact file path\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1032\u001b[0m     exact \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object(path)\n\u001b[1;32m   1033\u001b[0m     \u001b[38;5;66;03m# this condition finds a \"placeholder\" - still need to check if it's a directory\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_directory_marker(exact):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:563\u001b[0m, in \u001b[0;36mGCSFileSystem._get_object\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# Work around various permission settings. Prefer an object get (storage.objects.get), but\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# fall back to a bucket list + filter to object name (storage.objects.list).\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\n\u001b[1;32m    564\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/o/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, bucket, key, json_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, generation\u001b[38;5;241m=\u001b[39mgeneration\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForbidden\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:483\u001b[0m, in \u001b[0;36mGCSFileSystem._call\u001b[0;34m(self, method, path, json_out, info_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, path, \u001b[38;5;241m*\u001b[39margs, json_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, info_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    481\u001b[0m ):\n\u001b[1;32m    482\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 483\u001b[0m     status, headers, info, contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    484\u001b[0m         method, path, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    485\u001b[0m     )\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m json_out:\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(contents)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/decorator.py:224\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    223\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/retry.py:170\u001b[0m, in \u001b[0;36mretry_request\u001b[0;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    169\u001b[0m logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m non-retriable exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/retry.py:135\u001b[0m, in \u001b[0;36mretry_request\u001b[0;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mmin\u001b[39m(random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (retry \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m32\u001b[39m))\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    137\u001b[0m     HttpError,\n\u001b[1;32m    138\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     aiohttp\u001b[38;5;241m.\u001b[39mclient_exceptions\u001b[38;5;241m.\u001b[39mClientError,\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(e, HttpError)\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequester pays\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    147\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:467\u001b[0m, in \u001b[0;36mGCSFileSystem._request\u001b[0;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseek\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    461\u001b[0m     data\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    463\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    464\u001b[0m     url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_path(path, args),\n\u001b[1;32m    465\u001b[0m     params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(kwargs),\n\u001b[1;32m    466\u001b[0m     json\u001b[38;5;241m=\u001b[39mjson,\n\u001b[0;32m--> 467\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    468\u001b[0m     data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    469\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequests_timeout,\n\u001b[1;32m    470\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[1;32m    471\u001b[0m     status \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    472\u001b[0m     headers \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mheaders\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:444\u001b[0m, in \u001b[0;36mGCSFileSystem._get_headers\u001b[0;34m(self, headers)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m out:\n\u001b[1;32m    443\u001b[0m     out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython-gcsfs/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m version\n\u001b[0;32m--> 444\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/credentials.py:223\u001b[0m, in \u001b[0;36mGoogleCredentials.apply\u001b[0;34m(self, out)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, out):\n\u001b[1;32m    222\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Insert credential headers in-place to a dictionary\"\"\"\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_refresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials\u001b[38;5;241m.\u001b[39mapply(out)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/credentials.py:211\u001b[0m, in \u001b[0;36mGoogleCredentials.maybe_refresh\u001b[0;34m(self, refresh_buffer)\u001b[0m\n\u001b[1;32m    209\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGCS refresh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m gauth\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRefreshError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# Re-raise as HttpError with a 401 code and the expected message\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(\n\u001b[1;32m    215\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m401\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Credentials\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m    216\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/google/auth/external_account_authorized_user.py:281\u001b[0m, in \u001b[0;36mCredentials.refresh\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRefreshError(\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe credentials do not contain the necessary fields need to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefresh the access token. You must specify refresh_token, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_url, client_id, and client_secret.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     )\n\u001b[1;32m    280\u001b[0m now \u001b[38;5;241m=\u001b[39m _helpers\u001b[38;5;241m.\u001b[39mutcnow()\n\u001b[0;32m--> 281\u001b[0m response_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_sts_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;241m=\u001b[39m response_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccess_token\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m lifetime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtimedelta(seconds\u001b[38;5;241m=\u001b[39mresponse_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpires_in\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/google/auth/external_account_authorized_user.py:292\u001b[0m, in \u001b[0;36mCredentials._make_sts_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_sts_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sts_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_refresh_token\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/google/oauth2/sts.py:172\u001b[0m, in \u001b[0;36mClient.refresh_token\u001b[0;34m(self, request, refresh_token)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrefresh_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, request, refresh_token):\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Exchanges a refresh token for an access token based on the\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    RFC6749 spec.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m        subject_token (str): The OAuth 2.0 refresh token.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrant_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrefresh_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrefresh_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefresh_token\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/google/oauth2/sts.py:88\u001b[0m, in \u001b[0;36mClient._make_request\u001b[0;34m(self, request, headers, request_body)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# If non-200 response received, translate to OAuthError exception.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m http_client\u001b[38;5;241m.\u001b[39mOK:\n\u001b[0;32m---> 88\u001b[0m     \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_error_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m response_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response_body)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Return successful response.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/google/oauth2/utils.py:168\u001b[0m, in \u001b[0;36mhandle_error_response\u001b[0;34m(response_body)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    166\u001b[0m     error_details \u001b[38;5;241m=\u001b[39m response_body\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mOAuthError(error_details, response_body)\n",
      "\u001b[0;31mOAuthError\u001b[0m: ('Error code invalid_grant: Refresh token has expired', '{\"error\":\"invalid_grant\",\"error_description\":\"Refresh token has expired\"}')"
     ]
    }
   ],
   "source": [
    "# pull in the data & create dataframes\n",
    "df_tc = getdata_and_cleanheaders(f\"{gcs_path}caltrans_traffic_census_2022.csv\")  # Traffic Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a890154-e2c9-435a-b136-48d932f3359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the StreetLight Analysis to be used in the AADT comparison\n",
    "df_stl = getdata_and_cleanheaders(f\"{gcs_path}streetlight_605_d7_all_vehicles_np_2022.csv\")  # StreetLight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b9a53-0066-4643-b8b7-ec90ceec2f6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# comparing\n",
    "df_tc.to_csv(\"df_tc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59259345-ec3c-47e8-bd2d-cf5059babf9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# comparing\n",
    "df_stl.to_csv(\"df_stl.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa10f7c-2eb2-467c-8507-13362af70cc1",
   "metadata": {},
   "source": [
    "## Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ae164e-ae91-4f5d-9c9a-aac9d3cc730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_list(x):\n",
    "    if x is None: return []\n",
    "    if isinstance(x, (list, tuple, set)): return list(x)\n",
    "    return [x]\n",
    "\n",
    "def explode_locations_to_objectids(aadt_locs):\n",
    "    \"\"\"\n",
    "    Returns a list of dicts where each item is ONE objectid with:\n",
    "      name, daytype, objectids [list[str]], ahead_zones [list[str]], behind_zones [list[str]]\n",
    "    This shape is accepted by your existing traditional/non_traditional builders.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    # Case A: \"flat\" list like interstate_605_aadt_locations\n",
    "    if isinstance(aadt_locs, list) and aadt_locs and isinstance(aadt_locs[0], dict) and \"objectid\" in aadt_locs[0]:\n",
    "        for loc in aadt_locs:\n",
    "            oid = str(loc.get(\"objectid\"))\n",
    "            nm  = f\"{loc.get('location_description','UNKNOWN')} [{oid}]\"\n",
    "            day = loc.get(\"daytype\", \"0: All Days (M-Su)\")\n",
    "\n",
    "            ahead, behind = [], []\n",
    "            for k, v in loc.items():\n",
    "                if not k.startswith(\"zonename_\"):\n",
    "                    continue\n",
    "                idx = int(k.split(\"_\")[1])\n",
    "                # assume even indexes (0,2) are \"ahead\"/NB and odd (1,3) are \"behind\"/SB (matches your list)\n",
    "                if idx % 2 == 0: ahead.append(v)\n",
    "                else:            behind.append(v)\n",
    "\n",
    "            rows.append({\n",
    "                \"name\": nm,\n",
    "                \"daytype\": day,\n",
    "                \"objectids\": [oid],\n",
    "                \"ahead_zones\": [z for z in ahead if z],\n",
    "                \"behind_zones\": [z for z in behind if z],\n",
    "            })\n",
    "        return rows\n",
    "\n",
    "    # Case B: nested dict(s) like sr_605_d7_tc_aadt_locations\n",
    "    def _gather_objectids(node):\n",
    "        ids = []\n",
    "        if \"objectid\"  in node: ids.extend(_ensure_list(node[\"objectid\"]))\n",
    "        if \"objectids\" in node: ids.extend(_ensure_list(node[\"objectids\"]))\n",
    "        return [str(i) for i in ids if i is not None and str(i).strip() != \"\"]\n",
    "\n",
    "    if isinstance(aadt_locs, list):\n",
    "        iterable = []\n",
    "        for item in aadt_locs:\n",
    "            if isinstance(item, dict):\n",
    "                iterable.append(item)\n",
    "    elif isinstance(aadt_locs, dict):\n",
    "        iterable = [aadt_locs]\n",
    "    else:\n",
    "        iterable = []\n",
    "\n",
    "    for block in iterable:\n",
    "        for base_name, loc in block.items():\n",
    "            day = loc.get(\"daytype\", \"0: All Days (M-Su)\")\n",
    "            nodes = loc.get(\"nodes\", {}) or {}\n",
    "            for node_name, node in nodes.items():\n",
    "                oids = _gather_objectids(node)\n",
    "                if not oids: continue\n",
    "                nm = f\"{base_name} [{','.join(oids)}]\"\n",
    "\n",
    "                ahead = _ensure_list(node.get(\"zonename_ahead\", []))\n",
    "                behind = _ensure_list(node.get(\"zonename_behind\", []))\n",
    "\n",
    "                rows.append({\n",
    "                    \"name\": nm,\n",
    "                    \"daytype\": day,\n",
    "                    \"objectids\": oids,\n",
    "                    \"ahead_zones\": [z for z in ahead if z],\n",
    "                    \"behind_zones\": [z for z in behind if z],\n",
    "                })\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcdb53c-d668-4ff3-983b-90ac077bfa36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7480907-d484-4e8f-8e53-eeb4517487ce",
   "metadata": {},
   "source": [
    "## Step 1, Build a per-location summary of Traffic Census locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ff939-7384-4f6d-86bc-109a2a6a54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_aadt_by_location(aadt_locations, df_tc, as_df=True, use_parity=True):\n",
    "    \"\"\"\n",
    "    Build a per-location summary of *traditional* (Traffic Census) AADT.\n",
    "\n",
    "    Output columns:\n",
    "      location, daytype, objectids, n_objectids, n_found_in_tc, missing_objectids,\n",
    "      traditional_ahead_mean, traditional_behind_mean, traditional_aadt\n",
    "    \"\"\"\n",
    "    # Requires: import pandas as pd; import numpy as np\n",
    "\n",
    "    def _ensure_list(x):\n",
    "        if x is None: return []\n",
    "        if isinstance(x, (list, tuple, set)): return list(x)\n",
    "        return [x]\n",
    "\n",
    "    def _gather_objectids(node_dict):\n",
    "        ids = []\n",
    "        if not isinstance(node_dict, dict): return ids\n",
    "        if \"objectid\"  in node_dict: ids.extend(_ensure_list(node_dict[\"objectid\"]))\n",
    "        if \"objectids\" in node_dict: ids.extend(_ensure_list(node_dict[\"objectids\"]))\n",
    "        return [str(i) for i in ids if i is not None and str(i).strip() != \"\"]\n",
    "\n",
    "    def _dedup(seq):\n",
    "        seen=set(); out=[]\n",
    "        for x in seq:\n",
    "            if x not in seen:\n",
    "                out.append(x); seen.add(x)\n",
    "        return out\n",
    "\n",
    "    def _normalize_one_location(name, loc, include_oid_in_name=True):\n",
    "        # Handle nested \"nodes\" dict (sr_605_d7_tc_aadt_locations-style)\n",
    "        nodes = (loc.get(\"nodes\") if isinstance(loc, dict) else None) or {}\n",
    "        all_ids=[]\n",
    "        for _, node in nodes.items():\n",
    "            all_ids.extend(_gather_objectids(node))\n",
    "        if not all_ids and isinstance(loc, dict) and \"objectid\" in loc:\n",
    "            all_ids = [str(loc[\"objectid\"])]\n",
    "\n",
    "        name_out = name\n",
    "        if include_oid_in_name and all_ids:\n",
    "            name_out = f\"{name} [{','.join(all_ids)}]\"\n",
    "\n",
    "        return {\n",
    "            \"name\": name_out,\n",
    "            \"daytype\": (loc.get(\"daytype\") if isinstance(loc, dict) else None) or \"0: All Days (M-Su)\",\n",
    "            \"objectids\": _dedup(all_ids),\n",
    "        }\n",
    "\n",
    "    def _normalize_input(aadt_locs):\n",
    "        # Already normalized DataFrame?\n",
    "        if isinstance(aadt_locs, pd.DataFrame) and {\"name\",\"daytype\",\"objectids\"}.issubset(aadt_locs.columns):\n",
    "            return aadt_locs.to_dict(orient=\"records\")\n",
    "        # Already normalized list[dict]?\n",
    "        if isinstance(aadt_locs, list) and aadt_locs and isinstance(aadt_locs[0], dict) and \\\n",
    "           {\"name\",\"daytype\",\"objectids\"}.issubset(aadt_locs[0].keys()):\n",
    "            return aadt_locs\n",
    "\n",
    "        recs = []\n",
    "        # Case 1: dict keyed by location names\n",
    "        if isinstance(aadt_locs, dict):\n",
    "            for nm, loc in aadt_locs.items():\n",
    "                recs.append(_normalize_one_location(nm, loc))\n",
    "            return recs\n",
    "\n",
    "        # Case 2: list of items\n",
    "        if isinstance(aadt_locs, list):\n",
    "            for item in aadt_locs:\n",
    "                if not isinstance(item, dict):\n",
    "                    continue\n",
    "                if \"nodes\" in item:\n",
    "                    nm = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "                    recs.append(_normalize_one_location(nm, item))\n",
    "                elif \"objectid\" in item:\n",
    "                    # Flat interstate_605_aadt_locations-style row\n",
    "                    oid = str(item.get(\"objectid\"))\n",
    "                    nm  = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "                    recs.append({\n",
    "                        \"name\": f\"{nm} [{oid}]\",\n",
    "                        \"daytype\": item.get(\"daytype\", \"0: All Days (M-Su)\"),\n",
    "                        \"objectids\": [oid],\n",
    "                    })\n",
    "                else:\n",
    "                    # Fallback: assume dict keyed by name\n",
    "                    for nm, loc in item.items():\n",
    "                        recs.append(_normalize_one_location(nm, loc))\n",
    "        return recs\n",
    "\n",
    "    def _traditional_aadt_for_ids(df_tc_in, obj_ids):\n",
    "        \"\"\"\n",
    "        If use_parity=True: even OID -> back_aadt, odd OID -> ahead_aadt (matches your reviewed analysis).\n",
    "        Else: average ahead/back per objectid (original behavior).\n",
    "        \"\"\"\n",
    "        obj_ids = [str(x) for x in (obj_ids or []) if str(x).strip()]\n",
    "        if not obj_ids:\n",
    "            return np.nan, np.nan, np.nan, 0\n",
    "\n",
    "        sub = df_tc_in[df_tc_in[\"objectid\"].astype(str).isin(obj_ids)].copy()\n",
    "        if sub.empty:\n",
    "            return np.nan, np.nan, np.nan, 0\n",
    "\n",
    "        if use_parity:\n",
    "            vals = []\n",
    "            for oid in obj_ids:\n",
    "                row = sub[sub[\"objectid\"].astype(str) == oid]\n",
    "                if row.empty:\n",
    "                    continue\n",
    "                val = row.iloc[0][\"back_aadt\"] if int(oid) % 2 == 0 else row.iloc[0][\"ahead_aadt\"]\n",
    "                vals.append(pd.to_numeric(val, errors=\"coerce\"))\n",
    "            vals = pd.Series(vals, dtype=\"float64\").dropna()\n",
    "            if vals.empty:\n",
    "                return np.nan, np.nan, np.nan, 0\n",
    "            overall = float(vals.mean())\n",
    "            return overall, np.nan, np.nan, int(vals.shape[0])\n",
    "\n",
    "        # Fallback: average ahead/back per objectid group\n",
    "        ahead_vals = pd.to_numeric(sub.get(\"ahead_aadt\"), errors=\"coerce\").dropna()\n",
    "        back_vals  = pd.to_numeric(sub.get(\"back_aadt\"),  errors=\"coerce\").dropna()\n",
    "        mean_ahead = float(ahead_vals.mean()) if not ahead_vals.empty else np.nan\n",
    "        mean_back  = float(back_vals.mean())  if not back_vals.empty  else np.nan\n",
    "        overall    = np.nanmean([mean_ahead, mean_back])\n",
    "        count_used = int(sub.shape[0])\n",
    "        return overall, mean_ahead, mean_back, count_used\n",
    "\n",
    "    # ---- main ----\n",
    "    norm = _normalize_input(aadt_locations)\n",
    "    tc_ids_all = set(df_tc[\"objectid\"].astype(str).unique())\n",
    "\n",
    "    rows = []\n",
    "    for loc in norm:\n",
    "        obj_ids = [str(x) for x in (loc.get(\"objectids\") or [])]\n",
    "        overall, mean_ahead, mean_back, n_found = _traditional_aadt_for_ids(df_tc, obj_ids)\n",
    "        missing = [x for x in obj_ids if x not in tc_ids_all]\n",
    "\n",
    "        rows.append({\n",
    "            \"location\": loc.get(\"name\"),\n",
    "            \"daytype\":  loc.get(\"daytype\"),\n",
    "            \"objectids\": \"|\".join(obj_ids),   # pipe-separated string\n",
    "            \"n_objectids\": len(obj_ids),\n",
    "            \"n_found_in_tc\": int(n_found),\n",
    "            \"missing_objectids\": \"|\".join(missing) if missing else \"\",\n",
    "            \"traditional_ahead_mean\": mean_ahead,\n",
    "            \"traditional_behind_mean\": mean_back,\n",
    "            \"traditional_aadt\": overall,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows) if as_df else rows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db115308-4010-4c5a-a51c-49db69dfaa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run step 1 - traditional aadt counts\n",
    "trad_df = traditional_aadt_by_location(aadt_locations, df_tc, as_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23b26a-69a2-4889-b04b-14cc250a504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trad_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea7086-b4d3-47eb-bf6e-bae9c86c506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Step 1 as a CSV to take a look\n",
    "trad_df.to_csv(\"step_1_traditional_aadt_by_location.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30ac04-cfd4-45bc-a85f-16437dbafdd2",
   "metadata": {},
   "source": [
    "## Step 2 Identify Traffic Census location names for the StreetLight segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e68861-59c8-4426-bf8c-16db1ea1ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_traditional_aadt_by_location(\n",
    "    aadt_locations,\n",
    "    df_stl,\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\",\n",
    "    as_df=True,\n",
    "    agg=\"sum\",                 # \"sum\" mirrors your pipeline; \"mean\" averages zones\n",
    "    key_mode=\"id\",             # \"id\" = use trailing numeric id after '/', else \"label\"\n",
    "    **kwargs                   # absorb unused args (e.g., modeoftravel_filter)\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal version that just computes numbers. No mutation of df_stl.\n",
    "    Handles aadt_locations as:\n",
    "      - dict: { \"LOC\": {daytype, nodes{...}}, ... }\n",
    "      - list[dict] where each dict is either a normalized record or a mapping of many LOCs.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    def _ensure_list(x):\n",
    "        if x is None: return []\n",
    "        if isinstance(x, (list, tuple, set)): return list(x)\n",
    "        return [x]\n",
    "\n",
    "    def _dedup(seq):\n",
    "        seen=set(); out=[]\n",
    "        for v in seq:\n",
    "            if v not in seen:\n",
    "                out.append(v); seen.add(v)\n",
    "        return out\n",
    "\n",
    "    # --- normalize inputs into list of dicts with ahead/behind arrays ---\n",
    "    def _normalize_one_location(name, loc, include_oid=True):\n",
    "        ahead, behind, oids = [], [], []\n",
    "        nodes = (loc.get(\"nodes\") or {}) if isinstance(loc, dict) else {}\n",
    "        for _, n in nodes.items():\n",
    "            ahead += [z for z in _ensure_list(n.get(\"zonename_ahead\")) if z]\n",
    "            behind += [z for z in _ensure_list(n.get(\"zonename_behind\")) if z]\n",
    "            if \"objectid\"  in n:  oids += _ensure_list(n[\"objectid\"])\n",
    "            if \"objectids\" in n:  oids += _ensure_list(n[\"objectids\"])\n",
    "        nm = name\n",
    "        if include_oid and oids:\n",
    "            nm = f\"{name} [{','.join(_dedup([str(x) for x in oids if str(x).strip()]))}]\"\n",
    "        return {\n",
    "            \"name\": nm,\n",
    "            \"daytype\": loc.get(\"daytype\", \"0: All Days (M-Su)\") if isinstance(loc, dict) else \"0: All Days (M-Su)\",\n",
    "            \"ahead_zones\": _dedup(ahead),\n",
    "            \"behind_zones\": _dedup(behind),\n",
    "        }\n",
    "\n",
    "    def _normalize_locations(aadt_locs):\n",
    "        import pandas as pd\n",
    "        # Already normalized dataframe?\n",
    "        if isinstance(aadt_locs, pd.DataFrame) and {\"name\",\"daytype\",\"ahead_zones\",\"behind_zones\"}.issubset(aadt_locs.columns):\n",
    "            return aadt_locs.to_dict(\"records\")\n",
    "\n",
    "        # Mapping dict? (your case if you pass sr_99_d3_tc_aadt_locations[0])\n",
    "        if isinstance(aadt_locs, dict):\n",
    "            return [_normalize_one_location(nm, loc) for nm, loc in aadt_locs.items()]\n",
    "\n",
    "        # List inputs\n",
    "        recs=[]\n",
    "        if isinstance(aadt_locs, list):\n",
    "            if not aadt_locs:\n",
    "                return recs\n",
    "            first = aadt_locs[0]\n",
    "\n",
    "            # Case 1: list of normalized dicts\n",
    "            if isinstance(first, dict) and {\"name\",\"daytype\",\"ahead_zones\",\"behind_zones\"}.issubset(first.keys()):\n",
    "                return aadt_locs\n",
    "\n",
    "            # Case 2: list where items are mappings of many locations (YOUR CASE)\n",
    "            # e.g., [ { \"LOC1\": {...}, \"LOC2\": {...} }, { \"LOC3\": {...} } ]\n",
    "            all_items_are_mappings = all(isinstance(item, dict) and not {\"name\",\"ahead_zones\",\"behind_zones\"}.issubset(item.keys()) for item in aadt_locs)\n",
    "            if all_items_are_mappings:\n",
    "                for mapping in aadt_locs:\n",
    "                    for nm, loc in mapping.items():\n",
    "                        recs.append(_normalize_one_location(nm, loc))\n",
    "                return recs\n",
    "\n",
    "            # Case 3: list where each item is a single location dict with \"nodes\"\n",
    "            for item in aadt_locs:\n",
    "                if not isinstance(item, dict):\n",
    "                    continue\n",
    "                if \"nodes\" in item:\n",
    "                    nm = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "                    recs.append(_normalize_one_location(nm, item))\n",
    "                elif \"objectid\" in item:\n",
    "                    # flat record variant\n",
    "                    oid = str(item.get(\"objectid\"))\n",
    "                    nm  = item.get(\"location_description\") or item.get(\"name\") or \"UNKNOWN\"\n",
    "                    day = item.get(\"daytype\", \"0: All Days (M-Su)\")\n",
    "                    ahead, behind = [], []\n",
    "                    for k, v in item.items():\n",
    "                        if isinstance(k, str) and k.startswith(\"zonename_\"):\n",
    "                            try: idx = int(k.split(\"_\")[1])\n",
    "                            except: idx = None\n",
    "                            (ahead if (idx is not None and idx % 2 == 0) else behind).append(v)\n",
    "                    recs.append({\n",
    "                        \"name\": f\"{nm} [{oid}]\",\n",
    "                        \"daytype\": day,\n",
    "                        \"ahead_zones\": _dedup([z for z in ahead if z]),\n",
    "                        \"behind_zones\": _dedup([z for z in behind if z]),\n",
    "                    })\n",
    "        return recs\n",
    "\n",
    "    # --- choose key function (no mutations) ---\n",
    "    if key_mode == \"id\":\n",
    "        def make_key_series(labels_series):\n",
    "            s = labels_series.astype(str).str.strip()\n",
    "            keys = s.str.extract(r'/\\s*([0-9]+)\\s*$')[0]\n",
    "            fallback = s.str.extract(r'([0-9]+)(?!.*[0-9])')[0]\n",
    "            keys = keys.fillna(fallback)\n",
    "            keys = keys.fillna(s.str.lower())\n",
    "            return keys\n",
    "        def key_from_label(label):\n",
    "            lab = str(label).strip()\n",
    "            m = re.search(r'/\\s*([0-9]+)\\s*$', lab) or re.search(r'([0-9]+)(?!.*[0-9])', lab)\n",
    "            return m.group(1) if m else lab.lower()\n",
    "    else:  # key_mode == \"label\"\n",
    "        def make_key_series(labels_series):\n",
    "            return labels_series.astype(str).str.strip()\n",
    "        def key_from_label(label):\n",
    "            return str(label).strip()\n",
    "\n",
    "    # --- filter df_stl (exact if provided; otherwise use all) ---\n",
    "    required = {zonename_col, stl_volume_col}\n",
    "    if not required.issubset(df_stl.columns) or \"daytype\" not in df_stl.columns or \"daypart\" not in df_stl.columns:\n",
    "        missing = required.union({\"daytype\",\"daypart\"}) - set(df_stl.columns)\n",
    "        raise KeyError(f\"df_stl missing required column(s): {sorted(missing)}\")\n",
    "\n",
    "    if daytype_filter is None or daypart_filter is None:\n",
    "        stl_slice = df_stl.loc[:, [zonename_col, stl_volume_col]].copy()\n",
    "    else:\n",
    "        mask = (df_stl[\"daytype\"] == daytype_filter) & (df_stl[\"daypart\"] == daypart_filter)\n",
    "        stl_slice = df_stl.loc[mask, [zonename_col, stl_volume_col]].copy()\n",
    "        if stl_slice.empty:\n",
    "            # fall back to all rows if the exact strings don't exist\n",
    "            stl_slice = df_stl.loc[:, [zonename_col, stl_volume_col]].copy()\n",
    "\n",
    "    # coerce numeric; key by label or id\n",
    "    import pandas as pd\n",
    "    vals = pd.to_numeric(stl_slice[stl_volume_col], errors=\"coerce\")\n",
    "    labels = stl_slice[zonename_col]\n",
    "    keys = make_key_series(labels)\n",
    "    good = vals.notna() & keys.notna()\n",
    "    if not good.any():\n",
    "        raise ValueError(\"No usable StreetLight rows after filtering/keying; check stl_volume_col, zonename_col, and key_mode.\")\n",
    "\n",
    "    tmp = pd.DataFrame({\"key\": keys[good].astype(str), \"val\": vals[good].astype(float)})\n",
    "    zone_mean = tmp.groupby(\"key\")[\"val\"].mean()\n",
    "    zone_rows = tmp.groupby(\"key\")[\"val\"].size()\n",
    "    present_keys = set(zone_mean.index)\n",
    "\n",
    "    def agg_for_labels(label_list):\n",
    "        labels = [str(z).strip() for z in _ensure_list(label_list) if str(z).strip()]\n",
    "        if not labels:\n",
    "            return np.nan, 0, []\n",
    "        ks = [key_from_label(z) for z in labels]\n",
    "        present = [labels[i] for i, k in enumerate(ks) if k in present_keys]\n",
    "        missing = [labels[i] for i, k in enumerate(ks) if k not in present_keys]\n",
    "        vals_here = zone_mean.reindex([key_from_label(z) for z in present]).dropna().to_numpy()\n",
    "        if agg == \"sum\":\n",
    "            val = float(np.sum(vals_here)) if vals_here.size else np.nan\n",
    "        else:\n",
    "            val = float(np.mean(vals_here)) if vals_here.size else np.nan\n",
    "        n_rows = int(zone_rows.reindex([key_from_label(z) for z in present]).fillna(0).sum())\n",
    "        return val, n_rows, missing\n",
    "\n",
    "    # --- build rows ---\n",
    "    norm = _normalize_locations(aadt_locations)\n",
    "    if not norm:\n",
    "        raise ValueError(\"aadt_locations normalized to 0 locations. If you pass a list containing a mapping, pass the mapping (e.g., sr_99_d3_tc_aadt_locations[0]) or keep this function's new normalization.\")\n",
    "\n",
    "    rows = []\n",
    "    for loc in norm:\n",
    "        ahead = _ensure_list(loc.get(\"ahead_zones\"))\n",
    "        behind = _ensure_list(loc.get(\"behind_zones\"))\n",
    "        val_a, n_a, miss_a = agg_for_labels(ahead)\n",
    "        val_b, n_b, miss_b = agg_for_labels(behind)\n",
    "        overall = float(np.nansum([v for v in (val_a, val_b) if v is not None]))\n",
    "        rows.append({\n",
    "            \"location\": loc.get(\"name\"),\n",
    "            \"daytype_expected\": loc.get(\"daytype\"),\n",
    "            \"daytype_used\": (daytype_filter if daytype_filter is not None else \"\"),\n",
    "            \"daypart_used\": (daypart_filter if daypart_filter is not None else \"\"),\n",
    "            \"ahead_zones\": \"|\".join(ahead),\n",
    "            \"behind_zones\": \"|\".join(behind),\n",
    "            \"non_trad_ahead_mean\": val_a,\n",
    "            \"non_trad_behind_mean\": val_b,\n",
    "            \"non_trad_aadt\": overall,\n",
    "            \"stl_ahead_rows\": n_a,\n",
    "            \"stl_behind_rows\": n_b,\n",
    "            \"missing_ahead_zones\": \"|\".join(miss_a) if miss_a else \"\",\n",
    "            \"missing_behind_zones\": \"|\".join(miss_b) if miss_b else \"\",\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows) if as_df else rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bffe53-019a-424d-99bd-6880d47e4c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will run the \"non_traditional_aadt_by_location\" function if  you have the raw nested structure:\n",
    "# stl_df = non_traditional_aadt_by_location(\n",
    "#     aadt_locations,\n",
    "#     df_stl,\n",
    "#     daytype_filter=\"0: All Days (M-Su)\",\n",
    "#     daypart_filter=\"0: All Day (12am-12am)\",\n",
    "#     zonename_col=\"zonename\",\n",
    "#     stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\",\n",
    "#     as_df=True\n",
    "# )\n",
    "\n",
    "# stl_df = non_traditional_aadt_by_location(\n",
    "#     aadt_locations=sr_99_d3_tc_aadt_locations[0],   # <-- note the [0]\n",
    "#     df_stl=df_stl,\n",
    "#     daytype_filter=\"0: All Days (M-Su)\",\n",
    "#     daypart_filter=\"0: All Day (12am-12am)\",\n",
    "#     zonename_col=\"zonename\",\n",
    "#     stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\",\n",
    "#     key_mode=\"id\",        # use 'label' if IDs aren't in zonename\n",
    "#     as_df=True\n",
    "# )\n",
    "\n",
    "\n",
    "def _pick_mapping(locs):\n",
    "    # your data is a list with one big dict; if it’s already a dict, just return it\n",
    "    return locs[0] if isinstance(locs, list) else locs\n",
    "\n",
    "stl_df = non_traditional_aadt_by_location(\n",
    "    aadt_locations=_pick_mapping(sr_99_d3_tc_aadt_locations),\n",
    "    df_stl=df_stl,\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\",\n",
    "    key_mode=\"id\",\n",
    "    as_df=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1054eca1-bde9-4b77-881f-3faf0f052fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export step 2 to a CSV\n",
    "stl_df.to_csv(\"step_2_non_traditional_aadt_by_location.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d008e12-742e-43da-891a-68f12225ad31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccca634-da33-4b47-959d-3412176b3255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273fa667-10ab-4d5e-a05f-f5daeac69244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a11dbbc8-84a8-4a7c-bca5-5b89aee3259a",
   "metadata": {},
   "source": [
    "### Step 3, Build the per-location comparison DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec234022-6cff-4b8f-90c7-ed0f67a0bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------------------------------\n",
    "# # 3) Build the per-location comparison DataFrame\n",
    "# # ------------------------------------------------------\n",
    "\n",
    "def build_aadt_comparison_df(\n",
    "    aadt_locations,\n",
    "    df_tc,\n",
    "    df_stl,\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=None,\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a per-location comparison combining:\n",
    "      - Traditional (Traffic Census) AADT\n",
    "      - Non-traditional (StreetLight) AADT\n",
    "      - TCE (%) = 100 * (non_trad_aadt - traditional_aadt) / traditional_aadt\n",
    "\n",
    "    Returns a pandas DataFrame (one row per location).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Build the two sides using your updated functions\n",
    "    trad_df = traditional_aadt_by_location(\n",
    "        aadt_locations=aadt_locations,\n",
    "        df_tc=df_tc,\n",
    "        as_df=True\n",
    "    )\n",
    "\n",
    "    nt_df = non_traditional_aadt_by_location(\n",
    "        aadt_locations=aadt_locations,\n",
    "        df_stl=df_stl,\n",
    "        daytype_filter=daytype_filter,\n",
    "        daypart_filter=daypart_filter,\n",
    "        modeoftravel_filter=modeoftravel_filter,\n",
    "        zonename_col=zonename_col,\n",
    "        stl_volume_col=stl_volume_col,\n",
    "        as_df=True\n",
    "    )\n",
    "\n",
    "    # 2) Merge on 'location'\n",
    "    merged = pd.merge(\n",
    "        trad_df,\n",
    "        nt_df,\n",
    "        how=\"inner\",\n",
    "        on=\"location\",\n",
    "        suffixes=(\"_trad\", \"_nt\")\n",
    "    )\n",
    "\n",
    "    # 3) Compute TCE (%), guarding against zero / NaN\n",
    "    def _tce(row):\n",
    "        t = row.get(\"traditional_aadt\")\n",
    "        n = row.get(\"non_trad_aadt\")\n",
    "        if pd.notna(t) and t != 0 and pd.notna(n):\n",
    "            return 100.0 * (n - t) / t\n",
    "        return np.nan\n",
    "\n",
    "    merged[\"tce_percent\"] = merged.apply(_tce, axis=1)\n",
    "\n",
    "    # 4) Stable, readable column order (only keep those that exist)\n",
    "    preferred_cols = [\n",
    "        \"location\",\n",
    "        # IDs & zones (pipe-joined for spreadsheet safety)\n",
    "        \"objectids\", \"n_objectids\", \"n_found_in_tc\", \"missing_objectids\",\n",
    "        \"ahead_zones\", \"behind_zones\",\n",
    "        # AADT metrics\n",
    "        \"traditional_ahead_mean\", \"traditional_behind_mean\", \"traditional_aadt\",\n",
    "        \"non_trad_ahead_mean\", \"non_trad_behind_mean\", \"non_trad_aadt\",\n",
    "        \"tce_percent\",\n",
    "        # Filters / metadata\n",
    "        \"daytype\",            # from Step 1\n",
    "        \"daytype_expected\",   # from Step 2 (original location metadata)\n",
    "        \"daytype_used\", \"daypart_used\", \"modeoftravel_used\",\n",
    "        # Debug / row counts\n",
    "        \"stl_ahead_rows\", \"stl_behind_rows\",\n",
    "        \"missing_ahead_zones\", \"missing_behind_zones\",\n",
    "    ]\n",
    "    cols = [c for c in preferred_cols if c in merged.columns]\n",
    "    merged = merged[cols].copy()\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f183dac9-4407-491a-8e88-76339267e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1) Build the combined comparison DataFrame\n",
    "cmp_df = build_aadt_comparison_df(\n",
    "    aadt_locations=aadt_locations,  # your dict/list structure\n",
    "    df_tc=df_tc,                                 # Traffic Census dataframe\n",
    "    df_stl=df_stl,                               # StreetLight dataframe\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c1107-e9cb-46c9-b8de-7b720f210b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rows in cmp_df:\", len(cmp_df))\n",
    "print(cmp_df[\"tce_percent\"].describe())\n",
    "print(\"any STL zeros?\", (cmp_df[\"non_trad_aadt\"] == 0).sum())\n",
    "print(\"missing STL zones (any)?\",\n",
    "      (cmp_df[\"missing_ahead_zones\"] != \"\").sum() +\n",
    "      (cmp_df[\"missing_behind_zones\"] != \"\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d0c76-b4a0-41a8-8ecd-ae3b2a05fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2) Quick peek\n",
    "#cmp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5789ef-bea8-4369-9f79-b8ff23aef61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3) (Optional) sort by absolute TCE to see big deltas first\n",
    "cmp_df = cmp_df.sort_values(\"tce_percent\", key=lambda s: s.abs(), ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd90d29-5647-4cf8-ac6a-911e4cea503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4) Export to CSV \n",
    "cmp_df.to_csv(\"step_3_comparison_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaade07-ead0-4958-ac76-cf4a98169217",
   "metadata": {},
   "source": [
    "## Step 4 Confidence Interval over TCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c453dbb8-0d46-4847-b296-00aee2b97787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------------------------------\n",
    "# # 4) Confidence interval over TCE\n",
    "# # ------------------------------------------------------\n",
    "\n",
    "def tce_confidence_interval(detail_df, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Compute summary stats over `detail_df[\"tce_percent\"]`.\n",
    "    Returns: (mean_tce, ci_lo, ci_hi, tcrit, t_stat)\n",
    "    \"\"\"\n",
    "    # Clean and extract\n",
    "    tces = pd.to_numeric(detail_df[\"tce_percent\"], errors=\"coerce\") \\\n",
    "             .replace([np.inf, -np.inf], np.nan) \\\n",
    "             .dropna().values\n",
    "    n = len(tces)\n",
    "    if n == 0:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    mean_tce = float(np.mean(tces))\n",
    "    if n > 1:\n",
    "        std_tce = float(np.std(tces, ddof=1))\n",
    "        se = std_tce / np.sqrt(n)\n",
    "        if se > 0:\n",
    "            dof = n - 1\n",
    "            tcrit = float(stats.t.ppf((1 + confidence) / 2, dof))\n",
    "            ci_lo = mean_tce - tcrit * se\n",
    "            ci_hi = mean_tce + tcrit * se\n",
    "            t_stat = mean_tce / se\n",
    "        else:\n",
    "            tcrit = ci_lo = ci_hi = t_stat = None\n",
    "    else:\n",
    "        std_tce = 0.0\n",
    "        se = 0.0\n",
    "        tcrit = ci_lo = ci_hi = t_stat = None\n",
    "\n",
    "    return mean_tce, ci_lo, ci_hi, tcrit, t_stat\n",
    "\n",
    "def tce_confidence_interval_df(detail_df, confidence=0.95) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Same as tce_confidence_interval, but returns a one-row DataFrame with\n",
    "    extra fields useful for reporting/export.\n",
    "    \"\"\"\n",
    "    tces = pd.to_numeric(detail_df[\"tce_percent\"], errors=\"coerce\") \\\n",
    "             .replace([np.inf, -np.inf], np.nan) \\\n",
    "             .dropna()\n",
    "    n = int(tces.shape[0])\n",
    "    if n == 0:\n",
    "        return pd.DataFrame([{\n",
    "            \"confidence\": confidence,\n",
    "            \"n\": 0,\n",
    "            \"dof\": None,\n",
    "            \"mean_tce\": None,\n",
    "            \"std_tce\": None,\n",
    "            \"se\": None,\n",
    "            \"t_critical\": None,\n",
    "            \"margin_of_error\": None,\n",
    "            \"ci_lower\": None,\n",
    "            \"ci_upper\": None,\n",
    "            \"t_statistic\": None,\n",
    "            \"p_value_two_sided\": None\n",
    "        }])\n",
    "\n",
    "    mean_tce = float(tces.mean())\n",
    "    if n > 1:\n",
    "        std_tce = float(tces.std(ddof=1))\n",
    "        se = std_tce / np.sqrt(n)\n",
    "        dof = n - 1\n",
    "        if se > 0:\n",
    "            tcrit = float(stats.t.ppf((1 + confidence) / 2, dof))\n",
    "            moe = tcrit * se\n",
    "            ci_lo = mean_tce - moe\n",
    "            ci_hi = mean_tce + moe\n",
    "            t_stat = mean_tce / se\n",
    "            p_val = float(2 * (1 - stats.t.cdf(abs(t_stat), dof)))\n",
    "        else:\n",
    "            tcrit = moe = ci_lo = ci_hi = t_stat = p_val = None\n",
    "    else:\n",
    "        std_tce = 0.0\n",
    "        se = 0.0\n",
    "        dof = None\n",
    "        tcrit = moe = ci_lo = ci_hi = t_stat = p_val = None\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        \"confidence\": confidence,\n",
    "        \"n\": n,\n",
    "        \"dof\": dof,\n",
    "        \"mean_tce\": mean_tce,\n",
    "        \"std_tce\": std_tce if n > 1 else None,\n",
    "        \"se\": se if n > 1 else None,\n",
    "        \"t_critical\": tcrit,\n",
    "        \"margin_of_error\": moe,\n",
    "        \"ci_lower\": ci_lo,\n",
    "        \"ci_upper\": ci_hi,\n",
    "        \"t_statistic\": t_stat,\n",
    "        \"p_value_two_sided\": p_val\n",
    "    }])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241190c1-880c-4346-89d8-16d9f93e2435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0) Normalize to objectid rows (works for either of your location formats)\n",
    "norm_rows = explode_locations_to_objectids(aadt_locations)  # or sr_605_d7_tc_aadt_locations\n",
    "\n",
    "# 4.1) Build comparison\n",
    "cmp_df = build_aadt_comparison_df(\n",
    "    aadt_locations=norm_rows,\n",
    "    df_tc=df_tc,\n",
    "    df_stl=df_stl,\n",
    "    daytype_filter=\"0: All Days (M-Su)\",\n",
    "    daypart_filter=\"0: All Day (12am-12am)\",\n",
    "    modeoftravel_filter=None,\n",
    "    zonename_col=\"zonename\",\n",
    "    stl_volume_col=\"averagedailysegmenttraffic(stlvolume)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e59e5-0f98-4f84-8bd5-ab0316187796",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"rows in cmp_df:\", len(cmp_df))\n",
    "print(cmp_df[\"tce_percent\"].describe())\n",
    "print(\"any STL zeros?\", (cmp_df[\"non_trad_aadt\"] == 0).sum())\n",
    "print(\"missing STL zones (any)?\",\n",
    "      (cmp_df[\"missing_ahead_zones\"] != \"\").sum() +\n",
    "      (cmp_df[\"missing_behind_zones\"] != \"\").sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e873a5-bcd8-4768-b725-cea3d92d6b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a905e049-79c8-4ebd-8bcf-960a16b19cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e5c18d-d27b-4dac-b75b-94b197eedbec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b695fb-0e44-4645-ae8f-6fcb17175f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2) Get the CI summary as a DataFrame\n",
    "# tce_summary_df = tce_confidence_interval_df(cmp_df, confidence=0.95)\n",
    "tce_summary_df = tce_confidence_interval_df(cmp_df, confidence=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a1f62-dec4-4836-8a54-6b25852def26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3) Quick peek\n",
    "print(tce_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6363af16-0246-473a-a00c-d08a1be533f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cmp_df[\"tce_percent\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641e0b0-4a40-4036-882b-9920ed463689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many observations? should ≈ number of objectids that matched both TC and STL\n",
    "#len(cmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4175424-b957-4010-9335-a0739acd8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # spot check a row you know well\n",
    "# cmp_df.loc[cmp_df[\"location\"].str.contains(\"IRWINDALE\", case=False)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa3c47-86df-4bd4-9cbc-5799b8b3f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4) Export to CSV \n",
    "cmp_df.to_csv(\"step_4_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b665779a-9ff5-4a08-8a83-788ab9b90caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_tce, ci_lower, ci_upper, t_critical, t_statistic = tce_confidence_interval(\n",
    "    cmp_df, confidence=0.95\n",
    ")\n",
    "\n",
    "print(\"Mean TCE:\", mean_tce)\n",
    "print(\"95% Confidence Interval:\", (ci_lower, ci_upper))\n",
    "print(\"t-test statistic:\", t_statistic)\n",
    "print(\"t-critical:\", t_critical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d42671-ced0-447a-be82-78951c4391d8",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3cc97-454f-421c-919e-0598072d1020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1) Did the merge keep enough rows?\n",
    "print(\"cmp_df rows:\", len(cmp_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e079d-ed05-401c-9754-40102ac9b993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If this is small (< 5) for SR-99, inspect which side is missing:\n",
    "trad_df = traditional_aadt_by_location(aadt_locations, df_tc, as_df=True)\n",
    "nt_df   = non_traditional_aadt_by_location(aadt_locations, df_stl, as_df=True)\n",
    "print(\"trad rows:\", len(trad_df), \"nt rows:\", len(nt_df))\n",
    "print(\"only in trad:\", len(set(trad_df.location) - set(nt_df.location)))\n",
    "print(\"only in nt:\",   len(set(nt_df.location)   - set(trad_df.location)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20084ac7-86ed-47ec-aa81-b8da6f61bb26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2) See what labels SR-99 actually has\n",
    "print(\"daytype samples:\", df_stl[\"daytype\"].dropna().unique()[:10])\n",
    "print(\"daypart samples:\", df_stl[\"daypart\"].dropna().unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa349127-605f-4b91-ae03-9eaf389c5bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3) How many rows survive the filter?\n",
    "filt = (df_stl[\"daytype\"] == \"0: All Days (M-Su)\") & (df_stl[\"daypart\"] == \"0: All Day (12am-12am)\")\n",
    "print(\"stl_filtered rows (strict):\", int(filt.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15162b0f-e9e1-4a50-a52e-a75deb7a295b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ece7a389-e31e-43a2-9cf4-ccb7e7eec747",
   "metadata": {},
   "source": [
    "### Mean TCE: -3.62\n",
    "Traffic Census Error (TCE)\n",
    "* A negative TCE of -3.62% means that on average, the StreetLight AADT estimates are about 3.62% lower than the official Caltrans Traffic Census counts.\n",
    "\n",
    "### 95% Confidence Interval (-10.78%, 3.54%)\n",
    "* Based on the sample of locations, the results suggest 95% confidence that the true average TCE (i.e., the average percent difference between StreetLight and Census across the entire population) falls somewhere between -10.78% and +3.54%.\n",
    "    * Since this interval includes zero, it's possible that the true average error is zero, meaning StreetLight might not be significantly over- or underestimating, on average.\n",
    "    * But the range is quite wide (~14 percentage points), which indicates some variability in the data or a small sample size.\n",
    "\n",
    "### T-Test Statistic  \n",
    "* **-1.059**: This means your observed sample mean is about **1.059 standard errors** below the expected population mean. Since it's not far enough from the threshold (2.093), the result is **not significant**.\n",
    "\n",
    "### Summary\n",
    "* On average, StreetLight data is underestimating AADT by about 3.6% on this subset of locations.\n",
    "* But with 95% confidence, the actual average error could be as much as 10.8% under or 3.5% over the true value.\n",
    "* Because zero is in that range, you can't definitively say it's underestimating — the difference might not be statistically significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84bef9d-85b4-44d0-aa13-ebfa9ed7ef73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d35c3b37-f7e4-41e8-abf2-083155ea8641",
   "metadata": {},
   "source": [
    "# AADT Confidence Interval - Interstate 605, District 7\n",
    "\n",
    "## FHWA Links\n",
    "* Guidelines for Obtaining AADT Estimates from Non-Traditional Sources:\n",
    "    * https://www.fhwa.dot.gov/policyinformation/travel_monitoring/pubs/aadtnt/Guidelines_for_AADT_Estimates_Final.pdf\n",
    "\n",
    "## AADT Analysis Locations\n",
    "* Locations were determined based on the location on installed & recording Traffic Operations cameras\n",
    "    * for additional information contact Zhenyu Zhu with Traffic Operations\n",
    "\n",
    "## Traffic Census Data\n",
    "* https://dot.ca.gov/programs/traffic-operations/census/traffic-volumes\n",
    "* Back AADT, Peak Month, and Peak Hour usually represents traffic South or West of the count location.  \n",
    "* Ahead AADT, Peak Month, and Peak Hour usually represents traffic North or East of the count location. Listing of routes with their designated  \n",
    "\n",
    "* Because the Back & Ahead counts are included at each location in the Traffic Census Data, (e.g., \"IRWINDALE, ARROW HIGHWAY\") only one [OBJECTID*] per location was pulled; for this analysis the North Bound Nodes were used for the analysis. \n",
    "    * for more information see the diagram: https://traffic.onramp.dot.ca.gov/downloads/traffic/files/performance/census/Back_and_Ahead_Leg_Traffic_Count_Diagram.pdf\n",
    "\n",
    "## StreetLight Analysis Data\n",
    "* Analysis Type == Network Performance\n",
    "* Segment Metrics\n",
    "* 2022 was used to match currently available Traffic Census Data (as of 8/27/2025)\n",
    "* pulled a variety of Day Types, but plan to just look at \"\"\"All Day Types\"\"\"\n",
    "* pulled a variety of Day Parts, but plan to just look at \"\"\"All Day Parts\"\"\"\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
