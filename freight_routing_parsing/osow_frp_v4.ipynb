{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2d5a2c-4b02-44f3-af2d-0febddc5fbc6",
   "metadata": {},
   "source": [
    "# parsing the ['authorizedhighways'] column in the all_permits data for OSOW vehicle permits\n",
    "\n",
    "## Duplicate created to clean the script while the other notebook works on pulling in the coordinates\n",
    "\n",
    "- ns\n",
    "\n",
    "- Originally requested by Stephen Yoon  \n",
    "    - data provided by Stephen's office\n",
    "- next step ==   \n",
    "    - get the coordinates for the intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acace08e-1139-4d26-876f-0101415d1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import gcsfs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e08b8f81-5d63-4b8f-a1b1-c9ab9565f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original_mapping is needed to standardize the highway names for the various records \n",
    "original_mapping = {\n",
    "    \"5\": \"I-5\", \"10\": \"I-10\", \"15\": \"I-15\", \"40\": \"I-40\", \"80\": \"I-80\", \"105\": \"I-105\", \"110\": \"I-110\",\n",
    "    \"205\": \"I-205\", \"210\": \"I-210\", \"215\": \"I-215\", \"280\": \"I-280\", \"380\": \"I-380\", \"405\": \"I-405\",\n",
    "    \"505\": \"I-505\", \"580\": \"I-580\", \"605\": \"I-605\", \"680\": \"I-680\", \"710\": \"I-710\", \"805\": \"I-805\",\n",
    "    \"880\": \"I-880\", \"980\": \"I-980\", \"1\": \"SR-1\", \"2\": \"SR-2\", \"3\": \"SR-3\", \"4\": \"SR-4\", \"7\": \"SR-7\",\n",
    "    \"8\": \"SR-8\", \"9\": \"SR-9\", \"11\": \"SR-11\", \"12\": \"SR-12\", \"13\": \"SR-13\", \"14\": \"SR-14\", \"15\": \"SR-15\",\n",
    "    \"16\": \"SR-16\", \"17\": \"SR-17\", \"18\": \"SR-18\", \"20\": \"SR-20\", \"22\": \"SR-22\", \"23\": \"SR-23\",\n",
    "    \"24\": \"SR-24\", \"25\": \"SR-25\", \"26\": \"SR-26\", \"27\": \"SR-27\", \"28\": \"SR-28\", \"29\": \"SR-29\",\n",
    "    \"32\": \"SR-32\", \"33\": \"SR-33\", \"34\": \"SR-34\", \"35\": \"SR-35\", \"36\": \"SR-36\", \"37\": \"SR-37\",\n",
    "    \"38\": \"SR-38\", \"39\": \"SR-39\", \"41\": \"SR-41\", \"43\": \"SR-43\", \"44\": \"SR-44\", \"45\": \"SR-45\",\n",
    "    \"46\": \"SR-46\", \"47\": \"SR-47\", \"49\": \"SR-49\", \"51\": \"SR-51\", \"52\": \"SR-52\", \"53\": \"SR-53\",\n",
    "    \"54\": \"SR-54\", \"55\": \"SR-55\", \"56\": \"SR-56\", \"57\": \"SR-57\", \"58\": \"SR-58\", \"59\": \"SR-59\",\n",
    "    \"60\": \"SR-60\", \"61\": \"SR-61\", \"62\": \"SR-62\", \"63\": \"SR-63\", \"65\": \"SR-65\", \"66\": \"SR-66\",\n",
    "    \"67\": \"SR-67\", \"68\": \"SR-68\", \"70\": \"SR-70\", \"71\": \"SR-71\", \"72\": \"SR-72\", \"73\": \"SR-73\",\n",
    "    \"74\": \"SR-74\", \"75\": \"SR-75\", \"76\": \"SR-76\", \"77\": \"SR-77\", \"78\": \"SR-78\", \"79\": \"SR-79\",\n",
    "    \"82\": \"SR-82\", \"83\": \"SR-83\", \"84\": \"SR-84\", \"85\": \"SR-85\", \"86\": \"SR-86\", \"87\": \"SR-87\",\n",
    "    \"88\": \"SR-88\", \"89\": \"SR-89\", \"90\": \"SR-90\", \"91\": \"SR-91\", \"92\": \"SR-92\", \"94\": \"SR-94\",\n",
    "    \"96\": \"SR-96\", \"98\": \"SR-98\", \"imperial highway\": \"SR-99\", \"99\": \"SR-99\", \"103\": \"SR-103\", \"104\": \"SR-104\", \"107\": \"SR-107\",\n",
    "    \"108\": \"SR-108\", \"109\": \"SR-109\", \"110\": \"SR-110\", \"111\": \"SR-111\", \"112\": \"SR-112\",\n",
    "    \"113\": \"SR-113\", \"114\": \"SR-114\", \"115\": \"SR-115\", \"116\": \"SR-116\", \"118\": \"SR-118\",\n",
    "    \"119\": \"SR-119\", \"120\": \"SR-120\", \"121\": \"SR-121\", \"123\": \"SR-123\", \"124\": \"SR-124\",\n",
    "    \"125\": \"SR-125\", \"126\": \"SR-126\", \"127\": \"SR-127\", \"128\": \"SR-128\", \"129\": \"SR-129\",\n",
    "    \"130\": \"SR-130\", \"131\": \"SR-131\", \"132\": \"SR-132\", \"133\": \"SR-133\", \"134\": \"SR-134\",\n",
    "    \"135\": \"SR-135\", \"136\": \"SR-136\", \"137\": \"SR-137\", \"138\": \"SR-138\", \"139\": \"SR-139\",\n",
    "    \"140\": \"SR-140\", \"142\": \"SR-142\", \"144\": \"SR-144\", \"145\": \"SR-145\", \"146\": \"SR-146\",\n",
    "    \"147\": \"SR-147\", \"149\": \"SR-149\", \"150\": \"SR-150\", \"151\": \"SR-151\", \"152\": \"SR-152\",\n",
    "    \"153\": \"SR-153\", \"154\": \"SR-154\", \"155\": \"SR-155\", \"156\": \"SR-156\", \"158\": \"SR-158\",\n",
    "    \"160\": \"SR-160\", \"161\": \"SR-161\", \"162\": \"SR-162\", \"163\": \"SR-163\", \"164\": \"SR-164\",\n",
    "    \"165\": \"SR-165\", \"166\": \"SR-166\", \"167\": \"SR-167\", \"168\": \"SR-168\", \"169\": \"SR-169\",\n",
    "    \"170\": \"SR-170\", \"172\": \"SR-172\", \"173\": \"SR-173\", \"174\": \"SR-174\", \"175\": \"SR-175\",\n",
    "    \"177\": \"SR-177\", \"178\": \"SR-178\", \"180\": \"SR-180\", \"182\": \"SR-182\", \"183\": \"SR-183\",\n",
    "    \"184\": \"SR-184\", \"185\": \"SR-185\", \"186\": \"SR-186\", \"187\": \"SR-187\", \"188\": \"SR-188\",\n",
    "    \"189\": \"SR-189\", \"190\": \"SR-190\", \"191\": \"SR-191\", \"192\": \"SR-192\", \"193\": \"SR-193\",\n",
    "    \"197\": \"SR-197\", \"198\": \"SR-198\", \"200\": \"SR-200\", \"201\": \"SR-201\", \"202\": \"SR-202\",\n",
    "    \"203\": \"SR-203\", \"204\": \"SR-204\", \"207\": \"SR-207\", \"210\": \"SR-210\", \"211\": \"SR-211\",\n",
    "    \"213\": \"SR-213\", \"216\": \"SR-216\", \"217\": \"SR-217\", \"218\": \"SR-218\", \"219\": \"SR-219\",\n",
    "    \"220\": \"SR-220\", \"221\": \"SR-221\", \"222\": \"SR-222\", \"223\": \"SR-223\", \"227\": \"SR-227\",\n",
    "    \"229\": \"SR-229\", \"232\": \"SR-232\", \"233\": \"SR-233\", \"236\": \"SR-236\", \"237\": \"SR-237\",\n",
    "    \"238\": \"SR-238\", \"241\": \"SR-241\", \"242\": \"SR-242\", \"243\": \"SR-243\", \"244\": \"SR-244\",\n",
    "    \"245\": \"SR-245\", \"246\": \"SR-246\", \"247\": \"SR-247\", \"253\": \"SR-253\", \"254\": \"SR-254\",\n",
    "    \"255\": \"SR-255\", \"259\": \"SR-259\", \"260\": \"SR-260\", \"261\": \"SR-261\", \"262\": \"SR-262\",\n",
    "    \"263\": \"SR-263\", \"265\": \"SR-265\", \"266\": \"SR-266\", \"267\": \"SR-267\", \"269\": \"SR-269\",\n",
    "    \"270\": \"SR-270\", \"271\": \"SR-271\", \"273\": \"SR-273\", \"275\": \"SR-275\", \"281\": \"SR-281\",\n",
    "    \"282\": \"SR-282\", \"283\": \"SR-283\", \"284\": \"SR-284\", \"299\": \"SR-299\", \"330\": \"SR-330\",\n",
    "    \"371\": \"SR-371\", \"780\": \"SR-780\", \"905\": \"SR-905\", \"6\": \"US-6\", \"50\": \"US-50\",\n",
    "    \"95\": \"US-95\", \"97\": \"US-97\", \"101\": \"US-101\", \"199\": \"US-199\", \"395\": \"US-395\"\n",
    "}\n",
    "\n",
    "# Generate extended mapping to include leading zeros\n",
    "road_mapping = {}\n",
    "for key, value in original_mapping.items():\n",
    "    road_mapping[key] = value  # Original\n",
    "    road_mapping[key.zfill(2)] = value  # 2-character zero-padded\n",
    "    road_mapping[key.zfill(3)] = value  # 3-character zero-padded\n",
    "\n",
    "#print(road_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "482262aa-fbc6-478a-91ad-71ca26c8bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_path = \"gs://calitp-analytics-data/data-analyses/big_data/freight/all_permits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94dc7a2c-e5db-4a60-88b0-5c5aa4961dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"all_permits_2023_sampleset.xlsx\",\n",
    "              \"all_permits_2024_sampleset.xlsx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f234ea9-0098-47df-a456-01e9d585078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "def load_excel_sheets_1(gcs_path, file_names):\n",
    "    \"\"\"\n",
    "    Pull in the first sheet from each Excel file in GCS, add a 'year' column based on the filename,\n",
    "    and remove records with NaN values in the 'permitnumber' column. Returns a concatenated DataFrame\n",
    "    with data from all files.\n",
    "\n",
    "    Parameters:\n",
    "    gcs_path (str): The Google Cloud Storage path where the files are located.\n",
    "    file_names (list): A list of Excel file names in the GCS path.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A single concatenated DataFrame with data from all files, a 'year' column, and\n",
    "                  records with NaN values in 'permitnumber' removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a Google Cloud Storage file system object\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    \n",
    "    # List to store all DataFrames\n",
    "    df_list = []\n",
    "    \n",
    "    # Suppress any warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Suppress the specific UserWarning\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        message=\"Your application has authenticated using end user credentials from Google Cloud SDK without a quota project.\",\n",
    "        category=UserWarning,\n",
    "        module=\"google.auth._default\"\n",
    "    )\n",
    "    \n",
    "    # Define the columns to keep\n",
    "    columns_to_keep = ['permitnumber', 'year', 'permitvalidfrom', 'permitvalidto', \n",
    "                       'loaddescription', 'origin', 'destination', 'authorizedhighways']\n",
    "    \n",
    "    # Loop through each file in the file list\n",
    "    for file in file_names:\n",
    "        # Extract the year from the filename\n",
    "        year = file.split('_')[2]  # Assuming the year is the third element when split by '_'\n",
    "        \n",
    "        # Open the file and read only the first sheet\n",
    "        with fs.open(f\"{gcs_path}{file}\", 'rb') as f:\n",
    "            df = pd.read_excel(f, sheet_name=0)  # Load only the first sheet\n",
    "        \n",
    "        # Clean headers by removing spaces and making characters lowercase\n",
    "        df.columns = [col.replace(\" \", \"\").lower() for col in df.columns]\n",
    "        \n",
    "        # Add 'year' column\n",
    "        df['year'] = year\n",
    "        \n",
    "        # Filter columns and remove rows with NaN in 'permitnumber'\n",
    "        df = df[columns_to_keep].dropna(subset=['permitnumber'])\n",
    "        \n",
    "        # Append to list\n",
    "        df_list.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Parsing function to create individual route locations\n",
    "def parse_routes(route_info):\n",
    "    segments = []\n",
    "\n",
    "    # Split the data by \"from\", \"to\", or standalone dash patterns\n",
    "    raw_segments = re.split(r'\\s*-\\s*from\\s+|\\s*-\\s*to\\s+|(?<!\\s)-\\s*', route_info)\n",
    "\n",
    "    # Process each segment\n",
    "    for segment in raw_segments:\n",
    "        # Split by ' - ' or '-'\n",
    "        sub_segments = re.split(r'\\s*-\\s*|\\s*-\\s*', segment)\n",
    "        \n",
    "        # Clean and add sub-segments\n",
    "        segments.extend([sub.strip() for sub in sub_segments if sub.strip()])\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "# Custom parsing function\n",
    "def extract_location(text):\n",
    "    # Stop keywords pattern\n",
    "    stop_keywords = r\"\\b(?:dr|drive|rd|ave|way|pkwy|parkway|skyway|road|avenue|blvd|boulevard|st|street|line|lane|ln|hwy|highway)\\b\"\n",
    "\n",
    "    # If 'from' exists, process it as before\n",
    "    if \"from\" in text.lower():\n",
    "        match = re.search(r\"from\\s+(`.*?`|'.*?'|\\w+(?:\\s+\\w+)*)\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            location = match.group(1)  # Extract the text after \"from\"\n",
    "            # Keep the stop keywords and remove everything after them\n",
    "            location = re.sub(r\"(\" + stop_keywords + r\").*\", r\"\\1\", location, flags=re.IGNORECASE).strip()\n",
    "            return location\n",
    "    else:\n",
    "        # If 'from' doesn't exist, look for a stop keyword and capture location\n",
    "        match = re.search(r\"(`.*?`|'.*?'|\\w+(?:\\s+\\w+)*)\\s+(\" + stop_keywords + r\")\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            location = match.group(1)  # Capture location before stop keyword\n",
    "            return location.strip()\n",
    "\n",
    "    return None  # If no match is found\n",
    "\n",
    "\n",
    "# Function to clean each string \n",
    "def clean_route(route):\n",
    "    if not isinstance(route, str):  # Handle non-string entries\n",
    "        return route\n",
    "    \n",
    "    # Define stop keywords regex (including 'exit')\n",
    "    stop_keywords = r\"\\b(?:dr|drive|rd|ave|way|pkwy|parkway|skyway|road|avenue|blvd|boulevard|st|street|line|lane|ln|hwy|highway|exit)\\b\"\n",
    "\n",
    "    # Extract numeric highway number with optional \"rte\" or \"route\" prefixes\n",
    "    numeric_match = re.match(r\"(?:rte|route)?\\s*(\\d+)\", route, flags=re.IGNORECASE)\n",
    "    if numeric_match:\n",
    "        return numeric_match.group(1)  # Return the numeric value\n",
    "    \n",
    "    # Extract street name up to and including the first stop keyword (but excluding the stop word itself)\n",
    "    street_match = re.search(rf\"(.*?\\b{stop_keywords}(?!\\s*exit)\\b)\", route, flags=re.IGNORECASE)\n",
    "    if street_match:\n",
    "        return street_match.group(1).strip()  # Return the street name up to the stop keyword (excluding 'exit')\n",
    "    \n",
    "    # If neither is found, return the original string\n",
    "    return route\n",
    "\n",
    "# A function to parse the ['authorizedhighways'] column to get the route information\n",
    "def process_route_locations(df, parse_routes, extract_location, clean_route, road_mapping):\n",
    "    \"\"\"\n",
    "    Process a dataframe to format and clean route-related columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input dataframe to process.\n",
    "    parse_routes (function): Function to parse the 'authorizedhighways' column into route segments.\n",
    "    extract_location (function): Function to extract location from a route segment.\n",
    "    clean_route (function): Function to clean individual route location entries.\n",
    "    road_mapping (dict): Mapping dictionary for road numbers to their corresponding classes.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The processed dataframe with updated route locations and columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the authorized highways field so the text is not all capitalized\n",
    "    df['authorizedhighways'] = df['authorizedhighways'].str.capitalize()\n",
    "\n",
    "    # Apply the parsing function to create lists of individual route locations\n",
    "    df['route_segments'] = df['authorizedhighways'].apply(parse_routes)\n",
    "\n",
    "    # Determine the maximum number of locations to create the necessary columns\n",
    "    max_locations = df['route_segments'].apply(len).max()\n",
    "\n",
    "    # Create new columns for each route location based on the maximum number of locations\n",
    "    for i in range(max_locations):\n",
    "        df[f'route_location_{i}'] = df['route_segments'].apply(lambda x: x[i] if i < len(x) else None)\n",
    "\n",
    "    # Drop the temporary route_segments column\n",
    "    df.drop(columns=['route_segments'], inplace=True)\n",
    "\n",
    "    # Add a new column with all values set to \"California\"\n",
    "    df.insert(5, \"state\", \"California\")  # Index 5 corresponds to the 6th column position\n",
    "\n",
    "    # Apply title case to the 'origin' and 'destination' columns\n",
    "    df['origin'] = df['origin'].str.title()\n",
    "    df['destination'] = df['destination'].str.title()\n",
    "\n",
    "    # Apply the extract_location function to the column\n",
    "    df[\"route_location_start\"] = df[\"route_location_0\"].apply(extract_location)\n",
    "\n",
    "    # Insert the route_location_start column into the 8th position\n",
    "    df.insert(8, \"route_location_start\", df.pop(\"route_location_start\"))\n",
    "\n",
    "    # Drop the [authorizedhighways] column\n",
    "    #df.drop(columns=['authorizedhighways'], inplace=True)\n",
    "\n",
    "    # Drop the route_location_0 field\n",
    "    df = df.drop(columns=['route_location_0'])\n",
    "\n",
    "    # Identify target columns excluding \"route_location_start\"\n",
    "    route_columns = [col for col in df.columns if col.startswith(\"route_location_\") and col != \"route_location_start\"]\n",
    "\n",
    "    # Apply the clean_route cleaning function to the target columns (columns that begin with the words \"route_location\")\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].apply(clean_route)\n",
    "\n",
    "    # Iterate through each \"route_location_\" column to remove the word \"exit\"\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].apply(lambda x: str(x).replace(\"exit\", \"\").strip() if isinstance(x, str) else x)\n",
    "\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "\n",
    "    # Update the road numbers to their corresponding road class numbers\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].astype(str).map(road_mapping).fillna(df[col])  # Keep original value if no mapping found\n",
    "\n",
    "    # Create a new field called 'route_location_origin' that identifies the street and city/state\n",
    "    df['route_location_origin'] = df['route_location_start'] + \" \" + df['origin'] + \", \" + df['state']\n",
    "\n",
    "    # Move the new column (['route_location_origin_0']) to the 9th position\n",
    "    columns = list(df.columns)\n",
    "    columns.insert(9, columns.pop(columns.index('route_location_origin')))\n",
    "    df = df[columns]\n",
    "    \n",
    "    \n",
    "\n",
    "    # Remove the 'route_location_destination_city' column if it exists\n",
    "    if 'route_location_start' in df.columns:\n",
    "        df = df.drop(columns=['route_location_start'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# A function to create the ['route_intersection_x'] columns\n",
    "def process_route_intersections(df):\n",
    "    \"\"\"\n",
    "    Process a dataframe to identify and process route location and intersection columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe to process.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A cleaned dataframe with processed route intersections.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify all columns with \"route_location_\" prefix\n",
    "    route_location_cols = [col for col in df.columns if col.startswith(\"route_location_\")]\n",
    "\n",
    "    # Initialize a counter for the new intersection column names\n",
    "    intersection_counter = 0\n",
    "\n",
    "    # Create new columns for intersections\n",
    "    for i in range(len(route_location_cols) - 1):\n",
    "        col1 = route_location_cols[i]\n",
    "        col2 = route_location_cols[i + 1]\n",
    "\n",
    "        # Name the new intersection column based on the counter\n",
    "        intersection_col = f\"route_intersection_{intersection_counter}\"\n",
    "\n",
    "        # Combine adjacent columns into one field (handle None gracefully)\n",
    "        df[intersection_col] = df[col1].astype(str) + \" and \" + df[col2].astype(str)\n",
    "        df[intersection_col] = df[intersection_col].replace(\"None and None\", None)  # Optional cleanup for all-None rows\n",
    "        intersection_counter += 1\n",
    "\n",
    "    # Identify columns that start with \"route_intersection_\"\n",
    "    intersection_cols = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "\n",
    "    # Iterate over each intersection column\n",
    "    for col in intersection_cols:\n",
    "        # Replace values ending with \" & None\" with None (Null)\n",
    "        df[col] = df[col].apply(lambda x: None if isinstance(x, str) and x.endswith(\" and None\") else x)\n",
    "\n",
    "    # Identify columns that start with \"route_intersection_\"\n",
    "    intersection_cols = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "\n",
    "    # Iterate over each intersection column to remove leading zeros from numeric values\n",
    "    for col in intersection_cols:\n",
    "        # Apply the transformation to each value in the column\n",
    "        df[col] = df[col].apply(lambda x: ' and '.join([part.lstrip('0') if part.isdigit() else part for part in str(x).split(' and ')]) if isinstance(x, str) else x)\n",
    "\n",
    "    # Create a list(?) called core_columns to be included in the next iteration of the dataframe\n",
    "    core_columns = [\n",
    "        \"permitnumber\", \"year\", \"permitvalidfrom\", \"permitvalidto\",\n",
    "        \"loaddescription\", \"state\", \"origin\", \"destination\", \"authorizedhighways\", \"route_location_origin\"\n",
    "    ]\n",
    "\n",
    "    # subset_columns combines the core columns with the intersection_cols identified earlier in the script\n",
    "    subset_columns = core_columns + intersection_cols\n",
    "\n",
    "    # this next line utilizes the defined subset_columns to create a cleaned up version of the dataframe including only the columns needed for this analysis \n",
    "    df = df[subset_columns]\n",
    "\n",
    "    # Replace None values in intersection_cols with empty strings\n",
    "    for col in intersection_cols:\n",
    "        df[col] = df[col].apply(lambda x: \"\" if x is None else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to get the last 'route_intersection_x' field\n",
    "def get_last_intersection(row):\n",
    "    # Identify columns that match the pattern 'route_intersection_x'\n",
    "    intersection_columns = [col for col in df.columns if col.startswith('route_intersection_')]\n",
    "    # Get the last non-null value among these columns\n",
    "    return row[intersection_columns].dropna().iloc[-1] if intersection_columns else None\n",
    "\n",
    "def add_route_location_destination_city(df):\n",
    "    # Function to get the last 'route_intersection_x' value\n",
    "    def get_last_intersection(row):\n",
    "        # Identify columns that match the pattern 'route_intersection_x'\n",
    "        intersection_columns = [col for col in df.columns if col.startswith('route_intersection_')]\n",
    "        # Get the last non-null value among these columns\n",
    "        return row[intersection_columns].dropna().iloc[-1] if len(intersection_columns) > 0 else None\n",
    "\n",
    "    # Create the new column\n",
    "    df['route_location_destination_city'] = df.apply(\n",
    "        lambda row: f\"{get_last_intersection(row)} {row['destination']}, {row['state']}\", axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_route_intersection_last(df):\n",
    "    \"\"\"\n",
    "    Create a 'route_intersection_last' column to capture the last non-null value\n",
    "    from all 'route_intersection_x' columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe to process.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The dataframe with the new 'route_intersection_last' column.\n",
    "    \"\"\"\n",
    "    # Identify all 'route_intersection_x' columns\n",
    "    intersection_columns = [col for col in df.columns if col.startswith('route_intersection_')]\n",
    "\n",
    "    if not intersection_columns:\n",
    "        raise ValueError(\"No 'route_intersection_' columns found in the dataframe.\")\n",
    "\n",
    "    # Ensure the columns are processed in order\n",
    "    intersection_columns = sorted(intersection_columns, key=lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "    # Create 'route_intersection_last' by finding the last non-null value row-wise\n",
    "    df['route_intersection_last'] = df[intersection_columns].apply(\n",
    "        lambda row: next((val for val in reversed(row) if pd.notnull(val) and val != ''), None), axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_route_intersection_destination(df):\n",
    "    \"\"\"\n",
    "    Create a new column 'route_intersection_destination' by combining 'route_intersection_last' \n",
    "    and 'route_location_destination_city'. Then, clean up the column by removing extra whitespace.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input dataframe to process.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The dataframe with the new 'route_intersection_destination' column and cleaned columns.\n",
    "    \"\"\"\n",
    "    # Create the new 'route_intersection_destination' column\n",
    "    df['route_intersection_destination'] = df['route_intersection_last'] + \" \" + df['route_location_destination_city']\n",
    "    \n",
    "    # Remove any extra whitespace or empty strings in the resulting combination\n",
    "    df['route_intersection_destination'] = df['route_intersection_destination'].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    # Drop the 'route_location_destination_city' column if it exists\n",
    "    if 'route_location_destination_city' in df.columns:\n",
    "        df.drop(columns=['route_location_destination_city'], inplace=True)\n",
    "\n",
    "    # Drop the 'route_intersection_last' column if it exists\n",
    "    if 'route_intersection_last' in df.columns:\n",
    "        df.drop(columns=['route_intersection_last'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_intersection(df, intersection_columns):\n",
    "    \"\"\"\n",
    "    Normalizes intersections by ordering highway identifiers numerically.\n",
    "    If both sides of the intersection are highways (I-, SR-, or US-),\n",
    "    ensures the smaller-numbered highway appears first.\n",
    "\n",
    "    Args:\n",
    "        intersection (str): The intersection string in the format \"Location1 and Location2\".\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized intersection string, or the original string if no changes are needed.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    if not intersection_columns or pd.isna(intersection_columns):\n",
    "        return intersection_columns\n",
    "\n",
    "    parts = [part.strip() for part in intersection_columns.split(\"and\")]\n",
    "    if len(parts) != 2:\n",
    "        return intersection_columns  # Return as-is if not exactly two parts\n",
    "\n",
    "    pattern = r\"^(I-|SR-|US-)(\\d+)$\"  # Pattern to match highway identifiers\n",
    "\n",
    "    match1 = re.match(pattern, parts[0])\n",
    "    match2 = re.match(pattern, parts[1])\n",
    "\n",
    "    if match1 and match2:\n",
    "        # Extract numeric portions and compare\n",
    "        num1 = int(match1.group(2))\n",
    "        num2 = int(match2.group(2))\n",
    "\n",
    "        if num1 > num2:\n",
    "            # Swap to ensure the smaller number comes first\n",
    "            parts = [parts[1], parts[0]]\n",
    "\n",
    "    return \" and \".join(parts)\n",
    "\n",
    "\n",
    "def process_intersections(df, intersection_columns):\n",
    "    \"\"\"\n",
    "    Processes and normalizes intersections in specified columns of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns (list): List of column names to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with normalized intersections.\n",
    "    \"\"\"\n",
    "    for col in intersection_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(normalize_intersection)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A function to clean \"route_intersection\" columns\n",
    "def clean_intersections(df):\n",
    "    # Find all columns starting with \"route_intersection_\"\n",
    "    intersection_columns = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "    \n",
    "    # Replace variations of \"imperial highway\" (e.g., \"imperial hwy\") with \"SR-99\"\n",
    "    for col in intersection_columns:\n",
    "        df[col] = df[col].str.replace(\n",
    "            r\"(?i)\\bimperial (highway|hwy)\\b\", \"SR-99\", regex=True\n",
    "        )  # Matches \"imperial highway\" or \"imperial hwy\"\n",
    "    \n",
    "    # Specific replacement logic for \"route_intersection_0\"\n",
    "    if \"route_intersection_0\" in df.columns:\n",
    "        def replace_intersection(value):\n",
    "            if isinstance(value, str) and value.lower().startswith(\"az line\") and value.endswith(\"I-40\"):\n",
    "                return \"colorado river bridge and I-10, California\"\n",
    "            return value\n",
    "\n",
    "        df[\"route_intersection_0\"] = df[\"route_intersection_0\"].apply(replace_intersection)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e2e69-8f6b-46a2-b9aa-c4ea4d0efbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a9de59a-a73e-4fee-aeaa-4818265b8bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_workflow(gcs_path, file_names):\n",
    "    # Load the Excel sheets from GCS and get the initial DataFrame\n",
    "    df = load_excel_sheets_1(gcs_path, file_names)\n",
    "    \n",
    "    # Process route locations\n",
    "    df = process_route_locations(df, parse_routes, extract_location, clean_route, road_mapping)\n",
    "       \n",
    "    # Process route intersections\n",
    "    df = process_route_intersections(df)\n",
    "    \n",
    "    # clean the intersections to map mappign easier\n",
    "    df = clean_intersections(df)\n",
    "    \n",
    "    # Add route location destination city\n",
    "    df = add_route_location_destination_city(df)\n",
    "    \n",
    "    # Create route intersection last\n",
    "    df = create_route_intersection_last(df)\n",
    "    \n",
    "    # Create route intersection destination\n",
    "    df = create_route_intersection_destination(df)\n",
    "    \n",
    "    # Find all columns starting with \"route_intersection_\"\n",
    "    intersection_columns = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "    \n",
    "    # this function normalizes the State/US Routes & Interstates - so that the intersection order is standardized in the dataset (e.g., I-5 and I-80 (lower values go on the left))\n",
    "    df = process_intersections(df, intersection_columns)\n",
    "\n",
    "\n",
    "    \n",
    "    # # Replace the last non-null route intersection with destination\n",
    "    # df = replace_last_non_null_intersection(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e8e3284-a887-4282-9732-e8e06fe7ba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "normalize_intersection() missing 1 required positional argument: 'intersection_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgcs_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m, in \u001b[0;36mprocess_workflow\u001b[0;34m(gcs_path, file_names)\u001b[0m\n\u001b[1;32m     24\u001b[0m intersection_columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroute_intersection_\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# this function normalizes the State/US Routes & Interstates - so that the intersection order is standardized in the dataset (e.g., I-5 and I-80 (lower values go on the left))\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_intersections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintersection_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# # Replace the last non-null route intersection with destination\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# df = replace_last_non_null_intersection(df)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Cell \u001b[0;32mIn[5], line 415\u001b[0m, in \u001b[0;36mprocess_intersections\u001b[0;34m(df, intersection_columns)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m intersection_columns:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m--> 415\u001b[0m         df[col] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalize_intersection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: normalize_intersection() missing 1 required positional argument: 'intersection_columns'"
     ]
    }
   ],
   "source": [
    "df = process_workflow(gcs_path, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95d3c6-b34f-4992-82bf-0af1e4ef7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f06b84a-a49d-49a7-8e84-390ed25244f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"justlooking_subset_v1_od.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6eead4-5e1f-4376-805f-c4c8862d518a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f460d26-bc72-417c-b0c5-c2e65223fb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55719466-3268-42bf-8136-c67fbb56bb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
