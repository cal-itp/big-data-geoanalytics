{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470c9245-3453-414e-92a6-fcfa768d2fd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Oversized Overweight Vehicle Permit Route Parsing Concept Validation  \n",
    "* parsing the ['authorizedhighways'] column in the all_permits data for OSOW vehicle permits\n",
    "\n",
    "- Developed by the Caltrans Data and Digital Services Office of Big Data and GeoAnalytics\n",
    "\n",
    "- Originally requested by Stephen Yoon  \n",
    "    - Original data provided by Stephen's office\n",
    "\n",
    "## Second phase\n",
    "- this script is being developed to run the entirety of the OSOW Freight Data that was received from Traffic Opts and is currently being stored on GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61713604-8c30-4ef9-bb08-58758219b55a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 01.01 - load python modules into notebook\n",
    "\n",
    "# import modules\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import gcsfs\n",
    "import re\n",
    "\n",
    "# pull in the coordinates from the utils docs\n",
    "from osow_frp_o_d_utils_v3 import origin_intersections, destination_intersections\n",
    "from shs_intersections_utils import shs_intersections\n",
    "from shs_names_utils import original_mapping, road_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d25db29-6051-4e96-924d-ec0a2a785ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02.01 - data import path & file names\n",
    "\n",
    "# the GCS path where the data is stored\n",
    "gcs_path = \"gs://calitp-analytics-data/data-analyses/big_data/freight/all_permits/\"\n",
    "\n",
    "# just pulling in the sample sets for now\n",
    "file_names = [\"all_permits_2023_sampleset.xlsx\",\n",
    "              \"all_permits_2024_sampleset.xlsx\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524e2580-ec68-45e0-99b6-14425f6aa75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02.01 - data import functions\n",
    "def load_excel_sheets_1(gcs_path, file_names):\n",
    "    \"\"\"\n",
    "    Pull in the first sheet from each Excel file in GCS, add a 'year' column based on the filename,\n",
    "    and remove records with NaN values in the 'permitnumber' column. Returns a concatenated DataFrame\n",
    "    with data from all files.\n",
    "\n",
    "    Parameters:\n",
    "    gcs_path (str): The Google Cloud Storage path where the files are located.\n",
    "    file_names (list): A list of Excel file names in the GCS path.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A single concatenated DataFrame with data from all files, a 'year' column, and\n",
    "                  records with NaN values in 'permitnumber' removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a Google Cloud Storage file system object\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    \n",
    "    # List to store all DataFrames\n",
    "    df_list = []\n",
    "    \n",
    "    # Suppress any warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Suppress the specific UserWarning\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        message=\"Your application has authenticated using end user credentials from Google Cloud SDK without a quota project.\",\n",
    "        category=UserWarning,\n",
    "        module=\"google.auth._default\"\n",
    "    )\n",
    "    \n",
    "    # Define the columns to keep\n",
    "    columns_to_keep = ['permitnumber', 'year', 'permitvalidfrom', 'permitvalidto', \n",
    "                       'loaddescription', 'origin', 'destination', 'authorizedhighways']\n",
    "    \n",
    "    # Loop through each file in the file list\n",
    "    for file in file_names:\n",
    "        # Extract the year from the filename\n",
    "        year = file.split('_')[2]  # Assuming the year is the third element when split by '_'\n",
    "        \n",
    "        # Open the file and read only the first sheet\n",
    "        with fs.open(f\"{gcs_path}{file}\", 'rb') as f:\n",
    "            df = pd.read_excel(f, sheet_name=0)  # Load only the first sheet\n",
    "        \n",
    "        # Clean headers by removing spaces and making characters lowercase\n",
    "        df.columns = [col.replace(\" \", \"\").lower() for col in df.columns]\n",
    "        \n",
    "        # Add 'year' column\n",
    "        df['year'] = year\n",
    "        \n",
    "        # Filter columns and remove rows with NaN in 'permitnumber'\n",
    "        df = df[columns_to_keep].dropna(subset=['permitnumber'])\n",
    "        \n",
    "        # Append to list\n",
    "        df_list.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c3fa0-8259-413c-8445-0c37e1208fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0489f-537e-4e4e-993e-e0da65da53b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42b4517-4ac0-4ea5-9f0b-c48dd2faf164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7fa85c-b554-4249-855b-c20b9161e15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc2fc89-4954-4606-811d-8912983cc581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02.02 - data processing functions\n",
    "\n",
    "\n",
    "# Parsing function to create individual route locations\n",
    "def parse_routes(route_info):\n",
    "    segments = []\n",
    "\n",
    "    # Split the data by \"from\", \"to\", or standalone dash patterns\n",
    "    raw_segments = re.split(r'\\s*-\\s*from\\s+|\\s*-\\s*to\\s+|(?<!\\s)-\\s*', route_info)\n",
    "\n",
    "    # Process each segment\n",
    "    for segment in raw_segments:\n",
    "        # Split by ' - ' or '-'\n",
    "        sub_segments = re.split(r'\\s*-\\s*|\\s*-\\s*', segment)\n",
    "        \n",
    "        # Clean and add sub-segments\n",
    "        segments.extend([sub.strip() for sub in sub_segments if sub.strip()])\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "# Custom parsing function\n",
    "def extract_location(text):\n",
    "    # Stop keywords pattern\n",
    "    stop_keywords = r\"\\b(?:dr|drive|rd|ave|way|pkwy|parkway|skyway|road|avenue|blvd|boulevard|st|street|line|lane|ln|hwy|highway)\\b\"\n",
    "\n",
    "    # If 'from' exists, process it as before\n",
    "    if \"from\" in text.lower():\n",
    "        match = re.search(r\"from\\s+(`.*?`|'.*?'|\\w+(?:\\s+\\w+)*)\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            location = match.group(1)  # Extract the text after \"from\"\n",
    "            # Keep the stop keywords and remove everything after them\n",
    "            location = re.sub(r\"(\" + stop_keywords + r\").*\", r\"\\1\", location, flags=re.IGNORECASE).strip()\n",
    "            return location\n",
    "    else:\n",
    "        # If 'from' doesn't exist, look for a stop keyword and capture location\n",
    "        match = re.search(r\"(`.*?`|'.*?'|\\w+(?:\\s+\\w+)*)\\s+(\" + stop_keywords + r\")\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            location = match.group(1)  # Capture location before stop keyword\n",
    "            return location.strip()\n",
    "\n",
    "    return None  # If no match is found\n",
    "\n",
    "\n",
    "# Function to clean each string \n",
    "def clean_route(route):\n",
    "    if not isinstance(route, str):  # Handle non-string entries\n",
    "        return route\n",
    "    \n",
    "    # Define stop keywords regex (including 'exit')\n",
    "    stop_keywords = r\"\\b(?:dr|drive|rd|ave|way|pkwy|parkway|skyway|road|avenue|blvd|boulevard|st|street|line|lane|ln|hwy|highway|exit)\\b\"\n",
    "\n",
    "    # Extract numeric highway number with optional \"rte\" or \"route\" prefixes\n",
    "    numeric_match = re.match(r\"(?:rte|route)?\\s*(\\d+)\", route, flags=re.IGNORECASE)\n",
    "    if numeric_match:\n",
    "        return numeric_match.group(1)  # Return the numeric value\n",
    "    \n",
    "    # Extract street name up to and including the first stop keyword (but excluding the stop word itself)\n",
    "    street_match = re.search(rf\"(.*?\\b{stop_keywords}(?!\\s*exit)\\b)\", route, flags=re.IGNORECASE)\n",
    "    if street_match:\n",
    "        return street_match.group(1).strip()  # Return the street name up to the stop keyword (excluding 'exit')\n",
    "    \n",
    "    # If neither is found, return the original string\n",
    "    return route\n",
    "\n",
    "# A function to parse the ['authorizedhighways'] column to get the route information\n",
    "def process_route_locations(df, parse_routes, extract_location, clean_route, road_mapping):\n",
    "    \"\"\"\n",
    "    Process a dataframe to format and clean route-related columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input dataframe to process.\n",
    "    parse_routes (function): Function to parse the 'authorizedhighways' column into route segments.\n",
    "    extract_location (function): Function to extract location from a route segment.\n",
    "    clean_route (function): Function to clean individual route location entries.\n",
    "    road_mapping (dict): Mapping dictionary for road numbers to their corresponding classes.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The processed dataframe with updated route locations and columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the authorized highways field so the text is not all capitalized\n",
    "    df['authorizedhighways'] = df['authorizedhighways'].str.capitalize()\n",
    "\n",
    "    # Apply the parsing function to create lists of individual route locations\n",
    "    df['route_segments'] = df['authorizedhighways'].apply(parse_routes)\n",
    "\n",
    "    # Determine the maximum number of locations to create the necessary columns\n",
    "    max_locations = df['route_segments'].apply(len).max()\n",
    "\n",
    "    # Create new columns for each route location based on the maximum number of locations\n",
    "    for i in range(max_locations):\n",
    "        df[f'route_location_{i}'] = df['route_segments'].apply(lambda x: x[i] if i < len(x) else None)\n",
    "\n",
    "    # Drop the temporary route_segments column\n",
    "    df.drop(columns=['route_segments'], inplace=True)\n",
    "\n",
    "    # Add a new column with all values set to \"California\"\n",
    "    df.insert(5, \"state\", \"California\")  # Index 5 corresponds to the 6th column position\n",
    "\n",
    "    # Apply title case to the 'origin' and 'destination' columns\n",
    "    df['origin'] = df['origin'].str.title()\n",
    "    df['destination'] = df['destination'].str.title()\n",
    "\n",
    "    # Apply the extract_location function to the column\n",
    "    df[\"route_location_start\"] = df[\"route_location_0\"].apply(extract_location)\n",
    "\n",
    "    # Insert the route_location_start column into the 8th position\n",
    "    df.insert(8, \"route_location_start\", df.pop(\"route_location_start\"))\n",
    "\n",
    "    # Drop the [authorizedhighways] column\n",
    "    #df.drop(columns=['authorizedhighways'], inplace=True)\n",
    "\n",
    "    # Drop the route_location_0 field\n",
    "    df = df.drop(columns=['route_location_0'])\n",
    "\n",
    "    # Identify target columns excluding \"route_location_start\"\n",
    "    route_columns = [col for col in df.columns if col.startswith(\"route_location_\") and col != \"route_location_start\"]\n",
    "\n",
    "    # Apply the clean_route cleaning function to the target columns (columns that begin with the words \"route_location\")\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].apply(clean_route)\n",
    "\n",
    "    # Iterate through each \"route_location_\" column to remove the word \"exit\"\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].apply(lambda x: str(x).replace(\"exit\", \"\").strip() if isinstance(x, str) else x)\n",
    "\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "\n",
    "    # Update the road numbers to their corresponding road class numbers\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].astype(str).map(road_mapping).fillna(df[col])  # Keep original value if no mapping found\n",
    "\n",
    "    # Create a new field called 'route_location_origin' that identifies the street and city/state\n",
    "    df['route_location_origin'] = df['route_location_start'] + \" \" + df['origin'] + \", \" + df['state']\n",
    "\n",
    "    # Move the new column (['route_location_origin_0']) to the 9th position\n",
    "    columns = list(df.columns)\n",
    "    columns.insert(9, columns.pop(columns.index('route_location_origin')))\n",
    "    df = df[columns]\n",
    "    \n",
    "    \n",
    "\n",
    "    # Remove the 'route_location_destination_city' column if it exists\n",
    "    if 'route_location_start' in df.columns:\n",
    "        df = df.drop(columns=['route_location_start'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# A function to create the ['route_intersection_x'] columns\n",
    "def process_route_intersections(df):\n",
    "    \"\"\"\n",
    "    Process a dataframe to identify and process route location and intersection columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe to process.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A cleaned dataframe with processed route intersections.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify all columns with \"route_location_\" prefix\n",
    "    route_location_cols = [col for col in df.columns if col.startswith(\"route_location_\")]\n",
    "\n",
    "    # Initialize a counter for the new intersection column names\n",
    "    intersection_counter = 0\n",
    "\n",
    "    # Create new columns for intersections\n",
    "    for i in range(len(route_location_cols) - 1):\n",
    "        col1 = route_location_cols[i]\n",
    "        col2 = route_location_cols[i + 1]\n",
    "\n",
    "        # Name the new intersection column based on the counter\n",
    "        intersection_col = f\"route_intersection_{intersection_counter}\"\n",
    "\n",
    "        # Combine adjacent columns into one field (handle None gracefully)\n",
    "        df[intersection_col] = df[col1].astype(str) + \" and \" + df[col2].astype(str)\n",
    "        df[intersection_col] = df[intersection_col].replace(\"None and None\", None)  # Optional cleanup for all-None rows\n",
    "        intersection_counter += 1\n",
    "\n",
    "    # Identify columns that start with \"route_intersection_\"\n",
    "    intersection_cols = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "\n",
    "    # Iterate over each intersection column\n",
    "    for col in intersection_cols:\n",
    "        # Replace values ending with \" & None\" with None (Null)\n",
    "        df[col] = df[col].apply(lambda x: None if isinstance(x, str) and x.endswith(\" and None\") else x)\n",
    "\n",
    "    # Identify columns that start with \"route_intersection_\"\n",
    "    intersection_cols = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "\n",
    "    # Iterate over each intersection column to remove leading zeros from numeric values\n",
    "    for col in intersection_cols:\n",
    "        # Apply the transformation to each value in the column\n",
    "        df[col] = df[col].apply(lambda x: ' and '.join([part.lstrip('0') if part.isdigit() else part for part in str(x).split(' and ')]) if isinstance(x, str) else x)\n",
    "\n",
    "    # Create a list(?) called core_columns to be included in the next iteration of the dataframe\n",
    "    core_columns = [\n",
    "        \"permitnumber\", \"year\", \"permitvalidfrom\", \"permitvalidto\",\n",
    "        \"loaddescription\", \"state\", \"origin\", \"destination\", \"authorizedhighways\", \"route_location_origin\"\n",
    "    ]\n",
    "\n",
    "    # subset_columns combines the core columns with the intersection_cols identified earlier in the script\n",
    "    subset_columns = core_columns + intersection_cols\n",
    "\n",
    "    # this next line utilizes the defined subset_columns to create a cleaned up version of the dataframe including only the columns needed for this analysis \n",
    "    df = df[subset_columns]\n",
    "\n",
    "    # Replace None values in intersection_cols with empty strings\n",
    "    for col in intersection_cols:\n",
    "        df[col] = df[col].apply(lambda x: \"\" if x is None else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to get the last 'route_intersection_x' field\n",
    "def get_last_intersection(row):\n",
    "    # Identify columns that match the pattern 'route_intersection_x'\n",
    "    intersection_columns = [col for col in df.columns if col.startswith('route_intersection_')]\n",
    "    # Get the last non-null value among these columns\n",
    "    return row[intersection_columns].dropna().iloc[-1] if intersection_columns else None\n",
    "\n",
    "def add_route_location_destination_city(df):\n",
    "    # Function to get the last 'route_intersection_x' value\n",
    "    def get_last_intersection(row):\n",
    "        # Identify columns that match the pattern 'route_intersection_x'\n",
    "        intersection_columns = [col for col in df.columns if col.startswith('route_intersection_')]\n",
    "        # Get the last non-null value among these columns\n",
    "        return row[intersection_columns].dropna().iloc[-1] if len(intersection_columns) > 0 else None\n",
    "\n",
    "    # Create the new column\n",
    "    df['route_location_destination_city'] = df.apply(\n",
    "        lambda row: f\"{get_last_intersection(row)} {row['destination']}, {row['state']}\", axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_route_intersection_last(df):\n",
    "    \"\"\"\n",
    "    Create a 'route_intersection_last' column to capture the last non-null value\n",
    "    from all 'route_intersection_x' columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe to process.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The dataframe with the new 'route_intersection_last' column.\n",
    "    \"\"\"\n",
    "    # Identify all 'route_intersection_x' columns\n",
    "    intersection_columns = [col for col in df.columns if col.startswith('route_intersection_')]\n",
    "\n",
    "    if not intersection_columns:\n",
    "        raise ValueError(\"No 'route_intersection_' columns found in the dataframe.\")\n",
    "\n",
    "    # Ensure the columns are processed in order\n",
    "    intersection_columns = sorted(intersection_columns, key=lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "    # Create 'route_intersection_last' by finding the last non-null value row-wise\n",
    "    df['route_intersection_last'] = df[intersection_columns].apply(\n",
    "        lambda row: next((val for val in reversed(row) if pd.notnull(val) and val != ''), None), axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def create_route_intersection_destination(df):\n",
    "    \"\"\"\n",
    "    Create a new column 'route_intersection_destination' by combining 'route_intersection_last' \n",
    "    and 'route_location_destination_city'. Then, clean up the column by removing extra whitespace.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input dataframe to process.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The dataframe with the new 'route_intersection_destination' column and cleaned columns.\n",
    "    \"\"\"\n",
    "    # Create the new 'route_intersection_destination' column\n",
    "    df['route_intersection_destination'] = df['route_intersection_last'] + \" \" + df['route_location_destination_city']\n",
    "    \n",
    "    # Remove extra spaces by stripping and ensuring only single spaces exist\n",
    "    df['route_intersection_destination'] = df['route_intersection_destination'].apply(\n",
    "        lambda x: \" \".join(x.split()) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "    # Drop unnecessary columns if they exist\n",
    "    df.drop(columns=['route_location_destination_city', 'route_intersection_last'], errors='ignore', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# A function to clean \"route_intersection\" columns\n",
    "def clean_route_intersections(df):\n",
    "    # Find all columns starting with \"route_intersection_\"\n",
    "    intersection_columns = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "    \n",
    "    # Replace variations of \"imperial highway\" (e.g., \"imperial hwy\") with \"SR-99\"\n",
    "    for col in intersection_columns:\n",
    "        df[col] = df[col].str.replace(\n",
    "            r\"(?i)\\bimperial (highway|hwy)\\b\", \"SR-99\", regex=True\n",
    "        )  # Matches \"imperial highway\" or \"imperial hwy\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_workflow(gcs_path, file_names):\n",
    "    # Load the Excel sheets from GCS and get the initial DataFrame\n",
    "    df = load_excel_sheets_1(gcs_path, file_names)\n",
    "    \n",
    "    # Process route locations\n",
    "    df = process_route_locations(df, parse_routes, extract_location, clean_route, road_mapping)\n",
    "       \n",
    "    # Process route intersections\n",
    "    df = process_route_intersections(df)\n",
    "    \n",
    "    # Add route location destination city\n",
    "    df = add_route_location_destination_city(df)\n",
    "    \n",
    "    # Create route intersection last\n",
    "    df = create_route_intersection_last(df)\n",
    "    \n",
    "    # Create route intersection destination\n",
    "    df = create_route_intersection_destination(df)\n",
    "\n",
    "    # this is a new script - intended to help clean up the [\"route_locations_x\"] before they become intersections\n",
    "    df = clean_route_intersections(df)\n",
    "    \n",
    "    \n",
    "    # # Replace the last non-null route intersection with destination\n",
    "    # df = replace_last_non_null_intersection(df)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b69ddd-1772-42de-8435-c31f38d6c5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_request non-retriable exception: ('Error code invalid_grant: Refresh token has expired', '{\"error\":\"invalid_grant\",\"error_description\":\"Refresh token has expired\"}')\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/gcsfs/retry.py\", line 123, in retry_request\n",
      "    return await func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/gcsfs/core.py\", line 421, in _request\n",
      "    headers=self._get_headers(headers),\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/gcsfs/core.py\", line 400, in _get_headers\n",
      "    self.credentials.apply(out)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/gcsfs/credentials.py\", line 187, in apply\n",
      "    self.maybe_refresh()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/gcsfs/credentials.py\", line 182, in maybe_refresh\n",
      "    self.credentials.refresh(req)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/google/auth/external_account_authorized_user.py\", line 281, in refresh\n",
      "    response_data = self._make_sts_request(request)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/google/auth/external_account_authorized_user.py\", line 292, in _make_sts_request\n",
      "    return self._sts_client.refresh_token(request, self._refresh_token)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/google/oauth2/sts.py\", line 172, in refresh_token\n",
      "    return self._make_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/google/oauth2/sts.py\", line 88, in _make_request\n",
      "    utils.handle_error_response(response_body)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/google/oauth2/utils.py\", line 168, in handle_error_response\n",
      "    raise exceptions.OAuthError(error_details, response_body)\n",
      "google.auth.exceptions.OAuthError: ('Error code invalid_grant: Refresh token has expired', '{\"error\":\"invalid_grant\",\"error_description\":\"Refresh token has expired\"}')\n"
     ]
    },
    {
     "ename": "OAuthError",
     "evalue": "('Error code invalid_grant: Refresh token has expired', '{\"error\":\"invalid_grant\",\"error_description\":\"Refresh token has expired\"}')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOAuthError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgcs_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 316\u001b[0m, in \u001b[0;36mprocess_workflow\u001b[0;34m(gcs_path, file_names)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess_workflow\u001b[39m(gcs_path, file_names):\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# Load the Excel sheets from GCS and get the initial DataFrame\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mload_excel_sheets_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgcs_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# Process route locations\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     df \u001b[38;5;241m=\u001b[39m process_route_locations(df, parse_routes, extract_location, clean_route, road_mapping)\n",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m, in \u001b[0;36mload_excel_sheets_1\u001b[0;34m(gcs_path, file_names)\u001b[0m\n\u001b[1;32m     41\u001b[0m year \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m2\u001b[39m]  \u001b[38;5;66;03m# Assuming the year is the third element when split by '_'\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Open the file and read only the first sheet\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgcs_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     45\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(f, sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Load only the first sheet\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Clean headers by removing spaces and making characters lowercase\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/spec.py:1295\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[0;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[0;32m-> 1295\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:1516\u001b[0m, in \u001b[0;36mGCSFileSystem._open\u001b[0;34m(self, path, mode, block_size, cache_options, acl, consistency, metadata, autocommit, fixed_key_metadata, generation, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m     block_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_block_size\n\u001b[1;32m   1515\u001b[0m const \u001b[38;5;241m=\u001b[39m consistency \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsistency\n\u001b[0;32m-> 1516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGCSFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconsistency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43macl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_key_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_key_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:1675\u001b[0m, in \u001b[0;36mGCSFile.__init__\u001b[0;34m(self, gcsfs, path, mode, block_size, autocommit, cache_type, cache_options, acl, consistency, metadata, content_type, timeout, fixed_key_metadata, generation, **kwargs)\u001b[0m\n\u001b[1;32m   1673\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to open a bucket\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration \u001b[38;5;241m=\u001b[39m _coalesce_generation(generation, path_generation)\n\u001b[0;32m-> 1675\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgcsfs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocommit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcsfs \u001b[38;5;241m=\u001b[39m gcsfs\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbucket \u001b[38;5;241m=\u001b[39m bucket\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/spec.py:1651\u001b[0m, in \u001b[0;36mAbstractBufferedFile.__init__\u001b[0;34m(self, fs, path, mode, block_size, autocommit, cache_type, cache_options, size, **kwargs)\u001b[0m\n\u001b[1;32m   1649\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1651\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetails\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m caches[cache_type](\n\u001b[1;32m   1653\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_range, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcache_options\n\u001b[1;32m   1654\u001b[0m     )\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:1711\u001b[0m, in \u001b[0;36mGCSFile.details\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1708\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdetails\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1711\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1712\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_details\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/asyn.py:103\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/fsspec/asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     54\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     58\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:959\u001b[0m, in \u001b[0;36mGCSFileSystem._info\u001b[0;34m(self, path, generation, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;66;03m# Check exact file path\u001b[39;00m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 959\u001b[0m     exact \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object(path)\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;66;03m# this condition finds a \"placeholder\" - still need to check if it's a directory\u001b[39;00m\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exact[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exact[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:516\u001b[0m, in \u001b[0;36mGCSFileSystem._get_object\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m# Work around various permission settings. Prefer an object get (storage.objects.get), but\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# fall back to a bucket list + filter to object name (storage.objects.list).\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 516\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/o/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, bucket, key, json_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, generation\u001b[38;5;241m=\u001b[39mgeneration\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForbidden\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:437\u001b[0m, in \u001b[0;36mGCSFileSystem._call\u001b[0;34m(self, method, path, json_out, info_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, path, \u001b[38;5;241m*\u001b[39margs, json_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, info_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    435\u001b[0m ):\n\u001b[1;32m    436\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 437\u001b[0m     status, headers, info, contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    438\u001b[0m         method, path, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    439\u001b[0m     )\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m json_out:\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(contents)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/decorator.py:221\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    220\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/retry.py:158\u001b[0m, in \u001b[0;36mretry_request\u001b[0;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    157\u001b[0m logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m non-retriable exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/retry.py:123\u001b[0m, in \u001b[0;36mretry_request\u001b[0;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mmin\u001b[39m(random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (retry \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m32\u001b[39m))\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    125\u001b[0m     HttpError,\n\u001b[1;32m    126\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m     aiohttp\u001b[38;5;241m.\u001b[39mclient_exceptions\u001b[38;5;241m.\u001b[39mClientError,\n\u001b[1;32m    130\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(e, HttpError)\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequester pays\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    135\u001b[0m     ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:421\u001b[0m, in \u001b[0;36mGCSFileSystem._request\u001b[0;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;129m@retry_request\u001b[39m(retries\u001b[38;5;241m=\u001b[39mretries)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_request\u001b[39m(\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, path, \u001b[38;5;241m*\u001b[39margs, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    414\u001b[0m ):\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_session()\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    417\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    418\u001b[0m         url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_path(path, args),\n\u001b[1;32m    419\u001b[0m         params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_params(kwargs),\n\u001b[1;32m    420\u001b[0m         json\u001b[38;5;241m=\u001b[39mjson,\n\u001b[0;32m--> 421\u001b[0m         headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    422\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    423\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequests_timeout,\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[1;32m    425\u001b[0m         status \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    426\u001b[0m         headers \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mheaders\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/core.py:400\u001b[0m, in \u001b[0;36mGCSFileSystem._get_headers\u001b[0;34m(self, headers)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m out:\n\u001b[1;32m    399\u001b[0m     out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython-gcsfs/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m version\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/credentials.py:187\u001b[0m, in \u001b[0;36mGoogleCredentials.apply\u001b[0;34m(self, out)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, out):\n\u001b[1;32m    186\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Insert credential headers in-place to a dictionary\"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_refresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials\u001b[38;5;241m.\u001b[39mapply(out)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/gcsfs/credentials.py:182\u001b[0m, in \u001b[0;36mGoogleCredentials.maybe_refresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# repeat to avoid race (but don't want lock in common case)\u001b[39;00m\n\u001b[1;32m    181\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGCS refresh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/google/auth/external_account_authorized_user.py:281\u001b[0m, in \u001b[0;36mCredentials.refresh\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRefreshError(\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe credentials do not contain the necessary fields need to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefresh the access token. You must specify refresh_token, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_url, client_id, and client_secret.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     )\n\u001b[1;32m    280\u001b[0m now \u001b[38;5;241m=\u001b[39m _helpers\u001b[38;5;241m.\u001b[39mutcnow()\n\u001b[0;32m--> 281\u001b[0m response_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_sts_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;241m=\u001b[39m response_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccess_token\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m lifetime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtimedelta(seconds\u001b[38;5;241m=\u001b[39mresponse_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpires_in\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/google/auth/external_account_authorized_user.py:292\u001b[0m, in \u001b[0;36mCredentials._make_sts_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_sts_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[0;32m--> 292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sts_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_refresh_token\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/google/oauth2/sts.py:172\u001b[0m, in \u001b[0;36mClient.refresh_token\u001b[0;34m(self, request, refresh_token)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrefresh_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, request, refresh_token):\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Exchanges a refresh token for an access token based on the\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    RFC6749 spec.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m        subject_token (str): The OAuth 2.0 refresh token.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrant_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrefresh_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrefresh_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefresh_token\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/google/oauth2/sts.py:88\u001b[0m, in \u001b[0;36mClient._make_request\u001b[0;34m(self, request, headers, request_body)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# If non-200 response received, translate to OAuthError exception.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m http_client\u001b[38;5;241m.\u001b[39mOK:\n\u001b[0;32m---> 88\u001b[0m     \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_error_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m response_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response_body)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Return successful response.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/google/oauth2/utils.py:168\u001b[0m, in \u001b[0;36mhandle_error_response\u001b[0;34m(response_body)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m    166\u001b[0m     error_details \u001b[38;5;241m=\u001b[39m response_body\n\u001b[0;32m--> 168\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mOAuthError(error_details, response_body)\n",
      "\u001b[0;31mOAuthError\u001b[0m: ('Error code invalid_grant: Refresh token has expired', '{\"error\":\"invalid_grant\",\"error_description\":\"Refresh token has expired\"}')"
     ]
    }
   ],
   "source": [
    "df = process_workflow(gcs_path, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5729254-6a88-471f-b88f-f4188185328c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"osow_vehicle_permits_v2_justlooking_0.csv\", index=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c536f943-3bc3-4848-9c6c-08ad147b977d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce80ceb-be6b-48ff-b5de-51db07ad9e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6484e-5057-49a8-b4a4-9537fc47d8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60cdcac-c67c-4d32-ae0f-47528c4658d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02.02 - second round of data processing\n",
    "\n",
    "# Find all columns starting with \"route_intersection_\"\n",
    "intersection_columns = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "\n",
    "\n",
    "def normalize_intersection(intersection_columns):\n",
    "    \"\"\"\n",
    "    Normalizes intersections by ordering highway identifiers numerically.\n",
    "    If both sides of the intersection are highways (I-, SR-, or US-),\n",
    "    ensures the smaller-numbered highway appears first.\n",
    "\n",
    "    Args:\n",
    "        intersection (str): The intersection string in the format \"Location1 and Location2\".\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized intersection string, or the original string if no changes are needed.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    if not intersection_columns or pd.isna(intersection_columns):\n",
    "        return intersection_columns\n",
    "\n",
    "    parts = [part.strip() for part in intersection_columns.split(\"and\")]\n",
    "    if len(parts) != 2:\n",
    "        return intersection_columns  # Return as-is if not exactly two parts\n",
    "\n",
    "    pattern = r\"^(I-|SR-|US-)(\\d+)$\"  # Pattern to match highway identifiers\n",
    "\n",
    "    match1 = re.match(pattern, parts[0])\n",
    "    match2 = re.match(pattern, parts[1])\n",
    "\n",
    "    if match1 and match2:\n",
    "        # Extract numeric portions and compare\n",
    "        num1 = int(match1.group(2))\n",
    "        num2 = int(match2.group(2))\n",
    "\n",
    "        if num1 > num2:\n",
    "            # Swap to ensure the smaller number comes first\n",
    "            parts = [parts[1], parts[0]]\n",
    "\n",
    "    return \" and \".join(parts)\n",
    "\n",
    "\n",
    "def process_intersections(df, intersection_columns):\n",
    "    \"\"\"\n",
    "    Processes and normalizes intersections in specified columns of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns (list): List of column names to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with normalized intersections.\n",
    "    \"\"\"\n",
    "    for col in intersection_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(normalize_intersection)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_intersections_1(df):\n",
    "    \"\"\"\n",
    "    Cleans the 'route_intersection_0' column in the DataFrame by replacing specific values.\n",
    "    \n",
    "    - Replaces values that start with \"az line\" and end with \"I-40\" with \n",
    "      \"colorado river bridge and I-10, California\".\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with cleaned intersections.\n",
    "    \"\"\"\n",
    "    # Check if the column exists\n",
    "    if \"route_intersection_0\" in df.columns:\n",
    "        # Debugging: Display the initial state of the column\n",
    "        #print(\"Before cleaning:\")\n",
    "        #print(df[\"route_intersection_0\"].head())\n",
    "        \n",
    "        # Define the replacement logic\n",
    "        def replace_intersection(value):\n",
    "            if isinstance(value, str) and value.lower().startswith(\"az line\") and value.endswith(\"I-40\"):\n",
    "                return \"colorado river bridge and I-10, California\"\n",
    "            return value\n",
    "\n",
    "        # Apply the function to clean the column\n",
    "        df[\"route_intersection_0\"] = df[\"route_intersection_0\"].apply(replace_intersection)\n",
    "        \n",
    "        # Debugging: Display the updated state of the column\n",
    "        #print(\"After cleaning:\")\n",
    "        #print(df[\"route_intersection_0\"].head())\n",
    "    else:\n",
    "        print(\"Column 'route_intersection_0' does not exist in the DataFrame.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def update_route_intersection(df):\n",
    "    # Create a new column with the same values as ['route_intersection_0']\n",
    "    df['route_intersection_origin'] = df['route_intersection_0']\n",
    "    \n",
    "    # Define replacement mappings\n",
    "    replacements = {\n",
    "        \"colorado river bridge and I-10, California\": \"I-10 and Arizona Line\"\n",
    "    }\n",
    "    \n",
    "    # Pattern-based replacements\n",
    "    pattern_replacements = [\n",
    "        (r'^az line.*and I-10$', \"I-10 and Arizona Line\"),\n",
    "        (r'^az line.*and I-8$', \"I-8 and Arizona Line\"),\n",
    "        (r'^az line.*and SR-62$', \"SR-62 and Arizona Line\"),\n",
    "        (r'^or line.*and I-5$', \"I-5 and Oregon Line\"),\n",
    "        (r'^or line.*and US-97$', \"US-97 and Oregon Line\"),\n",
    "        (r'^or line.*and US-395$', \"US-395 and Oregon Line\"),\n",
    "        (r'^or line.*and SR-139$', \"SR-139 and Oregon Line\"),\n",
    "        (r'^nv line.*and I-80$', \"I-80 and Nevada Line\"),\n",
    "        (r'^nv line.*and SR-15$', \"I-15 and Nevada Line\"),\n",
    "        (r'^nv line.*and US-6$', \"US-6 and Nevada Line\"),\n",
    "        (r'^nv line.*and US-395$', \"US-395 and Nevada Line\"),\n",
    "        (r'^nv line.*and SR-178$', \"SR-178 and Nevada Line\")\n",
    "    ]\n",
    "    \n",
    "    # Apply direct replacements\n",
    "    df['route_intersection_origin'] = df['route_intersection_origin'].replace(replacements)\n",
    "    \n",
    "    # Apply pattern-based replacements\n",
    "    for pattern, replacement in pattern_replacements:\n",
    "        df.loc[df['route_intersection_origin'].str.match(pattern, case=False, na=False), \n",
    "               'route_intersection_origin'] = replacement\n",
    "    \n",
    "    # Place the new column in the 10th position\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(10, cols.pop(cols.index('route_intersection_origin')))\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Remove the original column\n",
    "    df.drop(columns=['route_intersection_0'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def update_route_intersection_destination(df):\n",
    "    # Create a new column with the same values as ['route_intersection_destination']\n",
    "    df['route_intersection_destination_1'] = df['route_intersection_destination']\n",
    "    \n",
    "    # Define replacement mappings\n",
    "    replacements = {\n",
    "        \"I-40 and az line\": \"I-40 and Arizona Line\",\n",
    "        \"I-10 and az line\": \"I-10 and Arizona Line\",\n",
    "        \"I-8 and az line\": \"I-8 and Arizona Line\",\n",
    "        \"SR-62 and az line\": \"SR-62 and Arizona Line\",\n",
    "        \"I-80 and nv line\": \"I-80 and Nevada Line\",\n",
    "        \"I-5 and or line\": \"I-5 and Oregon Line\",\n",
    "        \"SR-15 and nv line\": \"SR-15 and Nevada Line\",\n",
    "        \"I-15 and nv line\": \"I-15 and Nevada Line\"\n",
    "    }\n",
    "    \n",
    "    # Pattern-based replacements\n",
    "    pattern_replacements = [\n",
    "        (r'I-40 and az line', \"I-40 and Arizona Line\"),\n",
    "        (r'I-10 and az line', \"I-10 and Arizona Line\"),\n",
    "        (r'I-8 and az line', \"I-8 and Arizona Line\"),\n",
    "        (r'SR-62 and az line', \"SR-62 and Arizona Line\"),\n",
    "        (r'I-80 and nv line', \"I-80 and Nevada Line\"),\n",
    "        (r'I-5 and or line', \"I-5 and Oregon Line\"),\n",
    "        (r'SR-15 and nv line', \"I-15 and Nevada Line\"),\n",
    "        (r'I-15 and nv line', \"I-15 and Nevada Line\")\n",
    "    ]\n",
    "    \n",
    "    # Apply direct replacements\n",
    "    df['route_intersection_destination_1'] = df['route_intersection_destination_1'].replace(replacements)\n",
    "    \n",
    "    # Apply pattern-based replacements\n",
    "    for pattern, replacement in pattern_replacements:\n",
    "        df.loc[df['route_intersection_destination_1'].str.contains(pattern, case=False, na=False), \n",
    "               'route_intersection_destination_1'] = replacement\n",
    "    \n",
    "    # Drop the original column\n",
    "    df.drop(columns=['route_intersection_destination'], inplace=True)\n",
    "    \n",
    "    # Rename the new column\n",
    "    df.rename(columns={'route_intersection_destination_1': 'route_intersection_destination'}, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_workflow_step2(df):\n",
    "    df = process_intersections(df, intersection_columns)\n",
    "    df = clean_intersections_1(df)\n",
    "    df = update_route_intersection(df)\n",
    "    df = update_route_intersection_destination(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2f9b4-7de9-4e2b-bf0a-2815789668c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_workflow_step2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee1c584-ceb6-40f2-b928-bf3fd55633c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"osow_vehicle_permits_v2_justlooking_1.csv\", index=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca1c54b-0a58-48ae-8ceb-a68dd273a2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ab012-c4af-4741-8383-c6bc5af04381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_intersection_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans an intersection string to keep the full road names (including 'rd', 'ave', etc.)\n",
    "    before and after 'and', while trimming any trailing addresses or city names.\n",
    "    \n",
    "    Example:\n",
    "        \"clawiter rd Hayward, California and SR-92\" → \"clawiter rd and SR-92\"\n",
    "        \"I-15 and deer springs rd 414 Smilax Rd, San Marcos, Ca\" → \"I-15 and deer springs rd\"\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text  # Return unchanged if not a string\n",
    "\n",
    "    # Split into two road parts based on \" and \"\n",
    "    parts = re.split(r'\\s+and\\s+', text, maxsplit=1, flags=re.IGNORECASE)\n",
    "    if len(parts) != 2:\n",
    "        return text  # Return unchanged if format is not expected\n",
    "\n",
    "    # Function to keep everything up to the second street suffix (if present)\n",
    "    def clean_part(part):\n",
    "        # Keep up to the first known stopword OR end if not found\n",
    "        stop_keywords = r\"(rd|road|ave|avenue|st|street|blvd|boulevard|ln|lane|dr|drive|way|pkwy|parkway|hwy|highway|skyway|line)\"\n",
    "        match = re.search(rf\"((?:\\S+\\s+)*?\\b{stop_keywords}\\b)\", part, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        else:\n",
    "            # If no keyword found, just return up to first comma or 6 words as fallback\n",
    "            return ' '.join(part.split()[:6])\n",
    "\n",
    "    road1 = clean_part(parts[0])\n",
    "    road2 = clean_part(parts[1])\n",
    "\n",
    "    return f\"{road1} and {road2}\"\n",
    "\n",
    "def clean_intersections(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies intersection cleanup to both origin and destination columns in the DataFrame.\n",
    "    Keeps road suffixes (e.g., 'rd', 'ave') and trims city/address details.\n",
    "    \"\"\"\n",
    "    df['route_intersection_origin'] = df['route_intersection_origin'].apply(clean_intersection_text)\n",
    "    df['route_intersection_destination'] = df['route_intersection_destination'].apply(clean_intersection_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0300d-9e02-4c0f-b36f-76858f2f51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_intersections(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be29e6-673c-406e-b334-5519309c9489",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"osow_vehicle_permits_v2_justlooking_2.csv\", index=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17add5a1-e6b3-453d-8cdc-b32c10455ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdf3262-dd61-4467-a744-6145ae2e4ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e77564e-f843-440d-96a3-97064557bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03.01 - adding coordinates\n",
    "\n",
    "def add_intersection_coordinates(df, origin_intersections, destination_intersections):\n",
    "    \"\"\"\n",
    "    Adds x_coords and y_coords for origin and destination intersections.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe.\n",
    "    origin_intersections (list of dict): List of origin intersection dictionaries.\n",
    "    destination_intersections (list of dict): List of destination intersection dictionaries.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Updated dataframe with added coordinate columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert lists of dictionaries into dictionaries for quick lookup, handling missing keys safely\n",
    "    origin_lookup = {\n",
    "        d.get(\"origin_intersection\", None): (d.get(\"x_coords\", None), d.get(\"y_coords\", None))\n",
    "        for d in origin_intersections if \"origin_intersection\" in d\n",
    "    }\n",
    "\n",
    "    destination_lookup = {\n",
    "        d.get(\"destination_intersection\", None): (d.get(\"x_coords\", None), d.get(\"y_coords\", None))\n",
    "        for d in destination_intersections if \"destination_intersection\" in d\n",
    "    }\n",
    "\n",
    "    #Extract origin coordinates\n",
    "    df[\"route_intersection_origin_x_coords\"] = df[\"route_intersection_origin\"].map(\n",
    "        lambda x: origin_lookup.get(x, (None, None))[0]\n",
    "    )\n",
    "    df[\"route_intersection_origin_y_coords\"] = df[\"route_intersection_origin\"].map(\n",
    "        lambda x: origin_lookup.get(x, (None, None))[1]\n",
    "    )\n",
    "\n",
    "    # Insert new columns at position 11 and 12\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(11, cols.pop(cols.index(\"route_intersection_origin_x_coords\")))\n",
    "    cols.insert(12, cols.pop(cols.index(\"route_intersection_origin_y_coords\")))\n",
    "    df = df[cols]\n",
    "\n",
    "    # Extract destination coordinates\n",
    "    df[\"route_intersection_destination_x_coords\"] = df[\"route_intersection_destination\"].map(\n",
    "        lambda x: destination_lookup.get(x, (None, None))[0]\n",
    "    )\n",
    "    df[\"route_intersection_destination_y_coords\"] = df[\"route_intersection_destination\"].map(\n",
    "        lambda x: destination_lookup.get(x, (None, None))[1]\n",
    "    )\n",
    "\n",
    "    # Move the new destination coordinate columns to be right after the last column\n",
    "    last_col_index = df.columns.get_loc(\"route_intersection_destination\") + 1\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(last_col_index, cols.pop(cols.index(\"route_intersection_destination_x_coords\")))\n",
    "    cols.insert(last_col_index + 1, cols.pop(cols.index(\"route_intersection_destination_y_coords\")))\n",
    "    df = df[cols]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_mid_route_coordinates(df):\n",
    "    \"\"\"\n",
    "    Adds 'x_coords' and 'y_coords' fields next to each mid-route intersection \n",
    "    and moves all route_intersection-related columns to the end.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe with multiple route_intersection_X fields.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Updated dataframe with coordinate fields added next to each intersection.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a lookup dictionary for SHS intersections (uppercase, removing \", California\")\n",
    "    shs_lookup = {\n",
    "        d.get(\"shs_intersection\", \"\").strip().upper().replace(\", CALIFORNIA\", \"\"): \n",
    "        (d.get(\"x_coords\", None), d.get(\"y_coords\", None))\n",
    "        for d in shs_intersections if \"shs_intersection\" in d\n",
    "    }\n",
    "\n",
    "    # Identify mid-route intersection columns (excluding origin and destination)\n",
    "    mid_route_cols = sorted(\n",
    "        [col for col in df.columns if col.startswith(\"route_intersection_\") and col.split(\"_\")[-1].isdigit()],\n",
    "        key=lambda x: int(x.split(\"_\")[-1])  # Sort numerically\n",
    "    )\n",
    "\n",
    "    # Format DataFrame values for lookup (uppercase + remove double spaces)\n",
    "    for col in mid_route_cols:\n",
    "        df[col] = df[col].astype(str).str.strip().str.upper().replace(\" ,\", \",\").replace(\"  \", \" \")\n",
    "\n",
    "    # Add new coordinate columns (convert df values to uppercase for correct lookup)\n",
    "    for col in mid_route_cols:\n",
    "        formatted_col = df[col].apply(lambda x: x.replace(\", CALIFORNIA\", \"\").strip().upper() if isinstance(x, str) else None)\n",
    "        df[f\"{col}_x_coords\"] = formatted_col.apply(lambda x: shs_lookup.get(x, (None, None))[0] if x in shs_lookup else None)\n",
    "        df[f\"{col}_y_coords\"] = formatted_col.apply(lambda x: shs_lookup.get(x, (None, None))[1] if x in shs_lookup else None)\n",
    "\n",
    "    # Define route-related columns to move to the end\n",
    "    route_cols = []\n",
    "    for col in mid_route_cols:\n",
    "        route_cols.append(col)\n",
    "        route_cols.append(f\"{col}_x_coords\")\n",
    "        route_cols.append(f\"{col}_y_coords\")\n",
    "\n",
    "    # Add origin and destination fields to the route-related list\n",
    "    route_cols = (\n",
    "        [\"route_intersection_origin\", \"route_intersection_origin_x_coords\", \"route_intersection_origin_y_coords\"]\n",
    "        + route_cols\n",
    "        + [\"route_intersection_destination\", \"route_intersection_destination_x_coords\", \"route_intersection_destination_y_coords\"]\n",
    "    )\n",
    "\n",
    "    # Identify non-route columns (to keep at the front)\n",
    "    non_route_cols = [col for col in df.columns if col not in route_cols]\n",
    "\n",
    "    # Ensure all columns exist before reordering\n",
    "    final_col_order = non_route_cols + [col for col in route_cols if col in df.columns]\n",
    "    df = df[final_col_order]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec0cc75-aa94-4b80-9eaa-7ddbdc89022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = add_intersection_coordinates(df, origin_intersections, destination_intersections)\n",
    "# df = add_mid_route_coordinates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739c9a1-dbd1-4e20-8658-a646c8fceb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"osow_vehicle_permits_v2_justlooking_3.csv\", index=False)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d0a9f-b9a1-4dad-bcaf-bb87a05d46b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a838fab3-cf92-4510-bfd3-f468b36bd9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03.02 cleaning dataframe\n",
    "\n",
    "def update_and_remove_destination_fields(df):\n",
    "    \"\"\"\n",
    "    Update the last available intersection's coordinates with the destination coordinates,\n",
    "    then remove the destination fields.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing route intersection fields.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with destination fields removed.\n",
    "    \"\"\"\n",
    "    for index, row in df.iterrows():\n",
    "        last_intersection_col = None\n",
    "\n",
    "        # Identify the last non-null intersection before the destination\n",
    "        for i in range(1, 24):  # Assuming up to 23 intermediate intersections\n",
    "            intersection_col = f'route_intersection_{i}'\n",
    "            x_col = f'route_intersection_{i}_x_coords'\n",
    "            y_col = f'route_intersection_{i}_y_coords'\n",
    "\n",
    "            if pd.notna(row.get(intersection_col)) and row[intersection_col].strip():\n",
    "                last_intersection_col = (intersection_col, x_col, y_col)\n",
    "\n",
    "        # If a last intersection exists, update its coordinates with destination coordinates\n",
    "        if last_intersection_col:\n",
    "            _, last_x_col, last_y_col = last_intersection_col\n",
    "            dest_x_col = 'route_intersection_destination_x_coords'\n",
    "            dest_y_col = 'route_intersection_destination_y_coords'\n",
    "\n",
    "            if pd.notna(row.get(dest_x_col)) and pd.notna(row.get(dest_y_col)):\n",
    "                df.at[index, last_x_col] = row[dest_x_col]\n",
    "                df.at[index, last_y_col] = row[dest_y_col]\n",
    "\n",
    "    # Drop the destination fields from the dataset\n",
    "    df = df.drop(columns=['route_intersection_destination', \n",
    "                          'route_intersection_destination_x_coords', \n",
    "                          'route_intersection_destination_y_coords'], errors='ignore')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_and_shift_intersections(df):\n",
    "    \"\"\"\n",
    "    Cleans route intersection fields by:\n",
    "    1. Removing intersections that lack x_coords and y_coords.\n",
    "    2. Shifting remaining values left to fill gaps.\n",
    "    3. Removing columns that are empty after shifting.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing route intersection fields.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    max_intersections = 23  # Max expected number of intersections\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        cleaned_intersections = []  # Store valid intersections\n",
    "\n",
    "        # Collect valid intersections (non-null x_coords and y_coords)\n",
    "        for i in range(1, max_intersections + 1):\n",
    "            name_col = f'route_intersection_{i}'\n",
    "            x_col = f'route_intersection_{i}_x_coords'\n",
    "            y_col = f'route_intersection_{i}_y_coords'\n",
    "\n",
    "            if pd.notna(row.get(name_col)) and pd.notna(row.get(x_col)) and pd.notna(row.get(y_col)):\n",
    "                cleaned_intersections.append(\n",
    "                    (row[name_col], row[x_col], row[y_col])\n",
    "                )\n",
    "\n",
    "        # Clear existing values\n",
    "        for i in range(1, max_intersections + 1):\n",
    "            df.at[index, f'route_intersection_{i}'] = None\n",
    "            df.at[index, f'route_intersection_{i}_x_coords'] = None\n",
    "            df.at[index, f'route_intersection_{i}_y_coords'] = None\n",
    "\n",
    "        # Re-populate with shifted values\n",
    "        for i, (name, x, y) in enumerate(cleaned_intersections, start=1):\n",
    "            df.at[index, f'route_intersection_{i}'] = name\n",
    "            df.at[index, f'route_intersection_{i}_x_coords'] = x\n",
    "            df.at[index, f'route_intersection_{i}_y_coords'] = y\n",
    "\n",
    "    # Drop empty intersection columns\n",
    "    for i in range(1, max_intersections + 1):\n",
    "        name_col = f'route_intersection_{i}'\n",
    "        x_col = f'route_intersection_{i}_x_coords'\n",
    "        y_col = f'route_intersection_{i}_y_coords'\n",
    "\n",
    "        if df[name_col].isna().all():  # Check if entire column is empty\n",
    "            df.drop(columns=[name_col, x_col, y_col], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82647a91-e101-4ba7-8842-073a22d3d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_and_remove_destination_fields(df)\n",
    "df = clean_and_shift_intersections(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4270728e-553f-4902-bbd1-229570a6516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"osow_vehicle_permits_v2_justlooking_4.csv\", index=False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cca31b-bb2c-40ef-a734-630df124ff5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0ada25-3cda-4050-b291-33db6714d56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15393748-e123-4921-9548-d79584cc2851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e89161-c650-4b97-a198-c52a2d233780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
