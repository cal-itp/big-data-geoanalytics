{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2d5a2c-4b02-44f3-af2d-0febddc5fbc6",
   "metadata": {},
   "source": [
    "# Oversized Overweight Vehicle Permit Route Parsing Concept Validation  \n",
    "* parsing the ['authorizedhighways'] column in the all_permits data for OSOW vehicle permits\n",
    "\n",
    "- Developed by the Caltrans Data and Digital Services Office of Big Data and GeoAnalytics\n",
    "\n",
    "- Originally requested by Stephen Yoon  \n",
    "    - Original data provided by Stephen's office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acace08e-1139-4d26-876f-0101415d1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import gcsfs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afda019a-e695-4f89-9464-cf9be44c4b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in the coordinates from the utils docs\n",
    "from osow_frp_o_d_utils_v3 import origin_intersections, destination_intersections\n",
    "from shs_intersections_utils import shs_intersections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e08b8f81-5d63-4b8f-a1b1-c9ab9565f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original_mapping is needed to standardize the highway names for the various records \n",
    "original_mapping = {\n",
    "    \"5\": \"I-5\", \"10\": \"I-10\", \"15\": \"I-15\", \"8\": \"I-8\", \"40\": \"I-40\", \"80\": \"I-80\", \"105\": \"I-105\", \"110\": \"I-110\",\n",
    "    \"205\": \"I-205\", \"210\": \"I-210\", \"215\": \"I-215\", \"280\": \"I-280\", \"380\": \"I-380\", \"405\": \"I-405\",\n",
    "    \"505\": \"I-505\", \"580\": \"I-580\", \"605\": \"I-605\", \"680\": \"I-680\", \"710\": \"I-710\", \"805\": \"I-805\",\n",
    "    \"880\": \"I-880\", \"980\": \"I-980\", \"1\": \"SR-1\", \"2\": \"SR-2\", \"3\": \"SR-3\", \"4\": \"SR-4\", \"7\": \"SR-7\",\n",
    "    \"9\": \"SR-9\", \"11\": \"SR-11\", \"12\": \"SR-12\", \"13\": \"SR-13\", \"14\": \"SR-14\", \n",
    "    #\"15\": \"SR-15\",\n",
    "    \"16\": \"SR-16\", \"17\": \"SR-17\", \"18\": \"SR-18\", \"20\": \"SR-20\", \"22\": \"SR-22\", \"23\": \"SR-23\",\n",
    "    \"24\": \"SR-24\", \"25\": \"SR-25\", \"26\": \"SR-26\", \"27\": \"SR-27\", \"28\": \"SR-28\", \"29\": \"SR-29\",\n",
    "    \"32\": \"SR-32\", \"33\": \"SR-33\", \"34\": \"SR-34\", \"35\": \"SR-35\", \"36\": \"SR-36\", \"37\": \"SR-37\",\n",
    "    \"38\": \"SR-38\", \"39\": \"SR-39\", \"41\": \"SR-41\", \"43\": \"SR-43\", \"44\": \"SR-44\", \"45\": \"SR-45\",\n",
    "    \"46\": \"SR-46\", \"seaside highway\": \"SR-47\", \"47\": \"SR-47\", \"49\": \"SR-49\", \"51\": \"SR-51\", \"52\": \"SR-52\", \"53\": \"SR-53\",\n",
    "    \"54\": \"SR-54\", \"55\": \"SR-55\", \"56\": \"SR-56\", \"57\": \"SR-57\", \"58\": \"SR-58\", \"59\": \"SR-59\",\n",
    "    \"60\": \"SR-60\", \"61\": \"SR-61\", \"62\": \"SR-62\", \"63\": \"SR-63\", \"65\": \"SR-65\", \"66\": \"SR-66\",\n",
    "    \"67\": \"SR-67\", \"68\": \"SR-68\", \"70\": \"SR-70\", \"71\": \"SR-71\", \"72\": \"SR-72\", \"73\": \"SR-73\",\n",
    "    \"74\": \"SR-74\", \"75\": \"SR-75\", \"76\": \"SR-76\", \"77\": \"SR-77\", \"78\": \"SR-78\", \"79\": \"SR-79\",\n",
    "    \"82\": \"SR-82\", \"83\": \"SR-83\", \"84\": \"SR-84\", \"85\": \"SR-85\", \"86\": \"SR-86\", \"87\": \"SR-87\",\n",
    "    \"88\": \"SR-88\", \"89\": \"SR-89\", \"90\": \"SR-90\", \"91\": \"SR-91\", \"92\": \"SR-92\", \"94\": \"SR-94\",\n",
    "    \"96\": \"SR-96\", \"98\": \"SR-98\", \"99\": \"SR-99\", \"103\": \"SR-103\", \"104\": \"SR-104\", \"107\": \"SR-107\",\n",
    "    \"108\": \"SR-108\", \"109\": \"SR-109\", \"110\": \"SR-110\", \"111\": \"SR-111\", \"112\": \"SR-112\",\n",
    "    \"113\": \"SR-113\", \"114\": \"SR-114\", \"115\": \"SR-115\", \"116\": \"SR-116\", \"118\": \"SR-118\",\n",
    "    \"119\": \"SR-119\", \"120\": \"SR-120\", \"121\": \"SR-121\", \"123\": \"SR-123\", \"124\": \"SR-124\",\n",
    "    \"125\": \"SR-125\", \"126\": \"SR-126\", \"127\": \"SR-127\", \"128\": \"SR-128\", \"129\": \"SR-129\",\n",
    "    \"130\": \"SR-130\", \"131\": \"SR-131\", \"132\": \"SR-132\", \"133\": \"SR-133\", \"134\": \"SR-134\",\n",
    "    \"135\": \"SR-135\", \"136\": \"SR-136\", \"137\": \"SR-137\", \"138\": \"SR-138\", \"139\": \"SR-139\",\n",
    "    \"140\": \"SR-140\", \"142\": \"SR-142\", \"144\": \"SR-144\", \"145\": \"SR-145\", \"146\": \"SR-146\",\n",
    "    \"147\": \"SR-147\", \"149\": \"SR-149\", \"150\": \"SR-150\", \"151\": \"SR-151\", \"152\": \"SR-152\",\n",
    "    \"153\": \"SR-153\", \"154\": \"SR-154\", \"155\": \"SR-155\", \"156\": \"SR-156\", \"158\": \"SR-158\",\n",
    "    \"160\": \"SR-160\", \"161\": \"SR-161\", \"162\": \"SR-162\", \"163\": \"SR-163\", \"164\": \"SR-164\",\n",
    "    \"165\": \"SR-165\", \"166\": \"SR-166\", \"167\": \"SR-167\", \"168\": \"SR-168\", \"169\": \"SR-169\",\n",
    "    \"170\": \"SR-170\", \"172\": \"SR-172\", \"173\": \"SR-173\", \"174\": \"SR-174\", \"175\": \"SR-175\",\n",
    "    \"177\": \"SR-177\", \"178\": \"SR-178\", \"180\": \"SR-180\", \"182\": \"SR-182\", \"183\": \"SR-183\",\n",
    "    \"184\": \"SR-184\", \"185\": \"SR-185\", \"186\": \"SR-186\", \"187\": \"SR-187\", \"188\": \"SR-188\",\n",
    "    \"189\": \"SR-189\", \"190\": \"SR-190\", \"191\": \"SR-191\", \"192\": \"SR-192\", \"193\": \"SR-193\",\n",
    "    \"197\": \"SR-197\", \"198\": \"SR-198\", \"200\": \"SR-200\", \"201\": \"SR-201\", \"202\": \"SR-202\",\n",
    "    \"203\": \"SR-203\", \"204\": \"SR-204\", \"207\": \"SR-207\", \"210\": \"SR-210\", \"211\": \"SR-211\",\n",
    "    \"213\": \"SR-213\", \"216\": \"SR-216\", \"217\": \"SR-217\", \"218\": \"SR-218\", \"219\": \"SR-219\",\n",
    "    \"220\": \"SR-220\", \"221\": \"SR-221\", \"222\": \"SR-222\", \"223\": \"SR-223\", \"227\": \"SR-227\",\n",
    "    \"229\": \"SR-229\", \"232\": \"SR-232\", \"233\": \"SR-233\", \"236\": \"SR-236\", \"237\": \"SR-237\",\n",
    "    \"238\": \"SR-238\", \"241\": \"SR-241\", \"242\": \"SR-242\", \"243\": \"SR-243\", \"244\": \"SR-244\",\n",
    "    \"245\": \"SR-245\", \"246\": \"SR-246\", \"247\": \"SR-247\", \"253\": \"SR-253\", \"254\": \"SR-254\",\n",
    "    \"255\": \"SR-255\", \"259\": \"SR-259\", \"260\": \"SR-260\", \"261\": \"SR-261\", \"262\": \"SR-262\",\n",
    "    \"263\": \"SR-263\", \"265\": \"SR-265\", \"266\": \"SR-266\", \"267\": \"SR-267\", \"269\": \"SR-269\",\n",
    "    \"270\": \"SR-270\", \"271\": \"SR-271\", \"273\": \"SR-273\", \"275\": \"SR-275\", \"281\": \"SR-281\",\n",
    "    \"282\": \"SR-282\", \"283\": \"SR-283\", \"284\": \"SR-284\", \"299\": \"SR-299\", \"330\": \"SR-330\",\n",
    "    \"371\": \"SR-371\", \"780\": \"SR-780\", \"905\": \"SR-905\", \"6\": \"US-6\", \"50\": \"US-50\",\n",
    "    \"95\": \"US-95\", \"97\": \"US-97\", \"101\": \"US-101\", \"199\": \"US-199\", \"395\": \"US-395\"\n",
    "}\n",
    "\n",
    "# Generate extended mapping to include leading zeros\n",
    "road_mapping = {}\n",
    "for key, value in original_mapping.items():\n",
    "    road_mapping[key] = value  # Original\n",
    "    road_mapping[key.zfill(2)] = value  # 2-character zero-padded\n",
    "    road_mapping[key.zfill(3)] = value  # 3-character zero-padded\n",
    "\n",
    "#print(road_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0c73d-5a2b-457c-b7da-0fa2c918d45b",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482262aa-fbc6-478a-91ad-71ca26c8bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Excel files are stored in the Caltrans Google Cloud Storage GCS bucket \"calitp-analytics-data\"\n",
    "    # the Bucket file path is Buckets > calitp-analytics-data > data-analysis > big_data > freight > all_permits\n",
    "gcs_path = \"gs://calitp-analytics-data/data-analyses/big_data/freight/all_permits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94dc7a2c-e5db-4a60-88b0-5c5aa4961dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the datasets to be analyzed\n",
    "# the datasets are located in google cloud services or gcs\n",
    "file_names = [\"all_permits_2023_sampleset.xlsx\",\n",
    "              \"all_permits_2024_sampleset.xlsx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0291126b-d65f-4316-8046-e06aab1038d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the excel sheets stored in the GCS bucket and identified by name in the file_names list\n",
    "def load_excel_sheets_1(gcs_path, file_names):\n",
    "    \"\"\"\n",
    "    Pull in the first sheet from each Excel file in GCS, add a 'year' column based on the filename,\n",
    "    and remove records with NaN values in the 'permitnumber' column. Returns a concatenated DataFrame\n",
    "    with data from all files.\n",
    "\n",
    "    Parameters:\n",
    "    gcs_path (str): The Google Cloud Storage path where the files are located.\n",
    "    file_names (list): A list of Excel file names in the GCS path.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A single concatenated DataFrame with data from all files, a 'year' column, and\n",
    "                  records with NaN values in 'permitnumber' removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a Google Cloud Storage file system object\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    \n",
    "    # List to store all DataFrames\n",
    "    df_list = []\n",
    "    \n",
    "    # Suppress any warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Suppress the specific UserWarning\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        message=\"Your application has authenticated using end user credentials from Google Cloud SDK without a quota project.\",\n",
    "        category=UserWarning,\n",
    "        module=\"google.auth._default\"\n",
    "    )\n",
    "    \n",
    "    # Define the columns to keep\n",
    "    columns_to_keep = ['permitnumber', 'year', 'permitvalidfrom', 'permitvalidto', \n",
    "                       'loaddescription', 'origin', 'destination', 'authorizedhighways']\n",
    "    \n",
    "    # Loop through each file in the file list\n",
    "    for file in file_names:\n",
    "        # Extract the year from the filename\n",
    "        year = file.split('_')[2]  # Assuming the year is the third element when split by '_'\n",
    "        \n",
    "        # Open the file and read only the first sheet\n",
    "        with fs.open(f\"{gcs_path}{file}\", 'rb') as f:\n",
    "            df = pd.read_excel(f, sheet_name=0)  # Load only the first sheet\n",
    "        \n",
    "        # Clean headers by removing spaces and making characters lowercase\n",
    "        df.columns = [col.replace(\" \", \"\").lower() for col in df.columns]\n",
    "        \n",
    "        # Add 'year' column\n",
    "        df['year'] = year\n",
    "        \n",
    "        # Filter columns and remove rows with NaN in 'permitnumber'\n",
    "        df = df[columns_to_keep].dropna(subset=['permitnumber'])\n",
    "        \n",
    "        # Append to list\n",
    "        df_list.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61733eb-1b30-4f44-901d-f5d9f39668bf",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "### identifying the locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f234ea9-0098-47df-a456-01e9d585078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to decode embedded tokens like \"morn10w\" -> \"I-10\"\n",
    "def decode_hybrid_token(segment):\n",
    "    # Only decode compact alphanumeric strings\n",
    "    if not re.match(r'^[a-z]*\\d{1,3}[a-z]*$', segment, re.IGNORECASE):\n",
    "        return segment\n",
    "\n",
    "    match = re.search(r'([a-z]*)(\\d{1,3})([a-z]*)', segment, re.IGNORECASE)\n",
    "    if match:\n",
    "        number = match.group(2)\n",
    "        if number in {\"5\", \"10\", \"15\", \"80\", \"405\", \"110\"}:  # expand as needed\n",
    "            return f\"I-{number}\"\n",
    "        else:\n",
    "            return number\n",
    "    return segment\n",
    "\n",
    "\n",
    "# Updated parsing function\n",
    "def parse_routes(route_info):\n",
    "    segments = []\n",
    "\n",
    "    if not isinstance(route_info, str):\n",
    "        return segments\n",
    "\n",
    "    # Normalize input\n",
    "    route_info = route_info.lower().strip(\"* \").strip()\n",
    "\n",
    "    # Split into rough chunks\n",
    "    raw_segments = re.split(r'\\s*-\\s*from\\s+|\\s*-\\s*to\\s+|(?<!\\s)-\\s*', route_info)\n",
    "\n",
    "    for segment in raw_segments:\n",
    "        sub_segments = re.split(r'\\s*-\\s*|\\s*–\\s*', segment)\n",
    "        for sub in sub_segments:\n",
    "            cleaned = sub.strip()\n",
    "            if not cleaned:\n",
    "                continue\n",
    "\n",
    "            # Only decode if segment is a compact pattern (like morn10w)\n",
    "            if re.match(r'^[a-z]{3,}\\d{1,3}[a-z]*$', cleaned):\n",
    "                cleaned = decode_hybrid_token(cleaned)\n",
    "\n",
    "            segments.append(cleaned)\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "# Custom parsing function\n",
    "def extract_location(text):\n",
    "    # Stop keywords pattern\n",
    "    stop_keywords = r\"\\b(?:dr|drive|rd|ave|way|pkwy|parkway|skyway|road|avenue|blvd|boulevard|st|street|line|lane|ln|hwy|highway)\\b\"\n",
    "\n",
    "    # If 'from' exists, process it as before\n",
    "    if \"from\" in text.lower():\n",
    "        match = re.search(r\"from\\s+(`.*?`|'.*?'|\\w+(?:\\s+\\w+)*)\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            location = match.group(1)  # Extract the text after \"from\"\n",
    "            # Keep the stop keywords and remove everything after them\n",
    "            location = re.sub(r\"(\" + stop_keywords + r\").*\", r\"\\1\", location, flags=re.IGNORECASE).strip()\n",
    "            return location\n",
    "    else:\n",
    "        # If 'from' doesn't exist, look for a stop keyword and capture location\n",
    "        match = re.search(r\"(`.*?`|'.*?'|\\w+(?:\\s+\\w+)*)\\s+(\" + stop_keywords + r\")\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            location = match.group(1)  # Capture location before stop keyword\n",
    "            return location.strip()\n",
    "\n",
    "    return None  # If no match is found\n",
    "\n",
    "\n",
    "def clean_route(route):\n",
    "    if not isinstance(route, str):\n",
    "        return route\n",
    "\n",
    "    # If it looks like a highway number, clean it\n",
    "    numeric_match = re.match(r\"(?:rte|route)?\\s*(\\d+)\", route, flags=re.IGNORECASE)\n",
    "    if numeric_match:\n",
    "        return numeric_match.group(1)\n",
    "\n",
    "    # Otherwise, just return the string stripped — no aggressive trimming\n",
    "    return route.strip()\n",
    "\n",
    "\n",
    "# A function to parse the ['authorizedhighways'] column to get the route information\n",
    "def process_route_locations(df, parse_routes, extract_location, clean_route, road_mapping):\n",
    "    \"\"\"\n",
    "    Process a dataframe to format and clean route-related columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input dataframe to process.\n",
    "    parse_routes (function): Function to parse the 'authorizedhighways' column into route segments.\n",
    "    extract_location (function): Function to extract location from a route segment.\n",
    "    clean_route (function): Function to clean individual route location entries.\n",
    "    road_mapping (dict): Mapping dictionary for road numbers to their corresponding classes.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The processed dataframe with updated route locations and columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the authorized highways field so the text is not all capitalized\n",
    "    df['authorizedhighways'] = df['authorizedhighways'].str.capitalize()\n",
    "\n",
    "    # Apply the parsing function to create lists of individual route locations\n",
    "    df['route_segments'] = df['authorizedhighways'].apply(parse_routes)\n",
    "\n",
    "    # Determine the maximum number of locations to create the necessary columns\n",
    "    max_locations = df['route_segments'].apply(len).max()\n",
    "\n",
    "    # Create new columns for each route location based on the maximum number of locations\n",
    "    for i in range(max_locations):\n",
    "        df[f'route_location_{i}'] = df['route_segments'].apply(lambda x: x[i] if i < len(x) else None)\n",
    "\n",
    "    # Drop the temporary route_segments column\n",
    "    df.drop(columns=['route_segments'], inplace=True)\n",
    "\n",
    "    # Add a new column with all values set to \"California\"\n",
    "    df.insert(5, \"state\", \"California\")  # Index 5 corresponds to the 6th column position\n",
    "\n",
    "    # Apply title case to the 'origin' and 'destination' columns\n",
    "    df['origin'] = df['origin'].str.title()\n",
    "    df['destination'] = df['destination'].str.title()\n",
    "\n",
    "    # Apply the extract_location function to the column\n",
    "    df[\"route_location_start\"] = df[\"route_location_0\"].apply(extract_location)\n",
    "\n",
    "    # Insert the route_location_start column into the 8th position\n",
    "    df.insert(8, \"route_location_start\", df.pop(\"route_location_start\"))\n",
    "\n",
    "    # Drop the [authorizedhighways] column\n",
    "    #df.drop(columns=['authorizedhighways'], inplace=True)\n",
    "\n",
    "    # Drop the route_location_0 field\n",
    "    df = df.drop(columns=['route_location_0'])\n",
    "\n",
    "    # Identify target columns excluding \"route_location_start\"\n",
    "    route_columns = [col for col in df.columns if col.startswith(\"route_location_\") and col != \"route_location_start\"]\n",
    "\n",
    "    # Apply the clean_route cleaning function to the target columns (columns that begin with the words \"route_location\")\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].apply(clean_route)\n",
    "\n",
    "    # Iterate through each \"route_location_\" column to remove the word \"exit\"\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].apply(lambda x: str(x).replace(\"exit\", \"\").strip() if isinstance(x, str) else x)\n",
    "\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Update the road numbers to their corresponding road class numbers\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].apply(lambda x: road_mapping.get(x.strip(), x) if x.strip().isdigit() else x)\n",
    "    \n",
    "    # Create a new field called 'route_location_origin' that identifies the street and city/state\n",
    "    df['route_location_origin'] = df['route_location_start'] + \" \" + df['origin'] + \", \" + df['state']\n",
    "    \n",
    "    # Move the new column (['route_location_origin_0']) to the 9th position\n",
    "    columns = list(df.columns)\n",
    "    columns.insert(9, columns.pop(columns.index('route_location_origin')))\n",
    "    df = df[columns]\n",
    "    \n",
    "    # Remove the 'route_location_destination_city' column if it exists\n",
    "    if 'route_location_start' in df.columns:\n",
    "        df = df.drop(columns=['route_location_start'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d06db3-d3fe-480a-ba00-e6de1c1989ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_workflow_locations(gcs_path, file_names):\n",
    "    # Load the Excel sheets from GCS and get the initial DataFrame\n",
    "    df = load_excel_sheets_1(gcs_path, file_names)\n",
    "    \n",
    "    # Process route locations\n",
    "    df = process_route_locations(df, parse_routes, extract_location, clean_route, road_mapping)\n",
    "       \n",
    "    \n",
    "    \n",
    "    # # Replace the last non-null route intersection with destination\n",
    "    # df = replace_last_non_null_intersection(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8065145-d7e5-4693-9be8-e62a35283f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_workflow_locations(gcs_path, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a135e3cb-6a7e-460a-bd11-7c2485e20c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2399, 34)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# was (2399, 34)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0abd501-8bda-4181-8210-30919920af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing\n",
    "df.to_csv(\"osow_locations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320cf39a-66bd-4d55-8b53-18155259f8fd",
   "metadata": {},
   "source": [
    "### Create intersections from the locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7174457b-28cb-4698-9a79-8dce4c9c36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to create the ['route_intersection_x'] columns\n",
    "def process_route_intersections(df):\n",
    "    \"\"\"\n",
    "    Process a dataframe to identify and process route location and intersection columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe to process.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A cleaned dataframe with processed route intersections.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify all columns with \"route_location_\" prefix\n",
    "    route_location_cols = [col for col in df.columns if col.startswith(\"route_location_\")]\n",
    "\n",
    "    # Initialize a counter for the new intersection column names\n",
    "    intersection_counter = 0\n",
    "\n",
    "    # Create new columns for intersections\n",
    "    for i in range(len(route_location_cols) - 1):\n",
    "        col1 = route_location_cols[i]\n",
    "        col2 = route_location_cols[i + 1]\n",
    "\n",
    "        # Name the new intersection column based on the counter\n",
    "        intersection_col = f\"route_intersection_{intersection_counter}\"\n",
    "\n",
    "        # Combine adjacent columns into one field (handle None gracefully)\n",
    "        df[intersection_col] = df[col1].astype(str) + \" and \" + df[col2].astype(str)\n",
    "        df[intersection_col] = df[intersection_col].replace(\"None and None\", None)  # Optional cleanup for all-None rows\n",
    "        intersection_counter += 1\n",
    "\n",
    "    # Identify columns that start with \"route_intersection_\"\n",
    "    intersection_cols = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "\n",
    "    # Iterate over each intersection column\n",
    "    for col in intersection_cols:\n",
    "        # Replace values ending with \" & None\" with None (Null)\n",
    "        df[col] = df[col].apply(lambda x: None if isinstance(x, str) and x.endswith(\" and None\") else x)\n",
    "\n",
    "    # Identify columns that start with \"route_intersection_\"\n",
    "    intersection_cols = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "\n",
    "    # Iterate over each intersection column to remove leading zeros from numeric values\n",
    "    for col in intersection_cols:\n",
    "        # Apply the transformation to each value in the column\n",
    "        df[col] = df[col].apply(lambda x: ' and '.join([part.lstrip('0') if part.isdigit() else part for part in str(x).split(' and ')]) if isinstance(x, str) else x)\n",
    "\n",
    "    # Create a list(?) called core_columns to be included in the next iteration of the dataframe\n",
    "    core_columns = [\n",
    "        \"permitnumber\", \"year\", \"permitvalidfrom\", \"permitvalidto\",\n",
    "        \"loaddescription\", \"state\", \"origin\", \"destination\", \"authorizedhighways\", \"route_location_origin\"\n",
    "    ]\n",
    "\n",
    "    # subset_columns combines the core columns with the intersection_cols identified earlier in the script\n",
    "    subset_columns = core_columns + intersection_cols\n",
    "\n",
    "    # this next line utilizes the defined subset_columns to create a cleaned up version of the dataframe including only the columns needed for this analysis \n",
    "    df = df[subset_columns]\n",
    "\n",
    "    # Replace None values in intersection_cols with empty strings\n",
    "    for col in intersection_cols:\n",
    "        df[col] = df[col].apply(lambda x: \"\" if x is None else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to get the last 'route_intersection_x' field\n",
    "def get_last_intersection(row):\n",
    "    # Identify columns that match the pattern 'route_intersection_x'\n",
    "    intersection_columns = [col for col in df.columns if col.startswith('route_intersection_')]\n",
    "    # Get the last non-null value among these columns\n",
    "    return row[intersection_columns].dropna().iloc[-1] if intersection_columns else None\n",
    "\n",
    "def add_route_location_destination_city(df):\n",
    "    # Function to get the last 'route_intersection_x' value\n",
    "    def get_last_intersection(row):\n",
    "        # Identify columns that match the pattern 'route_intersection_x'\n",
    "        intersection_columns = [col for col in df.columns if col.startswith('route_intersection_')]\n",
    "        # Get the last non-null value among these columns\n",
    "        return row[intersection_columns].dropna().iloc[-1] if len(intersection_columns) > 0 else None\n",
    "\n",
    "    # Create the new column\n",
    "    df['route_location_destination_city'] = df.apply(\n",
    "        lambda row: f\"{get_last_intersection(row)} {row['destination']}, {row['state']}\", axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_route_intersection_last(df):\n",
    "    \"\"\"\n",
    "    Create a 'route_intersection_last' column to capture the last non-null value\n",
    "    from all 'route_intersection_x' columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe to process.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The dataframe with the new 'route_intersection_last' column.\n",
    "    \"\"\"\n",
    "    # Identify all 'route_intersection_x' columns\n",
    "    intersection_columns = [col for col in df.columns if col.startswith('route_intersection_')]\n",
    "\n",
    "    if not intersection_columns:\n",
    "        raise ValueError(\"No 'route_intersection_' columns found in the dataframe.\")\n",
    "\n",
    "    # Ensure the columns are processed in order\n",
    "    intersection_columns = sorted(intersection_columns, key=lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "    # Create 'route_intersection_last' by finding the last non-null value row-wise\n",
    "    df['route_intersection_last'] = df[intersection_columns].apply(\n",
    "        lambda row: next((val for val in reversed(row) if pd.notnull(val) and val != ''), None), axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_route_intersection_destination(df):\n",
    "    \"\"\"\n",
    "    Create a new column 'route_intersection_destination' by combining 'route_intersection_last' \n",
    "    and 'route_location_destination_city'. Then, clean up the column by removing extra whitespace.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input dataframe to process.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The dataframe with the new 'route_intersection_destination' column and cleaned columns.\n",
    "    \"\"\"\n",
    "    # Create the new 'route_intersection_destination' column\n",
    "    df['route_intersection_destination'] = df['route_intersection_last'] + \" \" + df['route_location_destination_city']\n",
    "    \n",
    "    # Remove extra spaces by stripping and ensuring only single spaces exist\n",
    "    df['route_intersection_destination'] = df['route_intersection_destination'].apply(\n",
    "        lambda x: \" \".join(x.split()) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "    # Drop unnecessary columns if they exist\n",
    "    df.drop(columns=['route_location_destination_city', 'route_intersection_last'], errors='ignore', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# A function to clean \"route_intersection\" columns\n",
    "def clean_route_intersections(df):\n",
    "    # Find all columns starting with \"route_intersection_\"\n",
    "    intersection_columns = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "    \n",
    "    # Replace variations of \"imperial highway\" (e.g., \"imperial hwy\") with \"SR-99\"\n",
    "    for col in intersection_columns:\n",
    "        df[col] = df[col].str.replace(\n",
    "            r\"(?i)\\bimperial (highway|hwy)\\b\", \"SR-99\", regex=True\n",
    "        )  # Matches \"imperial highway\" or \"imperial hwy\"\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89091985-be1e-47b2-8ff0-98c28bb3266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_workflow_intersections(df):       \n",
    "    # Process route intersections\n",
    "    df = process_route_intersections(df)\n",
    "    \n",
    "    # Add route location destination city\n",
    "    df = add_route_location_destination_city(df)\n",
    "    \n",
    "    # Create route intersection last\n",
    "    df = create_route_intersection_last(df)\n",
    "    \n",
    "    # Create route intersection destination\n",
    "    df = create_route_intersection_destination(df)\n",
    "\n",
    "    # this is a new script - intended to help clean up the [\"route_locations_x\"] before they become intersections\n",
    "    df = clean_route_intersections(df)\n",
    "    \n",
    "    \n",
    "    # # Replace the last non-null route intersection with destination\n",
    "    # df = replace_last_non_null_intersection(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "386a33a9-2319-459c-bd32-064ef8c29ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_workflow_intersections(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddfd2372-9342-4d71-9f8f-7eb75740633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing\n",
    "df.to_csv(\"osow_intersections.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3afdfa7-3325-484a-b1f5-f13316136102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2399, 35)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb0bbed5-fc32-4f91-ad9d-0a2c8521f28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all columns starting with \"route_intersection_\"\n",
    "intersection_columns = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "\n",
    "\n",
    "def normalize_intersection(intersection_columns):\n",
    "    \"\"\"\n",
    "    Normalizes intersections by ordering highway identifiers numerically.\n",
    "    If both sides of the intersection are highways (I-, SR-, or US-),\n",
    "    ensures the smaller-numbered highway appears first.\n",
    "\n",
    "    Args:\n",
    "        intersection (str): The intersection string in the format \"Location1 and Location2\".\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized intersection string, or the original string if no changes are needed.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    if not intersection_columns or pd.isna(intersection_columns):\n",
    "        return intersection_columns\n",
    "\n",
    "    parts = [part.strip() for part in intersection_columns.split(\"and\")]\n",
    "    if len(parts) != 2:\n",
    "        return intersection_columns  # Return as-is if not exactly two parts\n",
    "\n",
    "    pattern = r\"^(I-|SR-|US-)(\\d+)$\"  # Pattern to match highway identifiers\n",
    "\n",
    "    match1 = re.match(pattern, parts[0])\n",
    "    match2 = re.match(pattern, parts[1])\n",
    "\n",
    "    if match1 and match2:\n",
    "        # Extract numeric portions and compare\n",
    "        num1 = int(match1.group(2))\n",
    "        num2 = int(match2.group(2))\n",
    "\n",
    "        if num1 > num2:\n",
    "            # Swap to ensure the smaller number comes first\n",
    "            parts = [parts[1], parts[0]]\n",
    "\n",
    "    return \" and \".join(parts)\n",
    "\n",
    "\n",
    "def process_intersections(df, intersection_columns):\n",
    "    \"\"\"\n",
    "    Processes and normalizes intersections in specified columns of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns (list): List of column names to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with normalized intersections.\n",
    "    \"\"\"\n",
    "    for col in intersection_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(normalize_intersection)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc292b21-3a5d-4516-ac54-3656e0707230",
   "metadata": {},
   "source": [
    "### Clean Intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47cdd6b2-59b8-456b-a6fb-6ee15c681819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_intersections_1(df):\n",
    "    \"\"\"\n",
    "    Cleans the 'route_intersection_0' column in the DataFrame by replacing specific values.\n",
    "    \n",
    "    - Replaces values that start with \"az line\" and end with \"I-40\" with \n",
    "      \"colorado river bridge and I-10, California\".\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with cleaned intersections.\n",
    "    \"\"\"\n",
    "    # Check if the column exists\n",
    "    if \"route_intersection_0\" in df.columns:\n",
    "        # Debugging: Display the initial state of the column\n",
    "        #print(\"Before cleaning:\")\n",
    "        #print(df[\"route_intersection_0\"].head())\n",
    "        \n",
    "        # Define the replacement logic\n",
    "        def replace_intersection(value):\n",
    "            if isinstance(value, str) and value.lower().startswith(\"az line\") and value.endswith(\"I-40\"):\n",
    "                return \"colorado river bridge and I-10, California\"\n",
    "            return value\n",
    "\n",
    "        # Apply the function to clean the column\n",
    "        df[\"route_intersection_0\"] = df[\"route_intersection_0\"].apply(replace_intersection)\n",
    "        \n",
    "        # Debugging: Display the updated state of the column\n",
    "        #print(\"After cleaning:\")\n",
    "        #print(df[\"route_intersection_0\"].head())\n",
    "    else:\n",
    "        print(\"Column 'route_intersection_0' does not exist in the DataFrame.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def update_route_intersection(df):\n",
    "    # Create a new column with the same values as ['route_intersection_0']\n",
    "    df['route_intersection_origin'] = df['route_intersection_0']\n",
    "    \n",
    "    # Define replacement mappings\n",
    "    replacements = {\n",
    "        \"colorado river bridge and I-10, California\": \"I-10 and Arizona Line\"\n",
    "    }\n",
    "    \n",
    "    # Pattern-based replacements\n",
    "    pattern_replacements = [\n",
    "        (r'^az line.*and I-10$', \"I-10 and Arizona Line\"),\n",
    "        (r'^az line.*and I-8$', \"I-8 and Arizona Line\"),\n",
    "        (r'^az line.*and SR-62$', \"SR-62 and Arizona Line\"),\n",
    "        (r'^or line.*and I-5$', \"I-5 and Oregon Line\"),\n",
    "        (r'^or line.*and US-97$', \"US-97 and Oregon Line\"),\n",
    "        (r'^or line.*and US-395$', \"US-395 and Oregon Line\"),\n",
    "        (r'^or line.*and SR-139$', \"SR-139 and Oregon Line\"),\n",
    "        (r'^nv line.*and I-80$', \"I-80 and Nevada Line\"),\n",
    "        (r'^nv line.*and SR-15$', \"I-15 and Nevada Line\"),\n",
    "        (r'^nv line.*and US-6$', \"US-6 and Nevada Line\"),\n",
    "        (r'^nv line.*and US-395$', \"US-395 and Nevada Line\"),\n",
    "        (r'^nv line.*and SR-178$', \"SR-178 and Nevada Line\")\n",
    "    ]\n",
    "    \n",
    "    # Apply direct replacements\n",
    "    df['route_intersection_origin'] = df['route_intersection_origin'].replace(replacements)\n",
    "    \n",
    "    # Apply pattern-based replacements\n",
    "    for pattern, replacement in pattern_replacements:\n",
    "        df.loc[df['route_intersection_origin'].str.match(pattern, case=False, na=False), \n",
    "               'route_intersection_origin'] = replacement\n",
    "    \n",
    "    # Place the new column in the 10th position\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(10, cols.pop(cols.index('route_intersection_origin')))\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Remove the original column\n",
    "    df.drop(columns=['route_intersection_0'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def update_route_intersection_destination(df):\n",
    "    # Create a new column with the same values as ['route_intersection_destination']\n",
    "    df['route_intersection_destination_1'] = df['route_intersection_destination']\n",
    "    \n",
    "    # Define replacement mappings\n",
    "    replacements = {\n",
    "        \"I-40 and az line\": \"I-40 and Arizona Line\",\n",
    "        \"I-10 and az line\": \"I-10 and Arizona Line\",\n",
    "        \"I-8 and az line\": \"I-8 and Arizona Line\",\n",
    "        \"SR-62 and az line\": \"SR-62 and Arizona Line\",\n",
    "        \"I-80 and nv line\": \"I-80 and Nevada Line\",\n",
    "        \"I-5 and or line\": \"I-5 and Oregon Line\",\n",
    "        \"SR-15 and nv line\": \"SR-15 and Nevada Line\",\n",
    "        \"I-15 and nv line\": \"I-15 and Nevada Line\"\n",
    "    }\n",
    "    \n",
    "    # Pattern-based replacements\n",
    "    pattern_replacements = [\n",
    "        (r'I-40 and az line', \"I-40 and Arizona Line\"),\n",
    "        (r'I-10 and az line', \"I-10 and Arizona Line\"),\n",
    "        (r'I-8 and az line', \"I-8 and Arizona Line\"),\n",
    "        (r'SR-62 and az line', \"SR-62 and Arizona Line\"),\n",
    "        (r'I-80 and nv line', \"I-80 and Nevada Line\"),\n",
    "        (r'I-5 and or line', \"I-5 and Oregon Line\"),\n",
    "        (r'SR-15 and nv line', \"I-15 and Nevada Line\"),\n",
    "        (r'I-15 and nv line', \"I-15 and Nevada Line\")\n",
    "    ]\n",
    "    \n",
    "    # Apply direct replacements\n",
    "    df['route_intersection_destination_1'] = df['route_intersection_destination_1'].replace(replacements)\n",
    "    \n",
    "    # Apply pattern-based replacements\n",
    "    for pattern, replacement in pattern_replacements:\n",
    "        df.loc[df['route_intersection_destination_1'].str.contains(pattern, case=False, na=False), \n",
    "               'route_intersection_destination_1'] = replacement\n",
    "    \n",
    "    # Drop the original column\n",
    "    df.drop(columns=['route_intersection_destination'], inplace=True)\n",
    "    \n",
    "    # Rename the new column\n",
    "    df.rename(columns={'route_intersection_destination_1': 'route_intersection_destination'}, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e8e3284-a887-4282-9732-e8e06fe7ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_workflow_step2(df):\n",
    "    df = process_intersections(df, intersection_columns)\n",
    "    df = clean_intersections_1(df)\n",
    "    df = update_route_intersection(df)\n",
    "    df = update_route_intersection_destination(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c07d39-918a-49c5-85a6-4686d02e8a90",
   "metadata": {},
   "source": [
    "### Destination Intersections\n",
    "\n",
    "#### This is where I'm currently working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69eacccd-31b4-4089-87de-030b56cb52bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean and standardize the ['route_intersection_destination'] field - similar to the origin standardization\n",
    "\n",
    "def clean_intersection_destination(intersection_str):\n",
    "    \"\"\"\n",
    "    Cleans up a complex intersection string by removing parentheses and off-ramp/UC noise,\n",
    "    and formatting to retain primary intersection and city name.\n",
    "    \"\"\"\n",
    "    if not isinstance(intersection_str, str):\n",
    "        return None\n",
    "\n",
    "    # Step 1: Remove anything in parentheses\n",
    "    cleaned = re.sub(r\"\\(.*?\\)\", \"\", intersection_str)\n",
    "\n",
    "    # Step 2: Remove anything like '* bernal rd uc under 85 *' or extra asterisks\n",
    "    cleaned = re.sub(r\"\\*.*?\\*\", \"\", cleaned)\n",
    "\n",
    "    # Step 3: Remove off ramp / uc under / ramp / connector language\n",
    "    cleaned = re.sub(r\"\\b(?:off\\s*ramp|on\\s*ramp|uc\\s*under|connector|exit)\\b.*\", \"\", cleaned, flags=re.IGNORECASE)\n",
    "\n",
    "    # Step 4: Remove extra spaces, slashes, or multiple road names after a slash\n",
    "    cleaned = re.sub(r\"/.*?\\b\", \"\", cleaned)\n",
    "\n",
    "    # Step 5: Remove trailing commas and normalize spaces\n",
    "    cleaned = re.sub(r\",\\s*$\", \"\", cleaned)\n",
    "    cleaned = re.sub(r\"\\s{2,}\", \" \", cleaned).strip()\n",
    "    \n",
    "    # Remove \", California\" if present at the end\n",
    "    cleaned = re.sub(r\",?\\s*California$\", \"\", cleaned, flags=re.IGNORECASE)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5b81c8c-5ade-484a-9b2d-f59eb2f1118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"route_intersection_destination\"] = df[\"route_intersection_destination\"].apply(clean_intersection_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a3aa468-302b-4e1b-b65b-9a882b548f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"osow_destinations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1acb8808-e78f-4336-a076-619a07288d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2399, 35)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2399, 35)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b305fa-911b-4cfe-a876-e212a17abf35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b39328-df9e-4b83-8c97-9d7c32ff0e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724952ff-48e2-4c97-a793-08123bd9791b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bb0e3bc-c07d-4a60-b31e-5bb1cb5f3bf9",
   "metadata": {},
   "source": [
    "### Origin Intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "780ee875-8c46-477f-8043-6e274f7adc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_intersection_origin(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Remove anything in parentheses\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n",
    "\n",
    "    # Handle special case: starts with \"from co rd\"\n",
    "    if re.search(r\"\\bfrom co rd\\b\", text, flags=re.IGNORECASE):\n",
    "        # Extract first match of \"co rd <token>\" where token can be letters/numbers\n",
    "        co_rd_match = re.search(r\"\\bco rd\\s+[a-zA-Z0-9]+\", text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Extract first highway code like 005n, 036e, etc.\n",
    "        hwy_match = re.search(r\"\\b0?(\\d{2,3})[nsew]?\\b\", text)\n",
    "\n",
    "        if co_rd_match and hwy_match:\n",
    "            co_rd_str = co_rd_match.group(0).strip().lower()  # normalize casing\n",
    "            hwy_num = hwy_match.group(1).lstrip(\"0\")  # strip leading zeros\n",
    "\n",
    "            # Use SR for now; could expand to I- or US- if needed\n",
    "            hwy_str = f\"SR-{hwy_num}\"\n",
    "\n",
    "            return f\"{co_rd_str} and {hwy_str}\"\n",
    "\n",
    "    # ---- Normal processing continues here ----\n",
    "\n",
    "    # Remove \", California\" or variants — only when capitalized\n",
    "    text = re.sub(r\"\\s*,?\\s*(?<![a-z])California\\b\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    if \" and \" in text:\n",
    "        left, right = re.split(r\"\\s+and\\s+\", text, maxsplit=1)\n",
    "\n",
    "        if ',' in left:\n",
    "            left_cleaned = left.split(',')[0].strip()\n",
    "        else:\n",
    "            left = re.sub(r\"[/\\\\]\", \" \", left)\n",
    "\n",
    "            stopwords = [\n",
    "                \"st\", \"rd\", \"blvd\", \"ave\", \"dr\", \"ln\", \"ct\", \"pkwy\", \"way\", \"hwy\",\n",
    "                \"pl\", \"trl\", \"cir\", \"loop\", \"road\", \"entrance\", \"exit\",\n",
    "                \"street\", \"avenue\", \"boulevard\", \"parkway\", \"drive\", \"border\"\n",
    "            ]\n",
    "\n",
    "            tokens = left.strip().split()\n",
    "            stopword_idx = -1\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token.lower() in stopwords:\n",
    "                    stopword_idx = i\n",
    "                    break\n",
    "\n",
    "            if stopword_idx != -1:\n",
    "                left_cleaned = \" \".join(tokens[:stopword_idx + 1])\n",
    "            else:\n",
    "                keep_tokens = []\n",
    "                for token in tokens:\n",
    "                    if re.match(r\"^[A-Z]\", token):\n",
    "                        break\n",
    "                    keep_tokens.append(token)\n",
    "                left_cleaned = \" \".join(keep_tokens)\n",
    "\n",
    "        text = f\"{left_cleaned} and {right.strip()}\"\n",
    "\n",
    "    # Clean up junk punctuation\n",
    "    text = re.sub(r\"[`*'\\\"]*(\\b\\w{1,3}\\b)[`*'\\\"]*\", r\"\\1\", text)\n",
    "    text = re.sub(r\"(?<!\\w)[`*'\\\"]+|[`*'\\\"]+(?!\\w)\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cac788ab-0356-46f7-91f2-6f2c9aa84c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"route_intersection_0\"] = df[\"route_intersection_0\"].apply(clean_intersection_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "647d036b-2ef2-48a0-94a0-432f3b4369a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"osow_origins.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "949df4b6-01b5-4378-ac4f-c300554129b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2399, 35)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2399, 35)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8754d62-af8c-4e8a-a03a-66370e010598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_intersection_origin_2(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    stopwords = [\n",
    "        \"st\", \"rd\", \"blvd\", \"ave\", \"dr\", \"ln\", \"ct\", \"pkwy\", \"way\", \"hwy\",\n",
    "        \"pl\", \"trl\", \"cir\", \"loop\", \"road\", \"entrance\", \"exit\",\n",
    "        \"street\", \"avenue\", \"boulevard\", \"parkway\", \"drive\", \"border\"\n",
    "    ]\n",
    "\n",
    "    # Only act if \" and \" is present\n",
    "    if \" and \" in text:\n",
    "        left, right = re.split(r\"\\s+and\\s+\", text, maxsplit=1)\n",
    "\n",
    "        # Tokenize the left part\n",
    "        tokens = left.strip().split()\n",
    "\n",
    "        # Find first stopword\n",
    "        stopword_idx = -1\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token.lower() in stopwords:\n",
    "                stopword_idx = i\n",
    "                break\n",
    "\n",
    "        if stopword_idx != -1:\n",
    "            # Keep only the tokens up to and including the stopword\n",
    "            left_cleaned = \" \".join(tokens[:stopword_idx + 1])\n",
    "            text = f\"{left_cleaned} and {right.strip()}\"\n",
    "\n",
    "    # Normalize spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3facd14-f56e-4a14-a283-2dda112409c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"route_intersection_0\"] = df[\"route_intersection_0\"].apply(clean_intersection_origin_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13a8cc82-8420-49a5-84cb-5d0bf5d4bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"osow_origins_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74d92747-8bff-45ea-ba15-66d76076fcd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2399, 35)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "05133856-de90-4401-9a14-183ca5bda2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to help clean the records that are still returning invalid intersections\n",
    "def fix_co_rd_intersections(row):\n",
    "    origin = row['route_intersection_0']\n",
    "    highways = row['authorizedhighways']\n",
    "\n",
    "    if not isinstance(origin, str):\n",
    "        return origin\n",
    "\n",
    "    origin_lower = origin.lower()\n",
    "\n",
    "    # CASE 1: Starts with 'co rd'\n",
    "    if origin_lower.startswith(\"co rd\"):\n",
    "        match = re.search(r\"\\bco rd\\s+([a-zA-Z0-9]+)\", str(highways), flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            number_part = match.group(1)\n",
    "            numeric_only = re.sub(r\"[^\\d]\", \"\", number_part)\n",
    "            if numeric_only:\n",
    "                new_origin_left = f\"co rd {numeric_only}\"\n",
    "                if \" and \" in origin:\n",
    "                    _, right = origin.split(\" and \", 1)\n",
    "                    return f\"{new_origin_left} and {right.strip()}\"\n",
    "                else:\n",
    "                    return new_origin_left\n",
    "\n",
    "    # CASE 2: Starts with 'st and'\n",
    "    elif origin_lower.startswith(\"st and\"):\n",
    "        match = re.search(r\"from\\s+(.*?)\\bst\\b\", str(highways), flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            possible_street = match.group(1).strip()\n",
    "            tokens = possible_street.split()\n",
    "            if tokens:\n",
    "                new_origin_left = f\"{tokens[-1]} st\"\n",
    "                if \" and \" in origin:\n",
    "                    _, right = origin.split(\" and \", 1)\n",
    "                    return f\"{new_origin_left} and {right.strip()}\"\n",
    "                else:\n",
    "                    return new_origin_left\n",
    "\n",
    "    # CASE 3: Starts with 'ave and'\n",
    "    elif origin_lower.startswith(\"ave and\"):\n",
    "        match = re.search(r\"\\bave\\s+[`\\\"']?([a-zA-Z0-9]+)[`\\\"']?\", str(highways), flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            suffix = match.group(1).strip()\n",
    "            new_origin_left = f\"ave {suffix}\"\n",
    "            if \" and \" in origin:\n",
    "                _, right = origin.split(\" and \", 1)\n",
    "                return f\"{new_origin_left} and {right.strip()}\"\n",
    "            else:\n",
    "                return new_origin_left\n",
    "\n",
    "    # CASE 4: Starts with 'and'\n",
    "    elif origin_lower.startswith(\"and\"):\n",
    "        match = re.search(r\"from\\s+(.*?)(?:-|\\bon ramp\\b)\", str(highways), flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            location = match.group(1).strip()\n",
    "\n",
    "            location = re.sub(r\"\\b[enws]/b\\b\", \"\", location, flags=re.IGNORECASE)\n",
    "            location_cleaned = re.sub(r\"\\bhighway\\b\", \"hwy\", location, flags=re.IGNORECASE)\n",
    "            location_cleaned = re.sub(r\"\\s+\", \" \", location_cleaned).strip()\n",
    "\n",
    "            hwy_match = re.search(r\"\\b(1\\d{2})([ewns]?)\\b\", highways, flags=re.IGNORECASE)\n",
    "            if hwy_match:\n",
    "                hwy_number = hwy_match.group(1)\n",
    "                location_cleaned += f\" & I-{hwy_number}\"\n",
    "\n",
    "            return location_cleaned\n",
    "\n",
    "    # CASE 5: Starts with 'rd and'\n",
    "    if origin_lower.startswith(\"rd and\"):\n",
    "        rd_match = re.search(r\"\\brd\\s+\\d{1,4}\\b\", highways, flags=re.IGNORECASE)\n",
    "        if rd_match:\n",
    "            full_rd = rd_match.group(0).strip()\n",
    "            _, right = origin.split(\" and \", 1)\n",
    "            return f\"{full_rd.lower()} and {right.strip()}\"\n",
    "\n",
    "    # CASE 6: Starts with 'old us' and is incomplete\n",
    "    if origin_lower.startswith(\"old us\") and \" and \" in origin:\n",
    "        match = re.search(r\"\\bold us\\s+\\d{1,3}\", highways, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            full_old_us = match.group(0).strip()\n",
    "            _, right = origin.split(\" and \", 1)\n",
    "            return f\"{full_old_us.lower()} and {right.strip()}\"\n",
    "\n",
    "    # CASE 7: Starts with 'mt and'\n",
    "    if origin_lower.startswith(\"mt and\"):\n",
    "        match = re.search(r\"\\bmt\\.?\\s+view\\s+rd\\b\", highways, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            new_origin_left = \"mountain view rd\"\n",
    "            _, right = origin.split(\" and \", 1)\n",
    "            return f\"{new_origin_left} and {right.strip()}\"\n",
    "\n",
    "# This one was not working, I may attempt this cleaning step at a later time\n",
    "    # # CASE 8: Starts with 'ave e and'\n",
    "    # if origin_lower.startswith(\"ave e and\"):\n",
    "    #     loc = row.get(\"route_location_origin\", \"\")\n",
    "    #     if isinstance(loc, str):\n",
    "    #         # Extract everything before the first comma (remove city/state)\n",
    "    #         loc_cleaned = loc.split(\",\")[0].strip()\n",
    "    #         # Strip out trailing direction if present (e.g., \"ave e\")\n",
    "    #         match = re.match(r\"([a-z\\s]+ave)\", loc_cleaned, flags=re.IGNORECASE)\n",
    "    #         if match:\n",
    "    #             left_name = match.group(1).strip()\n",
    "    #             _, right = origin.split(\" and \", 1)\n",
    "    #             return f\"{left_name.lower()} and {right.strip()}\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # CASE 9: Starts with 'n and' — pull first location from authorizedhighways\n",
    "    if origin_lower.startswith(\"n and\"):\n",
    "        match = re.search(r\"from\\s+([^\\-/]+)\", highways, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            first_location = match.group(1).strip()\n",
    "\n",
    "            # If the first location has a slash (e.g., \"n. gate/railroad ave\"), take just the first part\n",
    "            first_location = first_location.split(\"/\")[0].strip()\n",
    "\n",
    "            # Normalize \"n.\" to \"n\" and expand to \"north\" if desired\n",
    "            first_location = first_location.replace(\"n.\", \"n\").strip()\n",
    "            first_location = re.sub(r\"\\s+\", \" \", first_location)\n",
    "\n",
    "            # Optional: expand \"n\" to \"north\" for clarity\n",
    "            if first_location.lower().startswith(\"n \"):\n",
    "                first_location = first_location.replace(\"n \", \"north \", 1)\n",
    "\n",
    "            if \" and \" in origin:\n",
    "                _, right = origin.split(\" and \", 1)\n",
    "                return f\"{first_location.lower()} and {right.strip()}\"\n",
    "    \n",
    "\n",
    "    # FINAL CLEANING CASE\n",
    "    if \" and \" in origin:\n",
    "        left, right = origin.split(\" and \", 1)\n",
    "        left_tokens = left.strip().split()\n",
    "        cleaned_tokens = []\n",
    "        seen_tokens = set()\n",
    "\n",
    "        for token in left_tokens:\n",
    "            token_lower = token.lower()\n",
    "            if token.islower() or token_lower in {\n",
    "                \"fwy\", \"blvd\", \"pkwy\", \"lane\", \"road\", \"rd\", \"st\", \"ave\", \"dr\", \"way\"\n",
    "            }:\n",
    "                if token_lower not in seen_tokens:\n",
    "                    cleaned_tokens.append(token_lower)\n",
    "                    seen_tokens.add(token_lower)\n",
    "\n",
    "        cleaned_left = \" \".join(cleaned_tokens).strip()\n",
    "        right_cleaned = re.sub(r\"\\b[nswe]/b on ramp\\b\", \"\", right.strip(), flags=re.IGNORECASE)\n",
    "        right_cleaned = re.sub(r\"\\s+\", \" \", right_cleaned).strip()\n",
    "\n",
    "        return f\"{cleaned_left} and {right_cleaned}\"\n",
    "\n",
    "    return origin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13edca35-cc48-4f57-85ed-c26f36d05a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['route_intersection_0'] = df.apply(fix_co_rd_intersections, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cf05507-43f4-480b-816f-fbefaec386da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_workflow_step2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21d07c15-a7d8-4af2-afa5-a4e6650aff3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_intersection_coordinates(df, origin_intersections, destination_intersections):\n",
    "    \"\"\"\n",
    "    Adds x_coords and y_coords for origin and destination intersections.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe.\n",
    "    origin_intersections (list of dict): List of origin intersection dictionaries.\n",
    "    destination_intersections (list of dict): List of destination intersection dictionaries.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Updated dataframe with added coordinate columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert lists of dictionaries into dictionaries for quick lookup, handling missing keys safely\n",
    "    origin_lookup = {\n",
    "        d.get(\"origin_intersection\", None): (d.get(\"x_coords\", None), d.get(\"y_coords\", None))\n",
    "        for d in origin_intersections if \"origin_intersection\" in d\n",
    "    }\n",
    "\n",
    "    destination_lookup = {\n",
    "        d.get(\"destination_intersection\", None): (d.get(\"x_coords\", None), d.get(\"y_coords\", None))\n",
    "        for d in destination_intersections if \"destination_intersection\" in d\n",
    "    }\n",
    "\n",
    "    #Extract origin coordinates\n",
    "    df[\"route_intersection_origin_x_coords\"] = df[\"route_intersection_origin\"].map(\n",
    "        lambda x: origin_lookup.get(x, (None, None))[0]\n",
    "    )\n",
    "    df[\"route_intersection_origin_y_coords\"] = df[\"route_intersection_origin\"].map(\n",
    "        lambda x: origin_lookup.get(x, (None, None))[1]\n",
    "    )\n",
    "\n",
    "    # Insert new columns at position 11 and 12\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(11, cols.pop(cols.index(\"route_intersection_origin_x_coords\")))\n",
    "    cols.insert(12, cols.pop(cols.index(\"route_intersection_origin_y_coords\")))\n",
    "    df = df[cols]\n",
    "\n",
    "    # Extract destination coordinates\n",
    "    df[\"route_intersection_destination_x_coords\"] = df[\"route_intersection_destination\"].map(\n",
    "        lambda x: destination_lookup.get(x, (None, None))[0]\n",
    "    )\n",
    "    df[\"route_intersection_destination_y_coords\"] = df[\"route_intersection_destination\"].map(\n",
    "        lambda x: destination_lookup.get(x, (None, None))[1]\n",
    "    )\n",
    "\n",
    "    # Move the new destination coordinate columns to be right after the last column\n",
    "    last_col_index = df.columns.get_loc(\"route_intersection_destination\") + 1\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(last_col_index, cols.pop(cols.index(\"route_intersection_destination_x_coords\")))\n",
    "    cols.insert(last_col_index + 1, cols.pop(cols.index(\"route_intersection_destination_y_coords\")))\n",
    "    df = df[cols]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654e815-ee6f-4f99-9d40-74f77f18efd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcaeaf5b-3853-41a1-828a-082f620b46d2",
   "metadata": {},
   "source": [
    "### Resume wrangling here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5aa7b8ea-0c06-4647-a127-055a7db6d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_direction_suffix(intersection):\n",
    "    if not isinstance(intersection, str):\n",
    "        return intersection\n",
    "\n",
    "    intersection = intersection.strip()\n",
    "    direction_words = {\"north\", \"south\", \"east\", \"west\", \"nb\", \"sb\", \"eb\", \"wb\"}\n",
    "\n",
    "    words = intersection.split()\n",
    "    if words and words[-1].lower() in direction_words:\n",
    "        return \" \".join(words[:-1])\n",
    "    \n",
    "    return intersection\n",
    "\n",
    "\n",
    "\n",
    "def clean_route_intersection_destination(row):\n",
    "    intersection = row.get('route_intersection_destination', '')\n",
    "    destination = row.get('destination', '')\n",
    "\n",
    "    if not isinstance(intersection, str) or not isinstance(destination, str):\n",
    "        return intersection\n",
    "\n",
    "    intersection = intersection.strip()\n",
    "    destination = destination.strip()\n",
    "\n",
    "    # Leave State Boundary records unchanged\n",
    "    if re.search(r\"and (Nevada|Arizona|Oregon) Line$\", intersection, re.IGNORECASE):\n",
    "        return intersection\n",
    "    \n",
    "    intersection = remove_direction_suffix(intersection)\n",
    "\n",
    "    # Remove direction suffix (nb, sb, eb, wb, north, south, east, west) if it's at the end\n",
    "    #intersection = re.sub(r\"\\s+(nb|sb|eb|wb|north|south|east|west)$\", \"\", intersection, flags=re.IGNORECASE)\n",
    "\n",
    "    # Normalize both for loose match comparison\n",
    "    dest_clean = re.sub(r\"[^\\w\\s]\", \"\", destination).lower().strip()\n",
    "    inter_clean = re.sub(r\"[^\\w\\s]\", \"\", intersection).lower().strip()\n",
    "\n",
    "    # Remove destination from end if it matches\n",
    "    if inter_clean.endswith(f\" {dest_clean}\"):\n",
    "        if intersection.endswith(f\" {destination}\"):\n",
    "            return intersection[: -len(destination)].rstrip()\n",
    "        else:\n",
    "            intersection_words = intersection.split()\n",
    "            destination_words = destination.split()\n",
    "            if intersection_words[-len(destination_words):] == destination_words:\n",
    "                return \" \".join(intersection_words[:-len(destination_words)])\n",
    "\n",
    "    return intersection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "211495fd-f7c0-401e-a314-df9331a2b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['route_intersection_destination'] = df.apply(clean_route_intersection_destination, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f72f86-1a2c-4962-93c7-1b77ae3d35b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52cb0530-3f09-48cb-8e05-ea172b1095fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the coordinates function after the Destination field has been cleaned\n",
    "df = add_intersection_coordinates(df, origin_intersections, destination_intersections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7bccf6-96d5-4005-ac23-4d54f9b53901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed57d4-b7d8-42c3-afea-3316283d746a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7975e026-c045-4ccf-8c8e-eef35a10896b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2399, 39)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2399, 39)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2c7437f-45e3-41fa-91fa-cd45483d3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"osow_destination_coords.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f00a6-c3df-4169-b86e-3ddfa30bdf61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9efedc0-51fe-4f63-b73d-ca579bce884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mid_route_coordinates(df):\n",
    "    \"\"\"\n",
    "    Adds 'x_coords' and 'y_coords' fields next to each mid-route intersection \n",
    "    and moves all route_intersection-related columns to the end.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe with multiple route_intersection_X fields.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Updated dataframe with coordinate fields added next to each intersection.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a lookup dictionary for SHS intersections (uppercase, removing \", California\")\n",
    "    shs_lookup = {\n",
    "        d.get(\"shs_intersection\", \"\").strip().upper().replace(\", CALIFORNIA\", \"\"): \n",
    "        (d.get(\"x_coords\", None), d.get(\"y_coords\", None))\n",
    "        for d in shs_intersections if \"shs_intersection\" in d\n",
    "    }\n",
    "\n",
    "    # Identify mid-route intersection columns (excluding origin and destination)\n",
    "    mid_route_cols = sorted(\n",
    "        [col for col in df.columns if col.startswith(\"route_intersection_\") and col.split(\"_\")[-1].isdigit()],\n",
    "        key=lambda x: int(x.split(\"_\")[-1])  # Sort numerically\n",
    "    )\n",
    "\n",
    "    # Format DataFrame values for lookup (uppercase + remove double spaces)\n",
    "    for col in mid_route_cols:\n",
    "        df[col] = df[col].astype(str).str.strip().str.upper().replace(\" ,\", \",\").replace(\"  \", \" \")\n",
    "\n",
    "    # Add new coordinate columns (convert df values to uppercase for correct lookup)\n",
    "    for col in mid_route_cols:\n",
    "        formatted_col = df[col].apply(lambda x: x.replace(\", CALIFORNIA\", \"\").strip().upper() if isinstance(x, str) else None)\n",
    "        df[f\"{col}_x_coords\"] = formatted_col.apply(lambda x: shs_lookup.get(x, (None, None))[0] if x in shs_lookup else None)\n",
    "        df[f\"{col}_y_coords\"] = formatted_col.apply(lambda x: shs_lookup.get(x, (None, None))[1] if x in shs_lookup else None)\n",
    "\n",
    "    # Define route-related columns to move to the end\n",
    "    route_cols = []\n",
    "    for col in mid_route_cols:\n",
    "        route_cols.append(col)\n",
    "        route_cols.append(f\"{col}_x_coords\")\n",
    "        route_cols.append(f\"{col}_y_coords\")\n",
    "\n",
    "    # Add origin and destination fields to the route-related list\n",
    "    route_cols = (\n",
    "        [\"route_intersection_origin\", \"route_intersection_origin_x_coords\", \"route_intersection_origin_y_coords\"]\n",
    "        + route_cols\n",
    "        + [\"route_intersection_destination\", \"route_intersection_destination_x_coords\", \"route_intersection_destination_y_coords\"]\n",
    "    )\n",
    "\n",
    "    # Identify non-route columns (to keep at the front)\n",
    "    non_route_cols = [col for col in df.columns if col not in route_cols]\n",
    "\n",
    "    # Ensure all columns exist before reordering\n",
    "    final_col_order = non_route_cols + [col for col in route_cols if col in df.columns]\n",
    "    df = df[final_col_order]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d496ea0-a05c-4b59-85b8-2b84a2f087c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_mid_route_coordinates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49b932b4-dde4-454b-afd4-ff6118703aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df.to_csv(\"osow_vehicle_permits_authorizedhighways_cv.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae624d9f-03b9-48ca-a008-2f686a0e7787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_and_remove_destination_fields(df):\n",
    "    \"\"\"\n",
    "    Update the last available intersection's coordinates with the destination coordinates,\n",
    "    then remove the destination fields.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing route intersection fields.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with destination fields removed.\n",
    "    \"\"\"\n",
    "    for index, row in df.iterrows():\n",
    "        last_intersection_col = None\n",
    "\n",
    "        # Identify the last non-null intersection before the destination\n",
    "        for i in range(1, 24):  # Assuming up to 23 intermediate intersections\n",
    "            intersection_col = f'route_intersection_{i}'\n",
    "            x_col = f'route_intersection_{i}_x_coords'\n",
    "            y_col = f'route_intersection_{i}_y_coords'\n",
    "\n",
    "            if pd.notna(row.get(intersection_col)) and row[intersection_col].strip():\n",
    "                last_intersection_col = (intersection_col, x_col, y_col)\n",
    "\n",
    "        # If a last intersection exists, update its coordinates with destination coordinates\n",
    "        if last_intersection_col:\n",
    "            _, last_x_col, last_y_col = last_intersection_col\n",
    "            dest_x_col = 'route_intersection_destination_x_coords'\n",
    "            dest_y_col = 'route_intersection_destination_y_coords'\n",
    "\n",
    "            if pd.notna(row.get(dest_x_col)) and pd.notna(row.get(dest_y_col)):\n",
    "                df.at[index, last_x_col] = row[dest_x_col]\n",
    "                df.at[index, last_y_col] = row[dest_y_col]\n",
    "\n",
    "    # Drop the destination fields from the dataset\n",
    "    df = df.drop(columns=['route_intersection_destination', \n",
    "                          'route_intersection_destination_x_coords', \n",
    "                          'route_intersection_destination_y_coords'], errors='ignore')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "042f80a7-d7d0-494d-abad-074d57c3a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_and_remove_destination_fields(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9c984ba0-b99b-4f5d-8d74-2f6836e5381e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"osow_destination_fields.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bb90405-4ca7-4648-a914-b134a85a31a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2399, 82)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fbd878-8327-486c-8bc6-d13e9b9be5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8661a3d-e3b3-4ca8-a71b-47bc51f223f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_and_shift_intersections(df):\n",
    "    \"\"\"\n",
    "    Cleans route intersection fields by:\n",
    "    1. Removing intersections that lack x_coords and y_coords.\n",
    "    2. Shifting remaining values left to fill gaps.\n",
    "    3. Removing columns that are empty after shifting.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing route intersection fields.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    max_intersections = 23  # Max expected number of intersections\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        cleaned_intersections = []  # Store valid intersections\n",
    "\n",
    "        # Collect valid intersections (non-null x_coords and y_coords)\n",
    "        for i in range(1, max_intersections + 1):\n",
    "            name_col = f'route_intersection_{i}'\n",
    "            x_col = f'route_intersection_{i}_x_coords'\n",
    "            y_col = f'route_intersection_{i}_y_coords'\n",
    "\n",
    "            if pd.notna(row.get(name_col)) and pd.notna(row.get(x_col)) and pd.notna(row.get(y_col)):\n",
    "                cleaned_intersections.append(\n",
    "                    (row[name_col], row[x_col], row[y_col])\n",
    "                )\n",
    "\n",
    "        # Clear existing values\n",
    "        for i in range(1, max_intersections + 1):\n",
    "            df.at[index, f'route_intersection_{i}'] = None\n",
    "            df.at[index, f'route_intersection_{i}_x_coords'] = None\n",
    "            df.at[index, f'route_intersection_{i}_y_coords'] = None\n",
    "\n",
    "        # Re-populate with shifted values\n",
    "        for i, (name, x, y) in enumerate(cleaned_intersections, start=1):\n",
    "            df.at[index, f'route_intersection_{i}'] = name\n",
    "            df.at[index, f'route_intersection_{i}_x_coords'] = x\n",
    "            df.at[index, f'route_intersection_{i}_y_coords'] = y\n",
    "\n",
    "    # Drop empty intersection columns\n",
    "    for i in range(1, max_intersections + 1):\n",
    "        name_col = f'route_intersection_{i}'\n",
    "        x_col = f'route_intersection_{i}_x_coords'\n",
    "        y_col = f'route_intersection_{i}_y_coords'\n",
    "\n",
    "        if df[name_col].isna().all():  # Check if entire column is empty\n",
    "            df.drop(columns=[name_col, x_col, y_col], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76c0e328-cd89-46bb-8678-e27d37a07ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = clean_and_shift_intersections(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fdc795f2-cb4c-4744-8a45-5be51cfc13e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df.to_csv(\"osow_vehicle_permits_authorizedhighways_cv_v3.csv\", index=False)\n",
    "\n",
    "# comparing\n",
    "df.to_csv(\"osow_vehicle_permits_authorizedhighways_cv_v4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "874fe2ce-3388-4de1-855b-8d6305f15ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2399, 82)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understand the shape\n",
    "# current shape is (2399, 67)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871bc4d7-d2a0-4369-9909-581f89c697cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33959c4-0954-4baf-9c96-f5b234ae48bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
