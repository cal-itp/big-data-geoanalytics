{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2d5a2c-4b02-44f3-af2d-0febddc5fbc6",
   "metadata": {},
   "source": [
    "# parsing the ['authorizedhighways'] column in the all_permits data for OSOW vehicle permits\n",
    "\n",
    "- ns\n",
    "\n",
    "- Originally requested by Stephen Yoon  \n",
    "    - data provided by Stephen's office\n",
    "- next step ==   \n",
    "    - get the coordinates for the intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acace08e-1139-4d26-876f-0101415d1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import gcsfs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afda019a-e695-4f89-9464-cf9be44c4b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in the coordinates from the utils docs\n",
    "from osow_frp_o_d_utils import origin_intersections, destination_intersections\n",
    "from shs_intersections_utils import shs_intersections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e08b8f81-5d63-4b8f-a1b1-c9ab9565f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original_mapping is needed to standardize the highway names for the various records \n",
    "original_mapping = {\n",
    "    \"5\": \"I-5\", \"10\": \"I-10\", \"15\": \"I-15\", \"8\": \"I-8\", \"40\": \"I-40\", \"80\": \"I-80\", \"105\": \"I-105\", \"110\": \"I-110\",\n",
    "    \"205\": \"I-205\", \"210\": \"I-210\", \"215\": \"I-215\", \"280\": \"I-280\", \"380\": \"I-380\", \"405\": \"I-405\",\n",
    "    \"505\": \"I-505\", \"580\": \"I-580\", \"605\": \"I-605\", \"680\": \"I-680\", \"710\": \"I-710\", \"805\": \"I-805\",\n",
    "    \"880\": \"I-880\", \"980\": \"I-980\", \"1\": \"SR-1\", \"2\": \"SR-2\", \"3\": \"SR-3\", \"4\": \"SR-4\", \"7\": \"SR-7\",\n",
    "    \"9\": \"SR-9\", \"11\": \"SR-11\", \"12\": \"SR-12\", \"13\": \"SR-13\", \"14\": \"SR-14\", \n",
    "    #\"15\": \"SR-15\",\n",
    "    \"16\": \"SR-16\", \"17\": \"SR-17\", \"18\": \"SR-18\", \"20\": \"SR-20\", \"22\": \"SR-22\", \"23\": \"SR-23\",\n",
    "    \"24\": \"SR-24\", \"25\": \"SR-25\", \"26\": \"SR-26\", \"27\": \"SR-27\", \"28\": \"SR-28\", \"29\": \"SR-29\",\n",
    "    \"32\": \"SR-32\", \"33\": \"SR-33\", \"34\": \"SR-34\", \"35\": \"SR-35\", \"36\": \"SR-36\", \"37\": \"SR-37\",\n",
    "    \"38\": \"SR-38\", \"39\": \"SR-39\", \"41\": \"SR-41\", \"43\": \"SR-43\", \"44\": \"SR-44\", \"45\": \"SR-45\",\n",
    "    \"46\": \"SR-46\", \"seaside highway\": \"SR-47\", \"47\": \"SR-47\", \"49\": \"SR-49\", \"51\": \"SR-51\", \"52\": \"SR-52\", \"53\": \"SR-53\",\n",
    "    \"54\": \"SR-54\", \"55\": \"SR-55\", \"56\": \"SR-56\", \"57\": \"SR-57\", \"58\": \"SR-58\", \"59\": \"SR-59\",\n",
    "    \"60\": \"SR-60\", \"61\": \"SR-61\", \"62\": \"SR-62\", \"63\": \"SR-63\", \"65\": \"SR-65\", \"66\": \"SR-66\",\n",
    "    \"67\": \"SR-67\", \"68\": \"SR-68\", \"70\": \"SR-70\", \"71\": \"SR-71\", \"72\": \"SR-72\", \"73\": \"SR-73\",\n",
    "    \"74\": \"SR-74\", \"75\": \"SR-75\", \"76\": \"SR-76\", \"77\": \"SR-77\", \"78\": \"SR-78\", \"79\": \"SR-79\",\n",
    "    \"82\": \"SR-82\", \"83\": \"SR-83\", \"84\": \"SR-84\", \"85\": \"SR-85\", \"86\": \"SR-86\", \"87\": \"SR-87\",\n",
    "    \"88\": \"SR-88\", \"89\": \"SR-89\", \"90\": \"SR-90\", \"91\": \"SR-91\", \"92\": \"SR-92\", \"94\": \"SR-94\",\n",
    "    \"96\": \"SR-96\", \"98\": \"SR-98\", \"imperial highway\": \"SR-99\", \"99\": \"SR-99\", \"103\": \"SR-103\", \"104\": \"SR-104\", \"107\": \"SR-107\",\n",
    "    \"108\": \"SR-108\", \"109\": \"SR-109\", \"110\": \"SR-110\", \"111\": \"SR-111\", \"112\": \"SR-112\",\n",
    "    \"113\": \"SR-113\", \"114\": \"SR-114\", \"115\": \"SR-115\", \"116\": \"SR-116\", \"118\": \"SR-118\",\n",
    "    \"119\": \"SR-119\", \"120\": \"SR-120\", \"121\": \"SR-121\", \"123\": \"SR-123\", \"124\": \"SR-124\",\n",
    "    \"125\": \"SR-125\", \"126\": \"SR-126\", \"127\": \"SR-127\", \"128\": \"SR-128\", \"129\": \"SR-129\",\n",
    "    \"130\": \"SR-130\", \"131\": \"SR-131\", \"132\": \"SR-132\", \"133\": \"SR-133\", \"134\": \"SR-134\",\n",
    "    \"135\": \"SR-135\", \"136\": \"SR-136\", \"137\": \"SR-137\", \"138\": \"SR-138\", \"139\": \"SR-139\",\n",
    "    \"140\": \"SR-140\", \"142\": \"SR-142\", \"144\": \"SR-144\", \"145\": \"SR-145\", \"146\": \"SR-146\",\n",
    "    \"147\": \"SR-147\", \"149\": \"SR-149\", \"150\": \"SR-150\", \"151\": \"SR-151\", \"152\": \"SR-152\",\n",
    "    \"153\": \"SR-153\", \"154\": \"SR-154\", \"155\": \"SR-155\", \"156\": \"SR-156\", \"158\": \"SR-158\",\n",
    "    \"160\": \"SR-160\", \"161\": \"SR-161\", \"162\": \"SR-162\", \"163\": \"SR-163\", \"164\": \"SR-164\",\n",
    "    \"165\": \"SR-165\", \"166\": \"SR-166\", \"167\": \"SR-167\", \"168\": \"SR-168\", \"169\": \"SR-169\",\n",
    "    \"170\": \"SR-170\", \"172\": \"SR-172\", \"173\": \"SR-173\", \"174\": \"SR-174\", \"175\": \"SR-175\",\n",
    "    \"177\": \"SR-177\", \"178\": \"SR-178\", \"180\": \"SR-180\", \"182\": \"SR-182\", \"183\": \"SR-183\",\n",
    "    \"184\": \"SR-184\", \"185\": \"SR-185\", \"186\": \"SR-186\", \"187\": \"SR-187\", \"188\": \"SR-188\",\n",
    "    \"189\": \"SR-189\", \"190\": \"SR-190\", \"191\": \"SR-191\", \"192\": \"SR-192\", \"193\": \"SR-193\",\n",
    "    \"197\": \"SR-197\", \"198\": \"SR-198\", \"200\": \"SR-200\", \"201\": \"SR-201\", \"202\": \"SR-202\",\n",
    "    \"203\": \"SR-203\", \"204\": \"SR-204\", \"207\": \"SR-207\", \"210\": \"SR-210\", \"211\": \"SR-211\",\n",
    "    \"213\": \"SR-213\", \"216\": \"SR-216\", \"217\": \"SR-217\", \"218\": \"SR-218\", \"219\": \"SR-219\",\n",
    "    \"220\": \"SR-220\", \"221\": \"SR-221\", \"222\": \"SR-222\", \"223\": \"SR-223\", \"227\": \"SR-227\",\n",
    "    \"229\": \"SR-229\", \"232\": \"SR-232\", \"233\": \"SR-233\", \"236\": \"SR-236\", \"237\": \"SR-237\",\n",
    "    \"238\": \"SR-238\", \"241\": \"SR-241\", \"242\": \"SR-242\", \"243\": \"SR-243\", \"244\": \"SR-244\",\n",
    "    \"245\": \"SR-245\", \"246\": \"SR-246\", \"247\": \"SR-247\", \"253\": \"SR-253\", \"254\": \"SR-254\",\n",
    "    \"255\": \"SR-255\", \"259\": \"SR-259\", \"260\": \"SR-260\", \"261\": \"SR-261\", \"262\": \"SR-262\",\n",
    "    \"263\": \"SR-263\", \"265\": \"SR-265\", \"266\": \"SR-266\", \"267\": \"SR-267\", \"269\": \"SR-269\",\n",
    "    \"270\": \"SR-270\", \"271\": \"SR-271\", \"273\": \"SR-273\", \"275\": \"SR-275\", \"281\": \"SR-281\",\n",
    "    \"282\": \"SR-282\", \"283\": \"SR-283\", \"284\": \"SR-284\", \"299\": \"SR-299\", \"330\": \"SR-330\",\n",
    "    \"371\": \"SR-371\", \"780\": \"SR-780\", \"905\": \"SR-905\", \"6\": \"US-6\", \"50\": \"US-50\",\n",
    "    \"95\": \"US-95\", \"97\": \"US-97\", \"101\": \"US-101\", \"199\": \"US-199\", \"395\": \"US-395\"\n",
    "}\n",
    "\n",
    "# Generate extended mapping to include leading zeros\n",
    "road_mapping = {}\n",
    "for key, value in original_mapping.items():\n",
    "    road_mapping[key] = value  # Original\n",
    "    road_mapping[key.zfill(2)] = value  # 2-character zero-padded\n",
    "    road_mapping[key.zfill(3)] = value  # 3-character zero-padded\n",
    "\n",
    "#print(road_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482262aa-fbc6-478a-91ad-71ca26c8bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_path = \"gs://calitp-analytics-data/data-analyses/big_data/freight/all_permits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94dc7a2c-e5db-4a60-88b0-5c5aa4961dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"all_permits_2023_sampleset.xlsx\",\n",
    "              \"all_permits_2024_sampleset.xlsx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f234ea9-0098-47df-a456-01e9d585078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "def load_excel_sheets_1(gcs_path, file_names):\n",
    "    \"\"\"\n",
    "    Pull in the first sheet from each Excel file in GCS, add a 'year' column based on the filename,\n",
    "    and remove records with NaN values in the 'permitnumber' column. Returns a concatenated DataFrame\n",
    "    with data from all files.\n",
    "\n",
    "    Parameters:\n",
    "    gcs_path (str): The Google Cloud Storage path where the files are located.\n",
    "    file_names (list): A list of Excel file names in the GCS path.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A single concatenated DataFrame with data from all files, a 'year' column, and\n",
    "                  records with NaN values in 'permitnumber' removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a Google Cloud Storage file system object\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    \n",
    "    # List to store all DataFrames\n",
    "    df_list = []\n",
    "    \n",
    "    # Suppress any warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Suppress the specific UserWarning\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        message=\"Your application has authenticated using end user credentials from Google Cloud SDK without a quota project.\",\n",
    "        category=UserWarning,\n",
    "        module=\"google.auth._default\"\n",
    "    )\n",
    "    \n",
    "    # Define the columns to keep\n",
    "    columns_to_keep = ['permitnumber', 'year', 'permitvalidfrom', 'permitvalidto', \n",
    "                       'loaddescription', 'origin', 'destination', 'authorizedhighways']\n",
    "    \n",
    "    # Loop through each file in the file list\n",
    "    for file in file_names:\n",
    "        # Extract the year from the filename\n",
    "        year = file.split('_')[2]  # Assuming the year is the third element when split by '_'\n",
    "        \n",
    "        # Open the file and read only the first sheet\n",
    "        with fs.open(f\"{gcs_path}{file}\", 'rb') as f:\n",
    "            df = pd.read_excel(f, sheet_name=0)  # Load only the first sheet\n",
    "        \n",
    "        # Clean headers by removing spaces and making characters lowercase\n",
    "        df.columns = [col.replace(\" \", \"\").lower() for col in df.columns]\n",
    "        \n",
    "        # Add 'year' column\n",
    "        df['year'] = year\n",
    "        \n",
    "        # Filter columns and remove rows with NaN in 'permitnumber'\n",
    "        df = df[columns_to_keep].dropna(subset=['permitnumber'])\n",
    "        \n",
    "        # Append to list\n",
    "        df_list.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Parsing function to create individual route locations\n",
    "def parse_routes(route_info):\n",
    "    segments = []\n",
    "\n",
    "    # Split the data by \"from\", \"to\", or standalone dash patterns\n",
    "    raw_segments = re.split(r'\\s*-\\s*from\\s+|\\s*-\\s*to\\s+|(?<!\\s)-\\s*', route_info)\n",
    "\n",
    "    # Process each segment\n",
    "    for segment in raw_segments:\n",
    "        # Split by ' - ' or '-'\n",
    "        sub_segments = re.split(r'\\s*-\\s*|\\s*-\\s*', segment)\n",
    "        \n",
    "        # Clean and add sub-segments\n",
    "        segments.extend([sub.strip() for sub in sub_segments if sub.strip()])\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "# Custom parsing function\n",
    "def extract_location(text):\n",
    "    # Stop keywords pattern\n",
    "    stop_keywords = r\"\\b(?:dr|drive|rd|ave|way|pkwy|parkway|skyway|road|avenue|blvd|boulevard|st|street|line|lane|ln|hwy|highway)\\b\"\n",
    "\n",
    "    # If 'from' exists, process it as before\n",
    "    if \"from\" in text.lower():\n",
    "        match = re.search(r\"from\\s+(`.*?`|'.*?'|\\w+(?:\\s+\\w+)*)\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            location = match.group(1)  # Extract the text after \"from\"\n",
    "            # Keep the stop keywords and remove everything after them\n",
    "            location = re.sub(r\"(\" + stop_keywords + r\").*\", r\"\\1\", location, flags=re.IGNORECASE).strip()\n",
    "            return location\n",
    "    else:\n",
    "        # If 'from' doesn't exist, look for a stop keyword and capture location\n",
    "        match = re.search(r\"(`.*?`|'.*?'|\\w+(?:\\s+\\w+)*)\\s+(\" + stop_keywords + r\")\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            location = match.group(1)  # Capture location before stop keyword\n",
    "            return location.strip()\n",
    "\n",
    "    return None  # If no match is found\n",
    "\n",
    "\n",
    "# Function to clean each string \n",
    "def clean_route(route):\n",
    "    if not isinstance(route, str):  # Handle non-string entries\n",
    "        return route\n",
    "    \n",
    "    # Define stop keywords regex (including 'exit')\n",
    "    stop_keywords = r\"\\b(?:dr|drive|rd|ave|way|pkwy|parkway|skyway|road|avenue|blvd|boulevard|st|street|line|lane|ln|hwy|highway|exit)\\b\"\n",
    "\n",
    "    # Extract numeric highway number with optional \"rte\" or \"route\" prefixes\n",
    "    numeric_match = re.match(r\"(?:rte|route)?\\s*(\\d+)\", route, flags=re.IGNORECASE)\n",
    "    if numeric_match:\n",
    "        return numeric_match.group(1)  # Return the numeric value\n",
    "    \n",
    "    # Extract street name up to and including the first stop keyword (but excluding the stop word itself)\n",
    "    street_match = re.search(rf\"(.*?\\b{stop_keywords}(?!\\s*exit)\\b)\", route, flags=re.IGNORECASE)\n",
    "    if street_match:\n",
    "        return street_match.group(1).strip()  # Return the street name up to the stop keyword (excluding 'exit')\n",
    "    \n",
    "    # If neither is found, return the original string\n",
    "    return route\n",
    "\n",
    "# A function to parse the ['authorizedhighways'] column to get the route information\n",
    "def process_route_locations(df, parse_routes, extract_location, clean_route, road_mapping):\n",
    "    \"\"\"\n",
    "    Process a dataframe to format and clean route-related columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input dataframe to process.\n",
    "    parse_routes (function): Function to parse the 'authorizedhighways' column into route segments.\n",
    "    extract_location (function): Function to extract location from a route segment.\n",
    "    clean_route (function): Function to clean individual route location entries.\n",
    "    road_mapping (dict): Mapping dictionary for road numbers to their corresponding classes.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The processed dataframe with updated route locations and columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the authorized highways field so the text is not all capitalized\n",
    "    df['authorizedhighways'] = df['authorizedhighways'].str.capitalize()\n",
    "\n",
    "    # Apply the parsing function to create lists of individual route locations\n",
    "    df['route_segments'] = df['authorizedhighways'].apply(parse_routes)\n",
    "\n",
    "    # Determine the maximum number of locations to create the necessary columns\n",
    "    max_locations = df['route_segments'].apply(len).max()\n",
    "\n",
    "    # Create new columns for each route location based on the maximum number of locations\n",
    "    for i in range(max_locations):\n",
    "        df[f'route_location_{i}'] = df['route_segments'].apply(lambda x: x[i] if i < len(x) else None)\n",
    "\n",
    "    # Drop the temporary route_segments column\n",
    "    df.drop(columns=['route_segments'], inplace=True)\n",
    "\n",
    "    # Add a new column with all values set to \"California\"\n",
    "    df.insert(5, \"state\", \"California\")  # Index 5 corresponds to the 6th column position\n",
    "\n",
    "    # Apply title case to the 'origin' and 'destination' columns\n",
    "    df['origin'] = df['origin'].str.title()\n",
    "    df['destination'] = df['destination'].str.title()\n",
    "\n",
    "    # Apply the extract_location function to the column\n",
    "    df[\"route_location_start\"] = df[\"route_location_0\"].apply(extract_location)\n",
    "\n",
    "    # Insert the route_location_start column into the 8th position\n",
    "    df.insert(8, \"route_location_start\", df.pop(\"route_location_start\"))\n",
    "\n",
    "    # Drop the [authorizedhighways] column\n",
    "    #df.drop(columns=['authorizedhighways'], inplace=True)\n",
    "\n",
    "    # Drop the route_location_0 field\n",
    "    df = df.drop(columns=['route_location_0'])\n",
    "\n",
    "    # Identify target columns excluding \"route_location_start\"\n",
    "    route_columns = [col for col in df.columns if col.startswith(\"route_location_\") and col != \"route_location_start\"]\n",
    "\n",
    "    # Apply the clean_route cleaning function to the target columns (columns that begin with the words \"route_location\")\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].apply(clean_route)\n",
    "\n",
    "    # Iterate through each \"route_location_\" column to remove the word \"exit\"\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].apply(lambda x: str(x).replace(\"exit\", \"\").strip() if isinstance(x, str) else x)\n",
    "\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "\n",
    "    # Update the road numbers to their corresponding road class numbers\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].astype(str).map(road_mapping).fillna(df[col])  # Keep original value if no mapping found\n",
    "\n",
    "    # Create a new field called 'route_location_origin' that identifies the street and city/state\n",
    "    df['route_location_origin'] = df['route_location_start'] + \" \" + df['origin'] + \", \" + df['state']\n",
    "\n",
    "    # Move the new column (['route_location_origin_0']) to the 9th position\n",
    "    columns = list(df.columns)\n",
    "    columns.insert(9, columns.pop(columns.index('route_location_origin')))\n",
    "    df = df[columns]\n",
    "    \n",
    "    \n",
    "\n",
    "    # Remove the 'route_location_destination_city' column if it exists\n",
    "    if 'route_location_start' in df.columns:\n",
    "        df = df.drop(columns=['route_location_start'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# A function to create the ['route_intersection_x'] columns\n",
    "def process_route_intersections(df):\n",
    "    \"\"\"\n",
    "    Process a dataframe to identify and process route location and intersection columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe to process.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A cleaned dataframe with processed route intersections.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify all columns with \"route_location_\" prefix\n",
    "    route_location_cols = [col for col in df.columns if col.startswith(\"route_location_\")]\n",
    "\n",
    "    # Initialize a counter for the new intersection column names\n",
    "    intersection_counter = 0\n",
    "\n",
    "    # Create new columns for intersections\n",
    "    for i in range(len(route_location_cols) - 1):\n",
    "        col1 = route_location_cols[i]\n",
    "        col2 = route_location_cols[i + 1]\n",
    "\n",
    "        # Name the new intersection column based on the counter\n",
    "        intersection_col = f\"route_intersection_{intersection_counter}\"\n",
    "\n",
    "        # Combine adjacent columns into one field (handle None gracefully)\n",
    "        df[intersection_col] = df[col1].astype(str) + \" and \" + df[col2].astype(str)\n",
    "        df[intersection_col] = df[intersection_col].replace(\"None and None\", None)  # Optional cleanup for all-None rows\n",
    "        intersection_counter += 1\n",
    "\n",
    "    # Identify columns that start with \"route_intersection_\"\n",
    "    intersection_cols = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "\n",
    "    # Iterate over each intersection column\n",
    "    for col in intersection_cols:\n",
    "        # Replace values ending with \" & None\" with None (Null)\n",
    "        df[col] = df[col].apply(lambda x: None if isinstance(x, str) and x.endswith(\" and None\") else x)\n",
    "\n",
    "    # Identify columns that start with \"route_intersection_\"\n",
    "    intersection_cols = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "\n",
    "    # Iterate over each intersection column to remove leading zeros from numeric values\n",
    "    for col in intersection_cols:\n",
    "        # Apply the transformation to each value in the column\n",
    "        df[col] = df[col].apply(lambda x: ' and '.join([part.lstrip('0') if part.isdigit() else part for part in str(x).split(' and ')]) if isinstance(x, str) else x)\n",
    "\n",
    "    # Create a list(?) called core_columns to be included in the next iteration of the dataframe\n",
    "    core_columns = [\n",
    "        \"permitnumber\", \"year\", \"permitvalidfrom\", \"permitvalidto\",\n",
    "        \"loaddescription\", \"state\", \"origin\", \"destination\", \"authorizedhighways\", \"route_location_origin\"\n",
    "    ]\n",
    "\n",
    "    # subset_columns combines the core columns with the intersection_cols identified earlier in the script\n",
    "    subset_columns = core_columns + intersection_cols\n",
    "\n",
    "    # this next line utilizes the defined subset_columns to create a cleaned up version of the dataframe including only the columns needed for this analysis \n",
    "    df = df[subset_columns]\n",
    "\n",
    "    # Replace None values in intersection_cols with empty strings\n",
    "    for col in intersection_cols:\n",
    "        df[col] = df[col].apply(lambda x: \"\" if x is None else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to get the last 'route_intersection_x' field\n",
    "def get_last_intersection(row):\n",
    "    # Identify columns that match the pattern 'route_intersection_x'\n",
    "    intersection_columns = [col for col in df.columns if col.startswith('route_intersection_')]\n",
    "    # Get the last non-null value among these columns\n",
    "    return row[intersection_columns].dropna().iloc[-1] if intersection_columns else None\n",
    "\n",
    "def add_route_location_destination_city(df):\n",
    "    # Function to get the last 'route_intersection_x' value\n",
    "    def get_last_intersection(row):\n",
    "        # Identify columns that match the pattern 'route_intersection_x'\n",
    "        intersection_columns = [col for col in df.columns if col.startswith('route_intersection_')]\n",
    "        # Get the last non-null value among these columns\n",
    "        return row[intersection_columns].dropna().iloc[-1] if len(intersection_columns) > 0 else None\n",
    "\n",
    "    # Create the new column\n",
    "    df['route_location_destination_city'] = df.apply(\n",
    "        lambda row: f\"{get_last_intersection(row)} {row['destination']}, {row['state']}\", axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_route_intersection_last(df):\n",
    "    \"\"\"\n",
    "    Create a 'route_intersection_last' column to capture the last non-null value\n",
    "    from all 'route_intersection_x' columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe to process.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The dataframe with the new 'route_intersection_last' column.\n",
    "    \"\"\"\n",
    "    # Identify all 'route_intersection_x' columns\n",
    "    intersection_columns = [col for col in df.columns if col.startswith('route_intersection_')]\n",
    "\n",
    "    if not intersection_columns:\n",
    "        raise ValueError(\"No 'route_intersection_' columns found in the dataframe.\")\n",
    "\n",
    "    # Ensure the columns are processed in order\n",
    "    intersection_columns = sorted(intersection_columns, key=lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "    # Create 'route_intersection_last' by finding the last non-null value row-wise\n",
    "    df['route_intersection_last'] = df[intersection_columns].apply(\n",
    "        lambda row: next((val for val in reversed(row) if pd.notnull(val) and val != ''), None), axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def create_route_intersection_destination(df):\n",
    "    \"\"\"\n",
    "    Create a new column 'route_intersection_destination' by combining 'route_intersection_last' \n",
    "    and 'route_location_destination_city'. Then, clean up the column by removing extra whitespace.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input dataframe to process.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The dataframe with the new 'route_intersection_destination' column and cleaned columns.\n",
    "    \"\"\"\n",
    "    # Create the new 'route_intersection_destination' column\n",
    "    df['route_intersection_destination'] = df['route_intersection_last'] + \" \" + df['route_location_destination_city']\n",
    "    \n",
    "    # Remove extra spaces by stripping and ensuring only single spaces exist\n",
    "    df['route_intersection_destination'] = df['route_intersection_destination'].apply(\n",
    "        lambda x: \" \".join(x.split()) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "    # Drop unnecessary columns if they exist\n",
    "    df.drop(columns=['route_location_destination_city', 'route_intersection_last'], errors='ignore', inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# A function to clean \"route_intersection\" columns\n",
    "def clean_route_intersections(df):\n",
    "    # Find all columns starting with \"route_intersection_\"\n",
    "    intersection_columns = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "    \n",
    "    # Replace variations of \"imperial highway\" (e.g., \"imperial hwy\") with \"SR-99\"\n",
    "    for col in intersection_columns:\n",
    "        df[col] = df[col].str.replace(\n",
    "            r\"(?i)\\bimperial (highway|hwy)\\b\", \"SR-99\", regex=True\n",
    "        )  # Matches \"imperial highway\" or \"imperial hwy\"\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_workflow(gcs_path, file_names):\n",
    "    # Load the Excel sheets from GCS and get the initial DataFrame\n",
    "    df = load_excel_sheets_1(gcs_path, file_names)\n",
    "    \n",
    "    # Process route locations\n",
    "    df = process_route_locations(df, parse_routes, extract_location, clean_route, road_mapping)\n",
    "       \n",
    "    # Process route intersections\n",
    "    df = process_route_intersections(df)\n",
    "    \n",
    "    # Add route location destination city\n",
    "    df = add_route_location_destination_city(df)\n",
    "    \n",
    "    # Create route intersection last\n",
    "    df = create_route_intersection_last(df)\n",
    "    \n",
    "    # Create route intersection destination\n",
    "    df = create_route_intersection_destination(df)\n",
    "\n",
    "    # this is a new script - intended to help clean up the [\"route_locations_x\"] before they become intersections\n",
    "    df = clean_route_intersections(df)\n",
    "    \n",
    "    \n",
    "    # # Replace the last non-null route intersection with destination\n",
    "    # df = replace_last_non_null_intersection(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386a33a9-2319-459c-bd32-064ef8c29ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "df = process_workflow(gcs_path, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47cdd6b2-59b8-456b-a6fb-6ee15c681819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all columns starting with \"route_intersection_\"\n",
    "intersection_columns = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "\n",
    "\n",
    "def normalize_intersection(intersection_columns):\n",
    "    \"\"\"\n",
    "    Normalizes intersections by ordering highway identifiers numerically.\n",
    "    If both sides of the intersection are highways (I-, SR-, or US-),\n",
    "    ensures the smaller-numbered highway appears first.\n",
    "\n",
    "    Args:\n",
    "        intersection (str): The intersection string in the format \"Location1 and Location2\".\n",
    "\n",
    "    Returns:\n",
    "        str: The normalized intersection string, or the original string if no changes are needed.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    if not intersection_columns or pd.isna(intersection_columns):\n",
    "        return intersection_columns\n",
    "\n",
    "    parts = [part.strip() for part in intersection_columns.split(\"and\")]\n",
    "    if len(parts) != 2:\n",
    "        return intersection_columns  # Return as-is if not exactly two parts\n",
    "\n",
    "    pattern = r\"^(I-|SR-|US-)(\\d+)$\"  # Pattern to match highway identifiers\n",
    "\n",
    "    match1 = re.match(pattern, parts[0])\n",
    "    match2 = re.match(pattern, parts[1])\n",
    "\n",
    "    if match1 and match2:\n",
    "        # Extract numeric portions and compare\n",
    "        num1 = int(match1.group(2))\n",
    "        num2 = int(match2.group(2))\n",
    "\n",
    "        if num1 > num2:\n",
    "            # Swap to ensure the smaller number comes first\n",
    "            parts = [parts[1], parts[0]]\n",
    "\n",
    "    return \" and \".join(parts)\n",
    "\n",
    "\n",
    "def process_intersections(df, intersection_columns):\n",
    "    \"\"\"\n",
    "    Processes and normalizes intersections in specified columns of a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns (list): List of column names to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with normalized intersections.\n",
    "    \"\"\"\n",
    "    for col in intersection_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(normalize_intersection)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_intersections_1(df):\n",
    "    \"\"\"\n",
    "    Cleans the 'route_intersection_0' column in the DataFrame by replacing specific values.\n",
    "    \n",
    "    - Replaces values that start with \"az line\" and end with \"I-40\" with \n",
    "      \"colorado river bridge and I-10, California\".\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated DataFrame with cleaned intersections.\n",
    "    \"\"\"\n",
    "    # Check if the column exists\n",
    "    if \"route_intersection_0\" in df.columns:\n",
    "        # Debugging: Display the initial state of the column\n",
    "        #print(\"Before cleaning:\")\n",
    "        #print(df[\"route_intersection_0\"].head())\n",
    "        \n",
    "        # Define the replacement logic\n",
    "        def replace_intersection(value):\n",
    "            if isinstance(value, str) and value.lower().startswith(\"az line\") and value.endswith(\"I-40\"):\n",
    "                return \"colorado river bridge and I-10, California\"\n",
    "            return value\n",
    "\n",
    "        # Apply the function to clean the column\n",
    "        df[\"route_intersection_0\"] = df[\"route_intersection_0\"].apply(replace_intersection)\n",
    "        \n",
    "        # Debugging: Display the updated state of the column\n",
    "        #print(\"After cleaning:\")\n",
    "        #print(df[\"route_intersection_0\"].head())\n",
    "    else:\n",
    "        print(\"Column 'route_intersection_0' does not exist in the DataFrame.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def update_route_intersection(df):\n",
    "    # Create a new column with the same values as ['route_intersection_0']\n",
    "    df['route_intersection_origin'] = df['route_intersection_0']\n",
    "    \n",
    "    # Define replacement mappings\n",
    "    replacements = {\n",
    "        \"colorado river bridge and I-10, California\": \"I-10 and Arizona Line\"\n",
    "    }\n",
    "    \n",
    "    # Pattern-based replacements\n",
    "    pattern_replacements = [\n",
    "        (r'^az line.*and I-10$', \"I-10 and Arizona Line\"),\n",
    "        (r'^az line.*and I-8$', \"I-8 and Arizona Line\"),\n",
    "        (r'^az line.*and SR-62$', \"SR-62 and Arizona Line\"),\n",
    "        (r'^or line.*and I-5$', \"I-5 and Oregon Line\"),\n",
    "        (r'^or line.*and US-97$', \"US-97 and Oregon Line\"),\n",
    "        (r'^or line.*and US-395$', \"US-395 and Oregon Line\"),\n",
    "        (r'^or line.*and SR-139$', \"SR-139 and Oregon Line\"),\n",
    "        (r'^nv line.*and I-80$', \"I-80 and Nevada Line\"),\n",
    "        (r'^nv line.*and SR-15$', \"I-15 and Nevada Line\"),\n",
    "        (r'^nv line.*and US-6$', \"US-6 and Nevada Line\"),\n",
    "        (r'^nv line.*and US-395$', \"US-395 and Nevada Line\"),\n",
    "        (r'^nv line.*and SR-178$', \"SR-178 and Nevada Line\")\n",
    "    ]\n",
    "    \n",
    "    # Apply direct replacements\n",
    "    df['route_intersection_origin'] = df['route_intersection_origin'].replace(replacements)\n",
    "    \n",
    "    # Apply pattern-based replacements\n",
    "    for pattern, replacement in pattern_replacements:\n",
    "        df.loc[df['route_intersection_origin'].str.match(pattern, case=False, na=False), \n",
    "               'route_intersection_origin'] = replacement\n",
    "    \n",
    "    # Place the new column in the 10th position\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(10, cols.pop(cols.index('route_intersection_origin')))\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Remove the original column\n",
    "    df.drop(columns=['route_intersection_0'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def update_route_intersection_destination(df):\n",
    "    # Create a new column with the same values as ['route_intersection_destination']\n",
    "    df['route_intersection_destination_1'] = df['route_intersection_destination']\n",
    "    \n",
    "    # Define replacement mappings\n",
    "    replacements = {\n",
    "        \"I-40 and az line\": \"I-40 and Arizona Line\",\n",
    "        \"I-10 and az line\": \"I-10 and Arizona Line\",\n",
    "        \"I-8 and az line\": \"I-8 and Arizona Line\",\n",
    "        \"SR-62 and az line\": \"SR-62 and Arizona Line\",\n",
    "        \"I-80 and nv line\": \"I-80 and Nevada Line\",\n",
    "        \"I-5 and or line\": \"I-5 and Oregon Line\",\n",
    "        \"SR-15 and nv line\": \"SR-15 and Nevada Line\",\n",
    "        \"I-15 and nv line\": \"I-15 and Nevada Line\"\n",
    "    }\n",
    "    \n",
    "    # Pattern-based replacements\n",
    "    pattern_replacements = [\n",
    "        (r'I-40 and az line', \"I-40 and Arizona Line\"),\n",
    "        (r'I-10 and az line', \"I-10 and Arizona Line\"),\n",
    "        (r'I-8 and az line', \"I-8 and Arizona Line\"),\n",
    "        (r'SR-62 and az line', \"SR-62 and Arizona Line\"),\n",
    "        (r'I-80 and nv line', \"I-80 and Nevada Line\"),\n",
    "        (r'I-5 and or line', \"I-5 and Oregon Line\"),\n",
    "        (r'SR-15 and nv line', \"I-15 and Nevada Line\"),\n",
    "        (r'I-15 and nv line', \"I-15 and Nevada Line\")\n",
    "    ]\n",
    "    \n",
    "    # Apply direct replacements\n",
    "    df['route_intersection_destination_1'] = df['route_intersection_destination_1'].replace(replacements)\n",
    "    \n",
    "    # Apply pattern-based replacements\n",
    "    for pattern, replacement in pattern_replacements:\n",
    "        df.loc[df['route_intersection_destination_1'].str.contains(pattern, case=False, na=False), \n",
    "               'route_intersection_destination_1'] = replacement\n",
    "    \n",
    "    # Drop the original column\n",
    "    df.drop(columns=['route_intersection_destination'], inplace=True)\n",
    "    \n",
    "    # Rename the new column\n",
    "    df.rename(columns={'route_intersection_destination_1': 'route_intersection_destination'}, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e8e3284-a887-4282-9732-e8e06fe7ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_workflow_step2(df):\n",
    "    df = process_intersections(df, intersection_columns)\n",
    "    df = clean_intersections_1(df)\n",
    "    df = update_route_intersection(df)\n",
    "    df = update_route_intersection_destination(df)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf05507-43f4-480b-816f-fbefaec386da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_workflow_step2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21d07c15-a7d8-4af2-afa5-a4e6650aff3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_intersection_coordinates(df, origin_intersections, destination_intersections):\n",
    "    \"\"\"\n",
    "    Adds x_coords and y_coords for origin and destination intersections.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe.\n",
    "    origin_intersections (list of dict): List of origin intersection dictionaries.\n",
    "    destination_intersections (list of dict): List of destination intersection dictionaries.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Updated dataframe with added coordinate columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert lists of dictionaries into dictionaries for quick lookup, handling missing keys safely\n",
    "    origin_lookup = {\n",
    "        d.get(\"origin_intersection\", None): (d.get(\"x_coords\", None), d.get(\"y_coords\", None))\n",
    "        for d in origin_intersections if \"origin_intersection\" in d\n",
    "    }\n",
    "\n",
    "    destination_lookup = {\n",
    "        d.get(\"destination_intersection\", None): (d.get(\"x_coords\", None), d.get(\"y_coords\", None))\n",
    "        for d in destination_intersections if \"destination_intersection\" in d\n",
    "    }\n",
    "\n",
    "    #Extract origin coordinates\n",
    "    df[\"route_intersection_origin_x_coords\"] = df[\"route_intersection_origin\"].map(\n",
    "        lambda x: origin_lookup.get(x, (None, None))[0]\n",
    "    )\n",
    "    df[\"route_intersection_origin_y_coords\"] = df[\"route_intersection_origin\"].map(\n",
    "        lambda x: origin_lookup.get(x, (None, None))[1]\n",
    "    )\n",
    "\n",
    "    # Insert new columns at position 11 and 12\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(11, cols.pop(cols.index(\"route_intersection_origin_x_coords\")))\n",
    "    cols.insert(12, cols.pop(cols.index(\"route_intersection_origin_y_coords\")))\n",
    "    df = df[cols]\n",
    "\n",
    "    # Extract destination coordinates\n",
    "    df[\"route_intersection_destination_x_coords\"] = df[\"route_intersection_destination\"].map(\n",
    "        lambda x: destination_lookup.get(x, (None, None))[0]\n",
    "    )\n",
    "    df[\"route_intersection_destination_y_coords\"] = df[\"route_intersection_destination\"].map(\n",
    "        lambda x: destination_lookup.get(x, (None, None))[1]\n",
    "    )\n",
    "\n",
    "    # Move the new destination coordinate columns to be right after the last column\n",
    "    last_col_index = df.columns.get_loc(\"route_intersection_destination\") + 1\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(last_col_index, cols.pop(cols.index(\"route_intersection_destination_x_coords\")))\n",
    "    cols.insert(last_col_index + 1, cols.pop(cols.index(\"route_intersection_destination_y_coords\")))\n",
    "    df = df[cols]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4654e815-ee6f-4f99-9d40-74f77f18efd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = add_intersection_coordinates(df, origin_intersections, destination_intersections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ac6621f-41d9-4675-9b60-e96b1078cebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"justlooking_subset_v9.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8821ae-7df5-4df5-9466-c25ca3ffcb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9efedc0-51fe-4f63-b73d-ca579bce884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mid_route_coordinates(df):\n",
    "    \"\"\"\n",
    "    Adds 'x_coords' and 'y_coords' fields next to each mid-route intersection \n",
    "    and moves all route_intersection-related columns to the end.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe with multiple route_intersection_X fields.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Updated dataframe with coordinate fields added next to each intersection.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a lookup dictionary for SHS intersections (uppercase, removing \", California\")\n",
    "    shs_lookup = {\n",
    "        d.get(\"shs_intersection\", \"\").strip().upper().replace(\", CALIFORNIA\", \"\"): \n",
    "        (d.get(\"x_coords\", None), d.get(\"y_coords\", None))\n",
    "        for d in shs_intersections if \"shs_intersection\" in d\n",
    "    }\n",
    "\n",
    "    # Identify mid-route intersection columns (excluding origin and destination)\n",
    "    mid_route_cols = sorted(\n",
    "        [col for col in df.columns if col.startswith(\"route_intersection_\") and col.split(\"_\")[-1].isdigit()],\n",
    "        key=lambda x: int(x.split(\"_\")[-1])  # Sort numerically\n",
    "    )\n",
    "\n",
    "    # Format DataFrame values for lookup (uppercase + remove double spaces)\n",
    "    for col in mid_route_cols:\n",
    "        df[col] = df[col].astype(str).str.strip().str.upper().replace(\" ,\", \",\").replace(\"  \", \" \")\n",
    "\n",
    "    # Add new coordinate columns (convert df values to uppercase for correct lookup)\n",
    "    for col in mid_route_cols:\n",
    "        formatted_col = df[col].apply(lambda x: x.replace(\", CALIFORNIA\", \"\").strip().upper() if isinstance(x, str) else None)\n",
    "        df[f\"{col}_x_coords\"] = formatted_col.apply(lambda x: shs_lookup.get(x, (None, None))[0] if x in shs_lookup else None)\n",
    "        df[f\"{col}_y_coords\"] = formatted_col.apply(lambda x: shs_lookup.get(x, (None, None))[1] if x in shs_lookup else None)\n",
    "\n",
    "    # Define route-related columns to move to the end\n",
    "    route_cols = []\n",
    "    for col in mid_route_cols:\n",
    "        route_cols.append(col)\n",
    "        route_cols.append(f\"{col}_x_coords\")\n",
    "        route_cols.append(f\"{col}_y_coords\")\n",
    "\n",
    "    # Add origin and destination fields to the route-related list\n",
    "    route_cols = (\n",
    "        [\"route_intersection_origin\", \"route_intersection_origin_x_coords\", \"route_intersection_origin_y_coords\"]\n",
    "        + route_cols\n",
    "        + [\"route_intersection_destination\", \"route_intersection_destination_x_coords\", \"route_intersection_destination_y_coords\"]\n",
    "    )\n",
    "\n",
    "    # Identify non-route columns (to keep at the front)\n",
    "    non_route_cols = [col for col in df.columns if col not in route_cols]\n",
    "\n",
    "    # Ensure all columns exist before reordering\n",
    "    final_col_order = non_route_cols + [col for col in route_cols if col in df.columns]\n",
    "    df = df[final_col_order]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d496ea0-a05c-4b59-85b8-2b84a2f087c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_mid_route_coordinates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49b932b4-dde4-454b-afd4-ff6118703aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"justlooking_subset_v11.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276a863-4648-4f8c-80e4-c2d5ef7aedc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f80a7-d7d0-494d-abad-074d57c3a455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
