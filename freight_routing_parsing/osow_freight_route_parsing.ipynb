{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f2d5a2c-4b02-44f3-af2d-0febddc5fbc6",
   "metadata": {},
   "source": [
    "# parsing the ['authorizedhighways'] column in the all_permits data for OSOW vehicle permits\n",
    "- ns\n",
    "\n",
    "- Originally requested by Stephen Yoon  \n",
    "    - data provided by Stephen's office\n",
    "- next step ==   \n",
    "    - get the coordinates for the intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acace08e-1139-4d26-876f-0101415d1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import gcsfs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e08b8f81-5d63-4b8f-a1b1-c9ab9565f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original_mapping is needed to standardize the highway names for the various records\n",
    "original_mapping = {\n",
    "    \"5\": \"I5\", \"10\": \"I10\", \"15\": \"I15\", \"40\": \"I40\", \"80\": \"I80\", \"105\": \"I105\", \"110\": \"I110\",\n",
    "    \"205\": \"I205\", \"210\": \"I210\", \"215\": \"I215\", \"280\": \"I280\", \"380\": \"I380\", \"405\": \"I405\",\n",
    "    \"505\": \"I505\", \"580\": \"I580\", \"605\": \"I605\", \"680\": \"I680\", \"710\": \"I710\", \"805\": \"I805\",\n",
    "    \"880\": \"I880\", \"980\": \"I980\", \"1\": \"SR1\", \"2\": \"SR2\", \"3\": \"SR3\", \"4\": \"SR4\", \"7\": \"SR7\",\n",
    "    \"8\": \"SR8\", \"9\": \"SR9\", \"11\": \"SR11\", \"12\": \"SR12\", \"13\": \"SR13\", \"14\": \"SR14\", \"15\": \"SR15\",\n",
    "    \"16\": \"SR16\", \"17\": \"SR17\", \"18\": \"SR18\", \"20\": \"SR20\", \"22\": \"SR22\", \"23\": \"SR23\",\n",
    "    \"24\": \"SR24\", \"25\": \"SR25\", \"26\": \"SR26\", \"27\": \"SR27\", \"28\": \"SR28\", \"29\": \"SR29\",\n",
    "    \"32\": \"SR32\", \"33\": \"SR33\", \"34\": \"SR34\", \"35\": \"SR35\", \"36\": \"SR36\", \"37\": \"SR37\",\n",
    "    \"38\": \"SR38\", \"39\": \"SR39\", \"41\": \"SR41\", \"43\": \"SR43\", \"44\": \"SR44\", \"45\": \"SR45\",\n",
    "    \"46\": \"SR46\", \"47\": \"SR47\", \"49\": \"SR49\", \"51\": \"SR51\", \"52\": \"SR52\", \"53\": \"SR53\",\n",
    "    \"54\": \"SR54\", \"55\": \"SR55\", \"56\": \"SR56\", \"57\": \"SR57\", \"58\": \"SR58\", \"59\": \"SR59\",\n",
    "    \"60\": \"SR60\", \"61\": \"SR61\", \"62\": \"SR62\", \"63\": \"SR63\", \"65\": \"SR65\", \"66\": \"SR66\",\n",
    "    \"67\": \"SR67\", \"68\": \"SR68\", \"70\": \"SR70\", \"71\": \"SR71\", \"72\": \"SR72\", \"73\": \"SR73\",\n",
    "    \"74\": \"SR74\", \"75\": \"SR75\", \"76\": \"SR76\", \"77\": \"SR77\", \"78\": \"SR78\", \"79\": \"SR79\",\n",
    "    \"82\": \"SR82\", \"83\": \"SR83\", \"84\": \"SR84\", \"85\": \"SR85\", \"86\": \"SR86\", \"87\": \"SR87\",\n",
    "    \"88\": \"SR88\", \"89\": \"SR89\", \"90\": \"SR90\", \"91\": \"SR91\", \"92\": \"SR92\", \"94\": \"SR94\",\n",
    "    \"96\": \"SR96\", \"98\": \"SR98\", \"99\": \"SR99\", \"103\": \"SR103\", \"104\": \"SR104\", \"107\": \"SR107\",\n",
    "    \"108\": \"SR108\", \"109\": \"SR109\", \"110\": \"SR110\", \"111\": \"SR111\", \"112\": \"SR112\",\n",
    "    \"113\": \"SR113\", \"114\": \"SR114\", \"115\": \"SR115\", \"116\": \"SR116\", \"118\": \"SR118\",\n",
    "    \"119\": \"SR119\", \"120\": \"SR120\", \"121\": \"SR121\", \"123\": \"SR123\", \"124\": \"SR124\",\n",
    "    \"125\": \"SR125\", \"126\": \"SR126\", \"127\": \"SR127\", \"128\": \"SR128\", \"129\": \"SR129\",\n",
    "    \"130\": \"SR130\", \"131\": \"SR131\", \"132\": \"SR132\", \"133\": \"SR133\", \"134\": \"SR134\",\n",
    "    \"135\": \"SR135\", \"136\": \"SR136\", \"137\": \"SR137\", \"138\": \"SR138\", \"139\": \"SR139\",\n",
    "    \"140\": \"SR140\", \"142\": \"SR142\", \"144\": \"SR144\", \"145\": \"SR145\", \"146\": \"SR146\",\n",
    "    \"147\": \"SR147\", \"149\": \"SR149\", \"150\": \"SR150\", \"151\": \"SR151\", \"152\": \"SR152\",\n",
    "    \"153\": \"SR153\", \"154\": \"SR154\", \"155\": \"SR155\", \"156\": \"SR156\", \"158\": \"SR158\",\n",
    "    \"160\": \"SR160\", \"161\": \"SR161\", \"162\": \"SR162\", \"163\": \"SR163\", \"164\": \"SR164\",\n",
    "    \"165\": \"SR165\", \"166\": \"SR166\", \"167\": \"SR167\", \"168\": \"SR168\", \"169\": \"SR169\",\n",
    "    \"170\": \"SR170\", \"172\": \"SR172\", \"173\": \"SR173\", \"174\": \"SR174\", \"175\": \"SR175\",\n",
    "    \"177\": \"SR177\", \"178\": \"SR178\", \"180\": \"SR180\", \"182\": \"SR182\", \"183\": \"SR183\",\n",
    "    \"184\": \"SR184\", \"185\": \"SR185\", \"186\": \"SR186\", \"187\": \"SR187\", \"188\": \"SR188\",\n",
    "    \"189\": \"SR189\", \"190\": \"SR190\", \"191\": \"SR191\", \"192\": \"SR192\", \"193\": \"SR193\",\n",
    "    \"197\": \"SR197\", \"198\": \"SR198\", \"200\": \"SR200\", \"201\": \"SR201\", \"202\": \"SR202\",\n",
    "    \"203\": \"SR203\", \"204\": \"SR204\", \"207\": \"SR207\", \"210\": \"SR210\", \"211\": \"SR211\",\n",
    "    \"213\": \"SR213\", \"216\": \"SR216\", \"217\": \"SR217\", \"218\": \"SR218\", \"219\": \"SR219\",\n",
    "    \"220\": \"SR220\", \"221\": \"SR221\", \"222\": \"SR222\", \"223\": \"SR223\", \"227\": \"SR227\",\n",
    "    \"229\": \"SR229\", \"232\": \"SR232\", \"233\": \"SR233\", \"236\": \"SR236\", \"237\": \"SR237\",\n",
    "    \"238\": \"SR238\", \"241\": \"SR241\", \"242\": \"SR242\", \"243\": \"SR243\", \"244\": \"SR244\",\n",
    "    \"245\": \"SR245\", \"246\": \"SR246\", \"247\": \"SR247\", \"253\": \"SR253\", \"254\": \"SR254\",\n",
    "    \"255\": \"SR255\", \"259\": \"SR259\", \"260\": \"SR260\", \"261\": \"SR261\", \"262\": \"SR262\",\n",
    "    \"263\": \"SR263\", \"265\": \"SR265\", \"266\": \"SR266\", \"267\": \"SR267\", \"269\": \"SR269\",\n",
    "    \"270\": \"SR270\", \"271\": \"SR271\", \"273\": \"SR273\", \"275\": \"SR275\", \"281\": \"SR281\",\n",
    "    \"282\": \"SR282\", \"283\": \"SR283\", \"284\": \"SR284\", \"299\": \"SR299\", \"330\": \"SR330\",\n",
    "    \"371\": \"SR371\", \"780\": \"SR780\", \"905\": \"SR905\", \"6\": \"US6\", \"50\": \"US50\",\n",
    "    \"95\": \"US95\", \"97\": \"US97\", \"101\": \"US101\", \"199\": \"US199\", \"395\": \"US395\"\n",
    "}\n",
    "\n",
    "# Generate extended mapping to include leading zeros\n",
    "road_mapping = {}\n",
    "for key, value in original_mapping.items():\n",
    "    road_mapping[key] = value  # Original\n",
    "    road_mapping[key.zfill(2)] = value  # 2-character zero-padded\n",
    "    road_mapping[key.zfill(3)] = value  # 3-character zero-padded\n",
    "\n",
    "#print(road_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "482262aa-fbc6-478a-91ad-71ca26c8bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_path = \"gs://calitp-analytics-data/data-analyses/big_data/freight/all_permits/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94dc7a2c-e5db-4a60-88b0-5c5aa4961dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\"all_permits_2023_sampleset.xlsx\",\n",
    "              \"all_permits_2024_sampleset.xlsx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f234ea9-0098-47df-a456-01e9d585078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_sheets_1(gcs_path, file_names):\n",
    "    \"\"\"\n",
    "    Pull in the first sheet from each Excel file in GCS, add a 'year' column based on the filename,\n",
    "    and remove records with NaN values in the 'permitnumber' column. Returns a concatenated DataFrame\n",
    "    with data from all files.\n",
    "\n",
    "    Parameters:\n",
    "    gcs_path (str): The Google Cloud Storage path where the files are located.\n",
    "    file_names (list): A list of Excel file names in the GCS path.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A single concatenated DataFrame with data from all files, a 'year' column, and\n",
    "                  records with NaN values in 'permitnumber' removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a Google Cloud Storage file system object\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    \n",
    "    # List to store all DataFrames\n",
    "    df_list = []\n",
    "    \n",
    "    # Suppress any warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Suppress the specific UserWarning\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        message=\"Your application has authenticated using end user credentials from Google Cloud SDK without a quota project.\",\n",
    "        category=UserWarning,\n",
    "        module=\"google.auth._default\"\n",
    "    )\n",
    "    \n",
    "    # Define the columns to keep\n",
    "    columns_to_keep = ['permitnumber', 'year', 'permitvalidfrom', 'permitvalidto', \n",
    "                       'loaddescription', 'origin', 'destination', 'authorizedhighways']\n",
    "    \n",
    "    # Loop through each file in the file list\n",
    "    for file in file_names:\n",
    "        # Extract the year from the filename\n",
    "        year = file.split('_')[2]  # Assuming the year is the third element when split by '_'\n",
    "        \n",
    "        # Open the file and read only the first sheet\n",
    "        with fs.open(f\"{gcs_path}{file}\", 'rb') as f:\n",
    "            df = pd.read_excel(f, sheet_name=0)  # Load only the first sheet\n",
    "        \n",
    "        # Clean headers by removing spaces and making characters lowercase\n",
    "        df.columns = [col.replace(\" \", \"\").lower() for col in df.columns]\n",
    "        \n",
    "        # Add 'year' column\n",
    "        df['year'] = year\n",
    "        \n",
    "        # Filter columns and remove rows with NaN in 'permitnumber'\n",
    "        df = df[columns_to_keep].dropna(subset=['permitnumber'])\n",
    "        \n",
    "        # Append to list\n",
    "        df_list.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    final_df = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Parsing function to create individual route locations\n",
    "def parse_routes(route_info):\n",
    "    segments = []\n",
    "\n",
    "    # Split the data by \"from\", \"to\", or standalone dash patterns\n",
    "    raw_segments = re.split(r'\\s*-\\s*from\\s+|\\s*-\\s*to\\s+|(?<!\\s)-\\s*', route_info)\n",
    "\n",
    "    # Process each segment\n",
    "    for segment in raw_segments:\n",
    "        # Split by ' - ' or '-'\n",
    "        sub_segments = re.split(r'\\s*-\\s*|\\s*-\\s*', segment)\n",
    "        \n",
    "        # Clean and add sub-segments\n",
    "        segments.extend([sub.strip() for sub in sub_segments if sub.strip()])\n",
    "\n",
    "    return segments\n",
    "\n",
    "\n",
    "# Custom parsing function\n",
    "def extract_location(text):\n",
    "    # Stop keywords pattern\n",
    "    stop_keywords = r\"\\b(?:dr|drive|rd|ave|way|pkwy|parkway|skyway|road|avenue|blvd|boulevard|st|street|line|lane|ln|hwy|highway)\\b\"\n",
    "\n",
    "    # If 'from' exists, process it as before\n",
    "    if \"from\" in text.lower():\n",
    "        match = re.search(r\"from\\s+(`.*?`|'.*?'|\\w+(?:\\s+\\w+)*)\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            location = match.group(1)  # Extract the text after \"from\"\n",
    "            # Keep the stop keywords and remove everything after them\n",
    "            location = re.sub(r\"(\" + stop_keywords + r\").*\", r\"\\1\", location, flags=re.IGNORECASE).strip()\n",
    "            return location\n",
    "    else:\n",
    "        # If 'from' doesn't exist, look for a stop keyword and capture location\n",
    "        match = re.search(r\"(`.*?`|'.*?'|\\w+(?:\\s+\\w+)*)\\s+(\" + stop_keywords + r\")\", text, re.IGNORECASE)\n",
    "        if match:\n",
    "            location = match.group(1)  # Capture location before stop keyword\n",
    "            return location.strip()\n",
    "\n",
    "    return None  # If no match is found\n",
    "\n",
    "\n",
    "# Function to clean each string \n",
    "def clean_route(route):\n",
    "    if not isinstance(route, str):  # Handle non-string entries\n",
    "        return route\n",
    "    \n",
    "    # Define stop keywords regex (including 'exit')\n",
    "    stop_keywords = r\"\\b(?:dr|drive|rd|ave|way|pkwy|parkway|skyway|road|avenue|blvd|boulevard|st|street|line|lane|ln|hwy|highway|exit)\\b\"\n",
    "\n",
    "    # Extract numeric highway number with optional \"rte\" or \"route\" prefixes\n",
    "    numeric_match = re.match(r\"(?:rte|route)?\\s*(\\d+)\", route, flags=re.IGNORECASE)\n",
    "    if numeric_match:\n",
    "        return numeric_match.group(1)  # Return the numeric value\n",
    "    \n",
    "    # Extract street name up to and including the first stop keyword (but excluding the stop word itself)\n",
    "    street_match = re.search(rf\"(.*?\\b{stop_keywords}(?!\\s*exit)\\b)\", route, flags=re.IGNORECASE)\n",
    "    if street_match:\n",
    "        return street_match.group(1).strip()  # Return the street name up to the stop keyword (excluding 'exit')\n",
    "    \n",
    "    # If neither is found, return the original string\n",
    "    return route\n",
    "\n",
    "# A function to parse the ['authorizedhighways'] column to get the route information\n",
    "def process_route_locations(df, parse_routes, extract_location, clean_route, road_mapping):\n",
    "    \"\"\"\n",
    "    Process a dataframe to format and clean route-related columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input dataframe to process.\n",
    "    parse_routes (function): Function to parse the 'authorizedhighways' column into route segments.\n",
    "    extract_location (function): Function to extract location from a route segment.\n",
    "    clean_route (function): Function to clean individual route location entries.\n",
    "    road_mapping (dict): Mapping dictionary for road numbers to their corresponding classes.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The processed dataframe with updated route locations and columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the authorized highways field so the text is not all capitalized\n",
    "    df['authorizedhighways'] = df['authorizedhighways'].str.capitalize()\n",
    "\n",
    "    # Apply the parsing function to create lists of individual route locations\n",
    "    df['route_segments'] = df['authorizedhighways'].apply(parse_routes)\n",
    "\n",
    "    # Determine the maximum number of locations to create the necessary columns\n",
    "    max_locations = df['route_segments'].apply(len).max()\n",
    "\n",
    "    # Create new columns for each route location based on the maximum number of locations\n",
    "    for i in range(max_locations):\n",
    "        df[f'route_location_{i}'] = df['route_segments'].apply(lambda x: x[i] if i < len(x) else None)\n",
    "\n",
    "    # Drop the temporary route_segments column\n",
    "    df.drop(columns=['route_segments'], inplace=True)\n",
    "\n",
    "    # Add a new column with all values set to \"California\"\n",
    "    df.insert(5, \"state\", \"California\")  # Index 5 corresponds to the 6th column position\n",
    "\n",
    "    # Apply title case to the 'origin' and 'destination' columns\n",
    "    df['origin'] = df['origin'].str.title()\n",
    "    df['destination'] = df['destination'].str.title()\n",
    "\n",
    "    # Apply the extract_location function to the column\n",
    "    df[\"route_location_start\"] = df[\"route_location_0\"].apply(extract_location)\n",
    "\n",
    "    # Insert the route_location_start column into the 8th position\n",
    "    df.insert(8, \"route_location_start\", df.pop(\"route_location_start\"))\n",
    "\n",
    "    # Drop the [authorizedhighways] column\n",
    "    #df.drop(columns=['authorizedhighways'], inplace=True)\n",
    "\n",
    "    # Drop the route_location_0 field\n",
    "    df = df.drop(columns=['route_location_0'])\n",
    "\n",
    "    # Identify target columns excluding \"route_location_start\"\n",
    "    route_columns = [col for col in df.columns if col.startswith(\"route_location_\") and col != \"route_location_start\"]\n",
    "\n",
    "    # Apply the clean_route cleaning function to the target columns (columns that begin with the words \"route_location\")\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].apply(clean_route)\n",
    "\n",
    "    # Iterate through each \"route_location_\" column to remove the word \"exit\"\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].apply(lambda x: str(x).replace(\"exit\", \"\").strip() if isinstance(x, str) else x)\n",
    "\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].astype(str)\n",
    "\n",
    "    # Update the road numbers to their corresponding road class numbers\n",
    "    for col in route_columns:\n",
    "        df[col] = df[col].astype(str).map(road_mapping).fillna(df[col])  # Keep original value if no mapping found\n",
    "\n",
    "    # Create a new field called 'route_location_origin' that identifies the street and city/state\n",
    "    df['route_location_origin'] = df['route_location_start'] + \" \" + df['origin'] + \", \" + df['state']\n",
    "\n",
    "    # Move the new column (['route_location_origin']) to the 9th position\n",
    "    columns = list(df.columns)\n",
    "    columns.insert(9, columns.pop(columns.index('route_location_origin')))\n",
    "    df = df[columns]\n",
    "\n",
    "    # Remove the 'route_location_destination_city' column if it exists\n",
    "    if 'route_location_start' in df.columns:\n",
    "        df = df.drop(columns=['route_location_start'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# A function to create the ['route_intersection_x'] columns\n",
    "def process_route_intersections(df):\n",
    "    \"\"\"\n",
    "    Process a dataframe to identify and process route location and intersection columns.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe to process.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A cleaned dataframe with processed route intersections.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify all columns with \"route_location_\" prefix\n",
    "    route_location_cols = [col for col in df.columns if col.startswith(\"route_location_\")]\n",
    "\n",
    "    # Initialize a counter for the new intersection column names\n",
    "    intersection_counter = 0\n",
    "\n",
    "    # Create new columns for intersections\n",
    "    for i in range(len(route_location_cols) - 1):\n",
    "        col1 = route_location_cols[i]\n",
    "        col2 = route_location_cols[i + 1]\n",
    "\n",
    "        # Name the new intersection column based on the counter\n",
    "        intersection_col = f\"route_intersection_{intersection_counter}\"\n",
    "\n",
    "        # Combine adjacent columns into one field (handle None gracefully)\n",
    "        df[intersection_col] = df[col1].astype(str) + \" & \" + df[col2].astype(str)\n",
    "        df[intersection_col] = df[intersection_col].replace(\"None & None\", None)  # Optional cleanup for all-None rows\n",
    "        intersection_counter += 1\n",
    "\n",
    "    # Identify columns that start with \"route_intersection_\"\n",
    "    intersection_cols = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "\n",
    "    # Iterate over each intersection column\n",
    "    for col in intersection_cols:\n",
    "        # Replace values ending with \" & None\" with None (Null)\n",
    "        df[col] = df[col].apply(lambda x: None if isinstance(x, str) and x.endswith(\" & None\") else x)\n",
    "\n",
    "    # Identify columns that start with \"route_intersection_\"\n",
    "    intersection_cols = [col for col in df.columns if col.startswith(\"route_intersection_\")]\n",
    "\n",
    "    # Iterate over each intersection column to remove leading zeros from numeric values\n",
    "    for col in intersection_cols:\n",
    "        # Apply the transformation to each value in the column\n",
    "        df[col] = df[col].apply(lambda x: ' & '.join([part.lstrip('0') if part.isdigit() else part for part in str(x).split(' & ')]) if isinstance(x, str) else x)\n",
    "\n",
    "    # Create a list(?) called core_columns to be included in the next iteration of the dataframe\n",
    "    core_columns = [\n",
    "        \"permitnumber\", \"year\", \"permitvalidfrom\", \"permitvalidto\",\n",
    "        \"loaddescription\", \"state\", \"origin\", \"destination\", \"authorizedhighways\", \"route_location_origin\"\n",
    "    ]\n",
    "\n",
    "    # subset_columns combines the core columns with the intersection_cols identified earlier in the script\n",
    "    subset_columns = core_columns + intersection_cols\n",
    "\n",
    "    # this next line utilizes the defined subset_columns to create a cleaned up version of the dataframe including only the columns needed for this analysis \n",
    "    df = df[subset_columns]\n",
    "\n",
    "    # Replace None values in intersection_cols with empty strings\n",
    "    for col in intersection_cols:\n",
    "        df[col] = df[col].apply(lambda x: \"\" if x is None else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to get the last 'route_intersection_x' field\n",
    "def get_last_intersection(row):\n",
    "    # Identify columns that match the pattern 'route_intersection_x'\n",
    "    intersection_columns = [col for col in df.columns if col.startswith('route_intersection_')]\n",
    "    # Get the last non-null value among these columns\n",
    "    return row[intersection_columns].dropna().iloc[-1] if intersection_columns else None\n",
    "\n",
    "def add_route_location_destination_city(df):\n",
    "    # Function to get the last 'route_intersection_x' value\n",
    "    def get_last_intersection(row):\n",
    "        # Identify columns that match the pattern 'route_intersection_x'\n",
    "        intersection_columns = [col for col in df.columns if col.startswith('route_intersection_')]\n",
    "        # Get the last non-null value among these columns\n",
    "        return row[intersection_columns].dropna().iloc[-1] if len(intersection_columns) > 0 else None\n",
    "\n",
    "    # Create the new column\n",
    "    df['route_location_destination_city'] = df.apply(\n",
    "        lambda row: f\"{get_last_intersection(row)} {row['destination']}, {row['state']}\", axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_route_intersection_last(df):\n",
    "    \"\"\"\n",
    "    Create a 'route_intersection_last' column to capture the last non-null value\n",
    "    from all 'route_intersection_x' columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe to process.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The dataframe with the new 'route_intersection_last' column.\n",
    "    \"\"\"\n",
    "    # Identify all 'route_intersection_x' columns\n",
    "    intersection_columns = [col for col in df.columns if col.startswith('route_intersection_')]\n",
    "\n",
    "    if not intersection_columns:\n",
    "        raise ValueError(\"No 'route_intersection_' columns found in the dataframe.\")\n",
    "\n",
    "    # Ensure the columns are processed in order\n",
    "    intersection_columns = sorted(intersection_columns, key=lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "    # Create 'route_intersection_last' by finding the last non-null value row-wise\n",
    "    df['route_intersection_last'] = df[intersection_columns].apply(\n",
    "        lambda row: next((val for val in reversed(row) if pd.notnull(val) and val != ''), None), axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_route_intersection_destination(df):\n",
    "    \"\"\"\n",
    "    Create a new column 'route_intersection_destination' by combining 'route_intersection_last' \n",
    "    and 'route_location_destination_city'. Then, clean up the column by removing extra whitespace.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input dataframe to process.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: The dataframe with the new 'route_intersection_destination' column and cleaned columns.\n",
    "    \"\"\"\n",
    "    # Create the new 'route_intersection_destination' column\n",
    "    df['route_intersection_destination'] = df['route_intersection_last'] + \" \" + df['route_location_destination_city']\n",
    "    \n",
    "    # Remove any extra whitespace or empty strings in the resulting combination\n",
    "    df['route_intersection_destination'] = df['route_intersection_destination'].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    # Drop the 'route_location_destination_city' column if it exists\n",
    "    if 'route_location_destination_city' in df.columns:\n",
    "        df.drop(columns=['route_location_destination_city'], inplace=True)\n",
    "\n",
    "    # Drop the 'route_intersection_last' column if it exists\n",
    "    if 'route_intersection_last' in df.columns:\n",
    "        df.drop(columns=['route_intersection_last'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_workflow(gcs_path, file_names):\n",
    "    # Load the Excel sheets from GCS and get the initial DataFrame\n",
    "    df = load_excel_sheets_1(gcs_path, file_names)\n",
    "    \n",
    "    # Process route locations\n",
    "    df = process_route_locations(df, parse_routes, extract_location, clean_route, road_mapping)\n",
    "    \n",
    "    # Process route intersections\n",
    "    df = process_route_intersections(df)\n",
    "    \n",
    "    # Add route location destination city\n",
    "    df = add_route_location_destination_city(df)\n",
    "    \n",
    "    # Create route intersection last\n",
    "    df = create_route_intersection_last(df)\n",
    "    \n",
    "    # Create route intersection destination\n",
    "    df = create_route_intersection_destination(df)\n",
    "    \n",
    "    # # Replace the last non-null route intersection with destination\n",
    "    # df = replace_last_non_null_intersection(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e8e3284-a887-4282-9732-e8e06fe7ba8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/google/auth/_default.py:78: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "df = process_workflow(gcs_path, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db95d3c6-b34f-4992-82bf-0af1e4ef7c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permitnumber</th>\n",
       "      <th>year</th>\n",
       "      <th>permitvalidfrom</th>\n",
       "      <th>permitvalidto</th>\n",
       "      <th>loaddescription</th>\n",
       "      <th>state</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>authorizedhighways</th>\n",
       "      <th>route_location_origin</th>\n",
       "      <th>...</th>\n",
       "      <th>route_intersection_15</th>\n",
       "      <th>route_intersection_16</th>\n",
       "      <th>route_intersection_17</th>\n",
       "      <th>route_intersection_18</th>\n",
       "      <th>route_intersection_19</th>\n",
       "      <th>route_intersection_20</th>\n",
       "      <th>route_intersection_21</th>\n",
       "      <th>route_intersection_22</th>\n",
       "      <th>route_intersection_23</th>\n",
       "      <th>route_intersection_destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e23-013125</td>\n",
       "      <td>2023</td>\n",
       "      <td>02/15/2023</td>\n",
       "      <td>02/21/2023</td>\n",
       "      <td>75' KELLY BAR</td>\n",
       "      <td>California</td>\n",
       "      <td>Hayward</td>\n",
       "      <td>Antelope</td>\n",
       "      <td>* from clawiter rd s/b on ramp - 092e - 880n -...</td>\n",
       "      <td>clawiter rd Hayward, California</td>\n",
       "      <td>...</td>\n",
       "      <td>I80 &amp; antelope rd</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>I80 &amp; antelope rd  Antelope, California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e23-021610</td>\n",
       "      <td>2023</td>\n",
       "      <td>03/20/2023</td>\n",
       "      <td>03/26/2023</td>\n",
       "      <td>UNLADEN 9 AXLE WITH 2 DECK INSERTS</td>\n",
       "      <td>California</td>\n",
       "      <td>Fontana</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>* from sierra ave w/b on ramp - 015s - 060w - ...</td>\n",
       "      <td>sierra ave Fontana, California</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SR60 &amp; vineyard ave  Ontario, California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e23-022752</td>\n",
       "      <td>2023</td>\n",
       "      <td>03/22/2023</td>\n",
       "      <td>03/28/2023</td>\n",
       "      <td>M95 TRACKED CONVEYOR</td>\n",
       "      <td>California</td>\n",
       "      <td>Dixon</td>\n",
       "      <td>Fresno</td>\n",
       "      <td>* from industrial way - 113n - 080w - 680s - 5...</td>\n",
       "      <td>industrial way Dixon, California</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SR180 &amp; north floyd ave  Fresno, California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e23-036568</td>\n",
       "      <td>2023</td>\n",
       "      <td>05/05/2023</td>\n",
       "      <td>05/11/2023</td>\n",
       "      <td>5 TROWEL MACHINES (END TO END) &amp; MISC LEGAL FR...</td>\n",
       "      <td>California</td>\n",
       "      <td>Elk Grove</td>\n",
       "      <td>Ca/Nv Border</td>\n",
       "      <td>* from grant line rd w/b on ramp - 099n - 051n...</td>\n",
       "      <td>grant line Elk Grove, California</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>I80 &amp; nv line  Ca/Nv Border, California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e23-017248</td>\n",
       "      <td>2023</td>\n",
       "      <td>03/02/2023</td>\n",
       "      <td>03/08/2023</td>\n",
       "      <td>150H GRADER</td>\n",
       "      <td>California</td>\n",
       "      <td>Fairfield</td>\n",
       "      <td>Saratoga</td>\n",
       "      <td>* from air base pkwy n/b on ramp - 080w - 680s...</td>\n",
       "      <td>air base pkwy Fairfield, California</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>SR17 &amp; scotts valley dr  Saratoga, California</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  permitnumber  year permitvalidfrom permitvalidto  \\\n",
       "0   e23-013125  2023      02/15/2023    02/21/2023   \n",
       "1   e23-021610  2023      03/20/2023    03/26/2023   \n",
       "2   e23-022752  2023      03/22/2023    03/28/2023   \n",
       "3   e23-036568  2023      05/05/2023    05/11/2023   \n",
       "4   e23-017248  2023      03/02/2023    03/08/2023   \n",
       "\n",
       "                                     loaddescription       state     origin  \\\n",
       "0                                      75' KELLY BAR  California    Hayward   \n",
       "1                 UNLADEN 9 AXLE WITH 2 DECK INSERTS  California    Fontana   \n",
       "2                               M95 TRACKED CONVEYOR  California      Dixon   \n",
       "3  5 TROWEL MACHINES (END TO END) & MISC LEGAL FR...  California  Elk Grove   \n",
       "4                                        150H GRADER  California  Fairfield   \n",
       "\n",
       "    destination                                 authorizedhighways  \\\n",
       "0      Antelope  * from clawiter rd s/b on ramp - 092e - 880n -...   \n",
       "1       Ontario  * from sierra ave w/b on ramp - 015s - 060w - ...   \n",
       "2        Fresno  * from industrial way - 113n - 080w - 680s - 5...   \n",
       "3  Ca/Nv Border  * from grant line rd w/b on ramp - 099n - 051n...   \n",
       "4      Saratoga  * from air base pkwy n/b on ramp - 080w - 680s...   \n",
       "\n",
       "                 route_location_origin  ... route_intersection_15  \\\n",
       "0      clawiter rd Hayward, California  ...     I80 & antelope rd   \n",
       "1       sierra ave Fontana, California  ...                         \n",
       "2     industrial way Dixon, California  ...                         \n",
       "3     grant line Elk Grove, California  ...                         \n",
       "4  air base pkwy Fairfield, California  ...                         \n",
       "\n",
       "  route_intersection_16 route_intersection_17 route_intersection_18  \\\n",
       "0                                                                     \n",
       "1                                                                     \n",
       "2                                                                     \n",
       "3                                                                     \n",
       "4                                                                     \n",
       "\n",
       "  route_intersection_19 route_intersection_20 route_intersection_21  \\\n",
       "0                                                                     \n",
       "1                                                                     \n",
       "2                                                                     \n",
       "3                                                                     \n",
       "4                                                                     \n",
       "\n",
       "  route_intersection_22 route_intersection_23  \\\n",
       "0                                               \n",
       "1                                               \n",
       "2                                               \n",
       "3                                               \n",
       "4                                               \n",
       "\n",
       "                  route_intersection_destination  \n",
       "0        I80 & antelope rd  Antelope, California  \n",
       "1       SR60 & vineyard ave  Ontario, California  \n",
       "2    SR180 & north floyd ave  Fresno, California  \n",
       "3        I80 & nv line  Ca/Nv Border, California  \n",
       "4  SR17 & scotts valley dr  Saratoga, California  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f06b84a-a49d-49a7-8e84-390ed25244f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"justlooking_subset_v1_od.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12f9287-c9dc-42e8-90cf-d1fe7b90f10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32424f9-4f6d-4fc9-af9b-986be36c806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put this into GitHub, then post in Teams to ask for assistance pulling in the Lat/Lon\n",
    "# maybe put this on Slack as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "037b35f8-a49c-4f85-9342-be0e14b2e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# may have to use mapquest - it was working pretty well to identify the locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab7eb9-3f34-470b-a675-f345cc5d7d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a79032-6148-4188-a41d-8aae8891897e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21eea06-3837-4077-81fd-b2767206789c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
