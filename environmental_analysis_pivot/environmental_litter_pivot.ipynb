{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c476aa22-706b-4c30-9353-890c7bba25b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Environmental Pivot\n",
    "* A notebook for the Division of Maintenance/Environmental Analysis to monitor litter volumes & costs\n",
    "\n",
    "* The purpose of this notebook is to concatenate IMMS LEMO Data (multiple datasets, each dataset a different Fiscal Year) & Manually entered data (one dataset containing multiple years)\n",
    "\n",
    "## Data\n",
    "\n",
    "### IMMS LEMO\n",
    "IMMS Detailed LEMO Download Instructions\n",
    "* Download updated IMMS raw data by navigating to MOMS through Enterprise Web Applications:  \n",
    "    * Welcome to MOMS | Maintenance & Operations Management Solution (MOMS)  \n",
    "* Click on IMMS -> Oracle Business Intelligence Reports-> IMMS Reports  \n",
    "* Navigate to the work management tab -> LEMO tab  \n",
    "* Select your district  \n",
    "* Copy the trash related activities listed below and insert into the Activity field box.  \n",
    "    * D40051;D40151;D40050;D40150;D30051;D30050;D41050;D41051;D42051;D42050;D44050;D44051;D43051;D45050;C50010;C50150;C60010;C60050;C60220;F20020;F20050;F70020;F70050;F70110  \n",
    "* Enter the latest work order dates for relevant FY (that is not already entered into the workbook)  \n",
    "* Under select view -> Click on District HR-PU-LEMO Detailed  \n",
    "* Export/Download to .CSV file  \n",
    "* Open the newly exported .CSV file  \n",
    "\n",
    "#### Activity Descriptions\n",
    "* Family C  \n",
    "    * C50010 - Repair/Replace Ditch/Channel  \n",
    "    * C50150 - Clean Ditch/Channel  \n",
    "    * C60010 - Repair/Replace Drainage  \n",
    "    * C60220 - Drainage Inspection  \n",
    "* Family D  \n",
    "    * D30050 - Sweep Highway/Shoulder  \n",
    "    * D30051 - Clean California Sweeping  \n",
    "    * D40050 - Litter Control  \n",
    "    * D40051 - Clean California Litter Control  \n",
    "    * D40150 - Road Patrol/Debris Pickup  \n",
    "    * D40151 - Clean California Road Patrol/Debris Pickup  \n",
    "    * D41050 - Adopt-A-Highway Litter Control  \n",
    "    * D41051 - Clean California Adopt-A-Highway Litter Control  \n",
    "    * D42050 - Unsheltered Encampment - Cleaning and Removal  \n",
    "    * D42051 - Clean California Encampment Litter/Debris Removal  \n",
    "    * D43051 - Clean California Dump Days  \n",
    "    * D44050 - Special Programs People (SPP) Litter Control  \n",
    "    * D44051 - Clean California Special Programs People (SPP) Litter Control  \n",
    "    * D45050 - Illegal Dumping Debris Removal  \n",
    "* Family F  \n",
    "    * F20020 - Drain Inlet Inspection  \n",
    "    * F20050 - Drain Cleaning  \n",
    "    * F70020 - Treatment DMP Inspection  \n",
    "    * F70050 - Clean/MOW Treatment BMP  \n",
    "    * F70110 - Repair of Treatment DMP  \n",
    "  \n",
    "  \n",
    "### Manually Entered Data  \n",
    "SPP (Olivia Liggins), Encampments (I forgot the gentlemen's name)  \n",
    "  \n",
    "#### Activity Description  \n",
    "* Special Programs  \n",
    "    * Haz Mat Encampment Contract  \n",
    "    * SPP (SHA & CC) Contract  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7eeea3-f896-433f-90d3-31aa8e308de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 01.00 import modules\n",
    "import pandas as pd\n",
    "import os\n",
    "import gcsfs\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f1962d-d233-42e0-8883-5c23774d0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02.00 Identify the path to the Data\n",
    "gcs_path_lemo = (\n",
    "    \"gs://calitp-analytics-data/data-analyses/big_data/environmental_pivot/1_imms_lemo/\"\n",
    ")\n",
    "gcs_path_supplemental_data = (\n",
    "    \"gs://calitp-analytics-data/data-analyses/big_data/environmental_pivot/2_supplemental_data/\"\n",
    ")\n",
    "\n",
    "\n",
    "# 02.01 Identify the file names\n",
    "imms_lemo_file_names = [\"lemo_fy18_final.csv\",\n",
    "              \"lemo_fy19_final.csv\",\n",
    "              \"lemo_fy20_final.csv\",\n",
    "              \"lemo_fy21_final.csv\",\n",
    "              \"lemo_fy22_final.csv\",\n",
    "              \"lemo_fy23_final.csv\",\n",
    "              \"lemo_fy24_final.csv\",\n",
    "              \"lemo_fy25_final.csv\",\n",
    "              \"lemo_fy26_20250826.csv\"] # This dataset name is typically updated when you download new data and want it included in the analysis\n",
    "\n",
    "supplemental_data_file_names = [\"encampments.csv\",\n",
    "                                \"spp_crew_expenditures.csv\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49c18aeb-5604-4000-a7fd-cc672654c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(columns):\n",
    "    \"\"\"\n",
    "    Clean column names by lowercasing, removing spaces and punctuation (e.g., '.').\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for col in columns:\n",
    "        col = col.lower()                # make lowercase\n",
    "        col = re.sub(r'[^\\w]', '', col)  # remove punctuation and spaces\n",
    "        cleaned.append(col)\n",
    "    return cleaned\n",
    "\n",
    "def load_and_concat_csvs_from_gcs(gcs_path, file_names):\n",
    "    \"\"\"\n",
    "    Load multiple CSV files from a GCS path, clean column names, and concatenate into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        gcs_path (str): The base GCS path (e.g., \"gs://bucket/folder\")\n",
    "        file_names (list of str): List of CSV file names to load\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A single concatenated DataFrame with cleaned columns\n",
    "    \"\"\"\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    dataframes = []\n",
    "\n",
    "    for file_name in file_names:\n",
    "        full_path = f\"{gcs_path.rstrip('/')}/{file_name}\"\n",
    "        try:\n",
    "            df = pd.read_csv(fs.open(full_path, 'rb'))\n",
    "\n",
    "            # Clean column names\n",
    "            df.columns = clean_column_names(df.columns)\n",
    "\n",
    "            # # Optional: track the source file\n",
    "            # df['sourcefile'] = file_name\n",
    "\n",
    "            dataframes.append(df)\n",
    "            print(f\"Loaded and cleaned: {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_name}: {e}\")\n",
    "\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No files were loaded.\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d397a09f-3cd9-466d-915f-36f957fe17a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_796/1472397373.py:29: DtypeWarning: Columns (31) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(fs.open(full_path, 'rb'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and cleaned: lemo_fy18_final.csv\n",
      "Loaded and cleaned: lemo_fy19_final.csv\n",
      "Loaded and cleaned: lemo_fy20_final.csv\n",
      "Loaded and cleaned: lemo_fy21_final.csv\n",
      "Loaded and cleaned: lemo_fy22_final.csv\n",
      "Loaded and cleaned: lemo_fy23_final.csv\n",
      "Loaded and cleaned: lemo_fy24_final.csv\n",
      "Loaded and cleaned: lemo_fy25_final.csv\n",
      "Error loading lemo_fy26_20250826.csv: b/calitp-analytics-data/o/data-analyses%2Fbig_data%2Fenvironmental_pivot%2F1_imms_lemo%2Flemo_fy26_20250826.csv\n"
     ]
    }
   ],
   "source": [
    "# This line will load all of the CSV files that are identified in the \"imms_lemo_file_names\" list\n",
    "df_lemo = load_and_concat_csvs_from_gcs(gcs_path_lemo, imms_lemo_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3916708e-9a8e-4b19-ad8c-6b0f8022c2ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and cleaned: encampments.csv\n",
      "Loaded and cleaned: spp_crew_expenditures.csv\n"
     ]
    }
   ],
   "source": [
    "# This line will load all of the CSV files that are identified in the \"supplemental_data_file_names\" list\n",
    "df_sup = load_and_concat_csvs_from_gcs(gcs_path_supplemental_data, supplemental_data_file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e072c3-94e6-4ca1-8b40-8c4e9f8f92a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update the name of the 'resp. district' column to 'district' to match the Supplemental dataset\n",
    "df_lemo = df_lemo.rename(columns={'respdistrict': 'district'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a8c4649-2646-40d0-9f94-5ccecb82c9a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a subset\n",
    "df_sup = df_sup[['district', 'fiscal_year', 'activity', 'activitydescription', 'totalcost']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50883265-0744-445d-b398-dfd9dc0d2adc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update the name of the 'fiscal_year' column to 'fiscalyear' to match the LEMO dataset\n",
    "df_sup = df_sup.rename(columns={'fiscal_year': 'fiscalyear'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ecbc7d4-74d4-4618-9ae8-8de445458e35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LEMO has all columns, and the Supplemental dataset only has a few\n",
    "df_combined = pd.concat([df_lemo, df_sup], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb8f4134-439d-4d3c-82a6-58f763f5d5b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the output_path is the name of the .csv file (include the .csv)\n",
    "file_path = \"environmental_litter_pivot.csv\"\n",
    "\n",
    "# # Save to CSV (create folder 'output' if needed)\n",
    "# df_combined.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9d3a072-8a63-4869-bc9e-3b37f1580432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 02.02 Identify the path to the output data\n",
    "gcs_output_folder = \"gs://calitp-analytics-data/data-analyses/big_data/environmental_pivot/3_python_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "739c5a58-26b0-48be-8ce6-e709ff15facf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined.to_csv(f\"{gcs_output_folder}/{file_path}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0c66579-9135-45cd-ae85-395ce609af8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1111785, 35)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current shape (520878, 35)\n",
    "df_combined.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
